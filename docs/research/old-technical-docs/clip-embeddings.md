# CLIP Embeddings

This service provides OpenCLIP image and text embeddings with lazy model loading, device auto‑selection, configurable preprocessing, optional multi‑crop, and memory‑aware batching. Embeddings are L2‑normalized and intended to be used with cosine similarity in the vector database.

`ClipEmbeddingService` initializes the OpenCLIP model via `create_model_and_transforms` using the configured model and pretrained weights. The default is `ViT-L-14/openai`. The service selects `cuda` when available, otherwise `cpu`, and loads the matching tokenizer for text encoding. The embed dimension is detected from the model visual tower (typically 768 for ViT‑L/14) and is exposed through the service info. Health reports DEGRADED until the model is loaded and HEALTHY afterward.

The service offers three main APIs. Image embedding is available via `embed_images(image_paths, batch_size=8, variant="default")`, which returns a list of normalized vectors. A detailed variant, `embed_images_detailed(image_paths, batch_size=8)`, yields per‑image results including the crop variant name or an error. Text embedding for CLIP is available via `embed_texts_clip(texts)`, which returns normalized CLIP text‑tower vectors suitable for text‑to‑image search. The `variant` argument in `embed_images` is currently reserved and not used; multi‑crop behavior is driven by configuration.

Preprocessing uses the OpenCLIP transform returned by `create_model_and_transforms`. The service infers the resize size from this transform (commonly 224 or 336 depending on the model) to estimate per‑tensor memory usage and determine safe sub‑batch sizes during encoding. When multi‑crop is disabled, each image is processed once. When enabled, the service generates a set of square crops (center and four corners); in `embed_images` the crop size is inferred from the transform, while `embed_images_detailed` currently uses a 224‑pixel crop size. The detailed API returns the associated `variant` labels.

Batching is adaptive to avoid out‑of‑memory errors. The service estimates bytes per tensor as approximately `3 * size * size * 4` and caps sub‑batches to about 512 MB on GPU or 256 MB on CPU. The `batch_size` parameter serves as an upper bound; the encoder may further split into sub‑batches based on these caps. Runtime metrics include total images processed, errors, last batch duration, and effective images per second.

The service integrates with the RAG pipeline for image ingestion and text‑to‑image retrieval. Image embeddings can be persisted through the `POST /api/rag/clip/ingest_images` endpoint, which accepts an `items` array of objects containing `image_id` and `path`. Paths are validated against the optional allowed‑roots configuration before processing. Embeddings are stored in the vector database with `model_id` set to `openclip`, `metric` set to `cosine`, and the embedding dimension taken from the service. For queries with modality set to `images`, the RAG query path embeds the input text via the CLIP text tower and performs a similarity search in the image index.

Configuration is centralized in `AppConfig`. The `rag_clip_model` setting selects the OpenCLIP model and pretrained weights using the `MODEL/PRETRAINED` format, with `ViT-L-14/openai` as the default. The `rag_clip_preprocess` setting selects the preprocess size, typically 224 or 336, which influences crop generation and memory estimates. The `rag_clip_multicrop` flag enables center and corner crops for robustness at the cost of throughput. Environment variables `RAG_CLIP_MODEL`, `RAG_CLIP_PREPROCESS`, and `RAG_CLIP_MULTICROP` are supported and override config values when present. Operational clamps include `rag_clip_max_items_per_request` for ingestion size, `rag_ingest_allowed_roots` to sandbox filesystem access, and standard RAG rate limits.

Operationally, the service depends on `torch` and uses OpenCLIP if available. It initializes lazily and stays DEGRADED if OpenCLIP cannot be imported, allowing the rest of the system to operate. Device selection prefers CUDA when available. All outputs are L2‑normalized, and downstream components should use cosine similarity. Do not mix embeddings from different CLIP model variants in the same index.

Practical guidance and best practices derived from OpenCLIP and CLIP usage in the community include the following. Always normalize embeddings and use cosine similarity for scoring. Prefer `ViT‑L/14` with `openai` weights for strong baseline performance and 768‑dimensional outputs; consider the 336‑pixel preprocess variants for higher‑resolution images when supported by the chosen model. Multi‑crop can improve robustness for challenging images but increases compute linearly with the number of crops; enable it selectively for ingestion pipelines where quality is preferred over speed. Choose `batch_size` conservatively and rely on the service’s memory caps to avoid OOMs, especially on CPU or smaller GPUs. For text‑to‑image search, use the model‑matched tokenizer and maintain the same model for both image and text encoders to ensure embedding compatibility.

- Files:
  - `app/services/integration/clip_embedding_service.py`
  - `app/api/rag.py`
  - `app/services/core/app_config.py`
