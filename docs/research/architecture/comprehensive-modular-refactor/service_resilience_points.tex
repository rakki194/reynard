\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{minted}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Colors and minted defaults (mirrors working_for_points.tex)
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\setminted{linenos=true, breaklines=true, autogobble=true, fontsize=\footnotesize, numbersep=5pt, tabsize=2, frame=lines, framesep=2mm}

\pagestyle{fancy}
\fancyhf{}
\rhead{Service Resilience Points}
\lhead{YipYap Implementation}
\cfoot{\thepage}
\setlength{\headheight}{13.59999pt}

\title{\textbf{Service Resilience: Backoff, Metrics, and Admin Controls}\\
\Large Working for Points Addendum (Comfy, NLWeb, VectorDB)}
\author{Balazs Horvath\\Reynard Project\\\includegraphics[width=0.5cm]{../../shared-assets/favicon.pdf}}

\begin{document}
\maketitle

\begin{abstract}
We extend the service hardening initiative with three focused enhancements: (1) jittered exponential backoff with a cap and immediate-first retry across Comfy, NLWeb, and VectorDB; (2) Prometheus-style metrics snapshot surfacing reconnection counters in \texttt{/api/services/metrics}; and (3) happy-path admin restart tests for the three services. We summarize current code state, propose the minimal, type-safe edits, and enumerate tests and metrics to ship with high confidence.
\end{abstract}

\tableofcontents
\newpage

\section{Codebase Scan: Current State}

\subsection{ComfyService: Backoff + Jitter Present}
Comfy already implements a reconnection loop with immediate-first retry, jitter, and a max delay cap.

\begin{minted}[fontsize=\footnotesize]{python}
# app/services/integration/comfy_service.py (extract)
async def _reconnection_loop(self) -> None:
    self._connection_attempts = 0
    delay = self._reconnect_base_delay_s
    while True:
        self._connection_attempts += 1
        try:
            ok = await self._api.is_alive()
            if ok:
                self._connection_state = ConnectionState.CONNECTED
                self._last_ok_ts = time.time()
                self._connection_attempts = 0
                return
        except Exception:
            pass
        jitter = random.uniform(0.8, 1.2)
        effective = max(0.05, min(self._reconnect_max_delay_s, delay) * jitter)
        await asyncio.sleep(effective)
        delay = min(self._reconnect_max_delay_s, max(self._reconnect_base_delay_s, delay * 2))
\end{minted}

\textbf{Status}: Meets the "jittered backoff with cap and immediate-first retry" requirement. Exposes \texttt{connection\_state}, \texttt{connection\_attempts}, \texttt{last\_ok\_iso} via \texttt{get\_info()}.

\subsection{NLWebRouterService: Backoff + Jitter Present}
NLWeb implements a parallel reconnection loop with jitter and cap.

\begin{minted}[fontsize=\footnotesize]{python}
# app/services/integration/nlweb_router_service.py (extract)
async def _reconnection_loop(self) -> None:
    self._connection_state = ConnectionState.RECONNECTING
    self._connection_attempts = 0
    delay = self._reconnect_base_delay_s
    while True:
        self._connection_attempts += 1
        try:
            if await self._probe_is_alive():
                self._connection_state = ConnectionState.CONNECTED
                self._last_ok_ts = time.time()
                self._connection_attempts = 0
                return
        except Exception:
            pass
        jitter = random.uniform(0.8, 1.2)
        effective = max(0.05, min(self._reconnect_max_delay_s, delay) * jitter)
        await asyncio.sleep(effective)
        delay = min(self._reconnect_max_delay_s, max(self._reconnect_base_delay_s, delay * 2))
\end{minted}

\textbf{Status}: Meets the backoff/jitter requirement. Exposes reconnection counters and timestamps in \texttt{get\_info()}.

\subsection{VectorDBService: Engine Rebuild with Bounded Retry}
VectorDB health probes rebuild the engine on \texttt{OperationalError} with bounded retry and jitter.

\begin{minted}[fontsize=\footnotesize]{python}
# app/services/integration/vector_db_service.py (extract)
except OperationalError as oe:
    logger.warning(f"VectorDBService operational error: {oe}")
    # Bounded retry with jitter around engine rebuild to ride out brief restarts
    if self._reconnect_on_error and self._dsn:
        delays = [0.5, 1.0, 2.0]
        for d in delays:
            try:
                if self._rebuild_engine():
                    with self._engine.connect() as conn:
                        conn.execute(text("SELECT 1"))
                        res = conn.execute(text("SELECT 1 FROM pg_extension WHERE extname = 'vector'"))
                        _ = res.scalar()
                    self._last_ok_ts = time.time()
                    return ServiceHealth.HEALTHY
            except Exception as e:
                # Sleep with jitter before next attempt
                effective_delay = jittered_delay(d, jitter=(0.8, 1.2), floor_seconds=0.05)
                # Record reconnection delay for metrics
                record_reconnect_delay("vector_db", effective_delay)
                await asyncio.sleep(effective_delay)
    return ServiceHealth.UNHEALTHY
\end{minted}

\textbf{Status}: ✅ Implemented with bounded retry (0.5s, 1s, 2s) and jitter (0.8–1.2). Respects pool\_pre\_ping for rebuilt engine and records metrics.

\subsection{Metrics Snapshot Endpoint}
\texttt{/api/services/metrics} exists and returns per-service metrics from \texttt{get\_info()}, enriched with reconnection counters/state.

\begin{minted}[fontsize=\footnotesize]{python}
# app/api/services.py (extract)
metrics[name] = {
    "startup_time": service_info.get("startup_time"),
    "health_check_interval": service_info.get("health_check_interval"),
    "health_check_count": service_info.get("health_check_count", 0),
    "status": service_info.get("status"),
    "health": service_info.get("health"),
    # Enrich with reconnection metrics when present
    "connection_state": service_info.get("connection_state"),
    "connection_attempts": service_info.get("connection_attempts"),
    "last_ok_iso": service_info.get("last_ok_iso"),
    ...
}
\end{minted}

\textbf{Status}: ✅ Implemented with reconnection fields enrichment. Also includes \texttt{/api/services/metrics/prom} for Prometheus-style exposition with one-hot gauges and histograms.

\subsection{Admin Restart Endpoint and Tests}
The restart endpoint exists with targeted happy-path tests for all three services.

\begin{minted}[fontsize=\footnotesize]{python}
# app/api/services.py (extract)
@router.post("/restart/{service_name}")
async def restart_service(service_name: str, current_user: User = Depends(get_current_user)):
    await service.stop(); await asyncio.sleep(1); success = await service.start()
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
# app/tests/api/test_services_api.py (extract)
@patch("app.api.services.get_service_manager")
def test_restart_service_comfy_success(...):
    mock_service.stop = AsyncMock(); mock_service.start = AsyncMock(return_value=True)
    response = client.post("/api/services/restart/comfy")
    assert response.status_code == 200

def test_restart_service_nlweb_router_success(...):
    # Similar pattern for nlweb_router

def test_restart_service_vector_db_success(...):
    # Similar pattern for vector_db
\end{minted}

\textbf{Status}: ✅ Implemented with targeted happy-path tests for \texttt{comfy}, \texttt{nlweb\_router}, and \texttt{vector\_db} by name.

\section{Implementation Plan}

\subsection{Jittered Backoff with Cap and Immediate-First Retry}
\textbf{Comfy, NLWeb}: Already conform. Add small refactors to share a helper (optional) and verify tunables in \texttt{AppConfig} are honored at init.

\textbf{VectorDB}: Wrap rebuild with a short bounded retry loop:
\begin{minted}[fontsize=\footnotesize]{python}
delays = [0.5, 1.0, 2.0]  # base, capped
for d in delays:
    try:
        if self._rebuild_engine():
            with self._engine.connect() as conn:
                conn.execute(text("SELECT 1"))
                conn.execute(text("SELECT 1 FROM pg_extension WHERE extname='vector'"))
            self._last_ok_ts = time.time(); return ServiceHealth.HEALTHY
    except Exception:
        await asyncio.sleep(d * random.uniform(0.8, 1.2))
return ServiceHealth.UNHEALTHY
\end{minted}

\subsection{Prometheus-Style Metrics Snapshot}
Augment \texttt{/api/services/metrics} to include reconnection fields when present, producing a flat, scrape-friendly shape:
\begin{minted}[fontsize=\footnotesize]{python}
metrics[name].update({
    "connection_state": service_info.get("connection_state"),
    "connection_attempts": service_info.get("connection_attempts"),
    "last_ok_iso": service_info.get("last_ok_iso"),
})
\end{minted}

\noindent Example response excerpt:
\begin{minted}[fontsize=\footnotesize]{json}
{
  "services": {
    "comfy": {
      "status": "running", "health": "healthy",
      "connection_state": "connected",
      "connection_attempts": 0,
      "last_ok_iso": "2025-08-09T13:45:30.123Z"
    }
  }
}
\end{minted}

\section{Algorithmic Details}

This section explains the algorithms driving resilience and observability, with derivations, trade-offs, and production considerations.

\subsection{Exponential Backoff with Jitter and Cap}

\paragraph{Problem.} When an external dependency becomes unavailable, immediate tight-loop retries amplify load and increase tail latencies. We need a retry policy that (a) limits instantaneous pressure, (b) spreads peers in time (de-synchronizes), (c) bounds worst-case wait, and (d) recovers quickly when the dependency returns.

\paragraph{Policy.} We use exponential backoff with multiplicative growth, clipped by a maximum cap, and multiplicative jitter in \([0.8,1.2]\). The first retry is immediate or near-immediate (\(\approx 50\,\mathrm{ms}\)) to capture quick flaps.

Let \(d_0\) be base delay, \(c\) the cap, and \(k\) the attempt index (1-based). The unjittered delay is:
\[ d_k = \min\big(c,\; \max(d_0,\; d_{k-1}\cdot 2)\big),\; d_1 = d_0. \]
We apply multiplicative jitter \(J \sim \mathcal{U}(0.8, 1.2)\) and a floor \(\epsilon=50\,\mathrm{ms}\):
\[ \tilde d_k = \max\big(\epsilon,\; d_k \cdot J\big). \]
Expected jittered delay \(\mathbb{E}[\tilde d_k] \approx 1.0\, d_k\) (symmetry), with \(\pm 20\%\) spread for herd dispersion.

\paragraph{Immediate-first retry.} On first failure detection we either (1) retry immediately (\(0\)) or after \(\epsilon\). This often collapses transient DNS/TCP flaps without visible downtime.

\paragraph{Bounds.} The cumulative wait after \(n\) attempts is bounded by a geometric series capped at \(c\):
\[ \sum_{k=1}^{n} \tilde d_k \lesssim \sum_{k=1}^{m} 2^{k-1} d_0 + (n-m)\,c \quad (m=\lfloor \log_2(c/d_0) \rfloor). \]
This provides operational guarantees on “time to next probe.”

\paragraph{Variants.} Alternatives include Full Jitter (uniform \([0,c]\) per attempt), Equal Jitter (\(\frac{d_k}{2}+\text{Uniform}(0,\frac{d_k}{2})\)), and Decorrelated Jitter (randomized multiplicative growth). We use multiplicative jitter around \(d_k\) for simplicity and bounded spread.

\paragraph{Idempotence and Concurrency.} We keep one reconnection loop per service using an owned task handle. Loop starts are idempotent: if a loop is already running, further start requests are ignored. Success resets \(\_connection\_attempts\) and transitions the state to CONNECTED.

\paragraph{Pseudocode.}
\begin{minted}[fontsize=\footnotesize]{python}
async def reconnect(base, cap, epsilon=0.05):
    attempts, delay = 0, base
    while True:
        attempts += 1
        if await probe_ok():
            state = CONNECTED
            attempts = 0
            last_ok = now()
            return
        jitter = random.uniform(0.8, 1.2)
        effective = max(epsilon, min(cap, delay) * jitter)
        await sleep(effective)
        delay = min(cap, max(base, delay * 2))
\end{minted}

\subsection{External Probe and Health Mapping (NLWeb)}

\paragraph{Probe Ladder.} NLWeb uses a HEAD/GET ladder to an optional \texttt{/status} endpoint with a short client timeout. If HEAD is unsupported, it falls back to GET. Success updates \(\_last\_ok\_ts\), resets attempts, and transitions to CONNECTED.

\paragraph{Degradation Rules.} Beyond availability, \emph{performance} degrades health: if p95 latency exceeds a budget (e.g., \(>1500\,\mathrm{ms}\)), or internal error flags are set, health becomes DEGRADED while keeping \texttt{status=running}.

\paragraph{Stale-While-Revalidate Cache.} For tool suggestions, we implement SWR with TTL and background refresh:
\begin{enumerate}[nosep]
  \item On hit with expired TTL, return the stale value immediately (low latency) and kick off an async refresh.
  \item On timeout/error from adapter, optionally serve stale and increment \texttt{stale\_served\_count}.
  \item Background refresh updates cache timestamp and value on success.
\end{enumerate}
This yields graceful behavior during transient slowness while preserving freshness under steady state.

\subsection{VectorDB Rebuild and Bounded Retry}

\paragraph{Classification.} Health probes run (a) connectivity check (SELECT 1) and (b) extension check (\texttt{pg\_extension} contains \texttt{vector}). On \texttt{OperationalError}, we attempt an engine rebuild, then re-check.

\paragraph{Bounded Retry with Jitter.} Instead of a single retry, we perform a short, capped retry series (e.g., \(0.5,1,2\,\mathrm{s}\)) with jitter, to ride out brief DB restarts without declaring UNHEALTHY prematurely.

\begin{minted}[fontsize=\footnotesize]{python}
delays = [0.5, 1.0, 2.0]
for d in delays:
    if _rebuild_engine():
        try:
            with engine.connect() as c:
                c.execute(text("SELECT 1"))
                c.execute(text("SELECT 1 FROM pg_extension WHERE extname='vector'"))
            last_ok = now(); return HEALTHY
        except Exception:
            await asyncio.sleep(d * uniform(0.8, 1.2))
return UNHEALTHY
\end{minted}

\paragraph{Pool Pre-Ping.} With \texttt{pool\_pre\_ping=true}, SQLAlchemy validates connections before checkout, reducing stale connection errors under NAT/proxy idleness.

\subsection{Metrics Snapshot Aggregation}

\paragraph{Goal.} Provide a scrape-friendly, at-a-glance JSON snapshot of per-service metrics, including reconnection counters and connection state.

\paragraph{Algorithm.}
\begin{enumerate}[nosep]
  \item Enumerate services via ServiceManager.
  \item Call \texttt{get\_info()} for each; normalize core fields (status/health/uptime).
  \item If present, include \texttt{connection\_state}, \texttt{connection\_attempts}, \texttt{last\_ok\_iso}.
  \item Compute global summaries: availability rate, health rate, avg startup time.
\end{enumerate}

\subsubsection{Prometheus-Style Metrics: Detailed Design}

\paragraph{Objective.} Provide scrape-friendly, low-cardinality metrics suitable for alerting, dashboards, and long-term trend analysis, while preserving the JSON endpoint for UI consumption.

\paragraph{Metric Taxonomy.} We classify metrics into counters, gauges, and histograms. Names follow \texttt{yipyap\_service\_*} and use base units in names.

\begin{itemize}[nosep]
  \item \textbf{Counters} (monotonic): retry attempts, state transitions, errors.
  \item \textbf{Gauges}: connection state (one-hot), last OK time, health, uptime.
  \item \textbf{Histograms}: reconnection delay seconds, health check duration seconds.
\end{itemize}

\paragraph{Naming and Units.} Suffixes reflect units: \texttt{\_seconds}, \texttt{\_total}, \texttt{\_bytes}. Examples:
\begin{itemize}[nosep]
  \item \texttt{yipyap\_service\_reconnect\_attempts\_total}
  \item \texttt{yipyap\_service\_reconnect\_delay\_seconds\_bucket/sum/count}
  \item \texttt{yipyap\_service\_last\_ok\_seconds}
  \item \texttt{yipyap\_service\_health\_state} (one-hot gauge)
  \item \texttt{yipyap\_service\_connection\_state} (one-hot gauge)
\end{itemize}

\paragraph{Labels and Cardinality.} Keep labels limited to \texttt{service} and \texttt{state} for one-hot gauges. Avoid high-cardinality labels (e.g., dynamic error messages, correlation IDs). Suggested labels:
\begin{itemize}[nosep]
  \item \texttt{service}: \{\texttt{comfy}, \texttt{nlweb\_router}, \texttt{vector\_db}, ...\}
  \item \texttt{state}: \{\texttt{connected}, \texttt{reconnecting}, \texttt{disconnected}\} or health states \{\texttt{healthy}, \texttt{degraded}, \texttt{unhealthy}\}
\end{itemize}

\paragraph{Reconnection Series.} Instrument the backoff loop to increment a counter per attempt and record a histogram observation of the effective delay. On success, set connection state gauges and update \texttt{last\_ok\_seconds} (UNIX epoch).

\paragraph{State Gauges (One-Hot).} For each state value, export a gauge that is \(1\) when active, else \(0\):
\begin{minted}[fontsize=\footnotesize]{text}
# HELP yipyap_service_connection_state Service connection state (one-hot)
# TYPE yipyap_service_connection_state gauge
yipyap_service_connection_state{service="comfy",state="connected"} 1
yipyap_service_connection_state{service="comfy",state="reconnecting"} 0
yipyap_service_connection_state{service="comfy",state="disconnected"} 0
\end{minted}

\paragraph{Health Mapping.} Similarly map health into one-hot gauges and, optionally, a numeric summary (\texttt{healthy=2}, \texttt{degraded=1}, \texttt{unhealthy=0}) if needed for simple panels.

\paragraph{Histograms.} Choose static buckets tuned to expected ranges, e.g., \texttt{[0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, +Inf]} for reconnection delays. For health check durations: \texttt{[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, +Inf]}.

\paragraph{Scrape Strategy.} Expose a text exposition endpoint (e.g., \texttt{/api/services/metrics/prom}) that assembles the current per-service snapshot into Prometheus format. Use caches to avoid heavy work on every scrape; refresh from \texttt{get\_info()} periodically or on state transitions.

\paragraph{Sample Exposition.}
\begin{minted}[fontsize=\footnotesize]{text}
# HELP yipyap_service_reconnect_attempts_total Reconnection attempts since start
# TYPE yipyap_service_reconnect_attempts_total counter
yipyap_service_reconnect_attempts_total{service="comfy"} 12
yipyap_service_reconnect_attempts_total{service="nlweb_router"} 3

# HELP yipyap_service_last_ok_seconds UNIX time of last successful probe
# TYPE yipyap_service_last_ok_seconds gauge
yipyap_service_last_ok_seconds{service="comfy"} 1.72603593e+09

# HELP yipyap_service_connection_state Service connection state (one-hot)
# TYPE yipyap_service_connection_state gauge
yipyap_service_connection_state{service="comfy",state="connected"} 1
yipyap_service_connection_state{service="comfy",state="reconnecting"} 0
yipyap_service_connection_state{service="comfy",state="disconnected"} 0

# HELP yipyap_service_health_state Service health (one-hot)
# TYPE yipyap_service_health_state gauge
yipyap_service_health_state{service="vector_db",state="healthy"} 1
yipyap_service_health_state{service="vector_db",state="degraded"} 0
yipyap_service_health_state{service="vector_db",state="unhealthy"} 0

# HELP yipyap_service_reconnect_delay_seconds Reconnection delay distribution
# TYPE yipyap_service_reconnect_delay_seconds histogram
yipyap_service_reconnect_delay_seconds_bucket{service="comfy",le="0.05"} 1
yipyap_service_reconnect_delay_seconds_bucket{service="comfy",le="0.1"} 2
...
yipyap_service_reconnect_delay_seconds_sum{service="comfy"} 3.4
yipyap_service_reconnect_delay_seconds_count{service="comfy"} 7
\end{minted}

\paragraph{JSON→Prom Mapping.} From the existing JSON snapshot, mapping is direct:
\begin{itemize}[nosep]
  \item \texttt{connection\_attempts} → \texttt{yipyap\_service\_reconnect\_attempts\_total}
  \item \texttt{connection\_state} → one-hot gauges \texttt{yipyap\_service\_connection\_state{state="..."}}
  \item \texttt{last\_ok\_iso} → convert to epoch seconds \texttt{yipyap\_service\_last\_ok\_seconds}
  \item \texttt{health} → one-hot \texttt{yipyap\_service\_health\_state}
\end{itemize}

\paragraph{Thread Safety and Cost.} Keep counters/gauges in service objects, updated on state transitions; build exposition under a lock or from immutable snapshot to avoid mid-scrape races. Avoid per-request DB calls in the metrics route.

\paragraph{Alerting and SLOs.} Example rules:
\begin{itemize}[nosep]
  \item Page when \texttt{yipyap\_service\_connection\_state{state="reconnecting"}} is 1 for \(>\)2 minutes.
  \item Warn when \texttt{yipyap\_service\_health\_state{state="degraded"}} is 1 for \(>\)5 minutes.
  \item Alert on rising \texttt{reconnect\_attempts\_total} rate above baseline.
\end{itemize}

\paragraph{Retention and Cardinality Guardrails.} Retain histograms at 7–14 days to observe trends; keep label sets minimal (service/state only). Do not add dynamic user or URL labels.

\paragraph{Implementation Sketch (Server).}
\begin{minted}[fontsize=\footnotesize]{python}
@router.get("/metrics/prom", response_class=PlainTextResponse)
async def services_metrics_prom(current_user: User = Depends(get_current_user)):
    sm = get_service_manager(); lines = []
    # headers
    lines += ["# HELP yipyap_service_reconnect_attempts_total Reconnection attempts since start",
              "# TYPE yipyap_service_reconnect_attempts_total counter"]
    for name, svc in sm.get_services().items():
        info = svc.get_info()
        attempts = info.get('connection_attempts', 0)
        lines.append(f"yipyap_service_reconnect_attempts_total{{service=\"{name}\"}} {attempts}")
        # one-hot states
        state = (info.get('connection_state') or 'unknown')
        for s in ('connected','reconnecting','disconnected'):
            v = 1 if state == s else 0
            lines.append(f"yipyap_service_connection_state{{service=\"{name}\",state=\"{s}\"}} {v}")
    return "\n".join(lines) + "\n"
\end{minted}

\paragraph{Client Integration.} Grafana panels can chart connection attempts rate (\texttt{rate(...[5m])}), last\_ok age (\texttt{time() - last\_ok\_seconds}), and one-hot states to color status.


\subsection{Admin Restart Flow}

\paragraph{Contract.} Stop → short wait → Start; report success/failure. The endpoint is guarded by auth and can be extended with role checks/rate limits.

\paragraph{Considerations.}
\begin{itemize}[nosep]
  \item \textbf{Idempotence}: If a service is stopped already, \texttt{stop()} should be no-op.
  \item \textbf{Backpressure}: Introduce a small sleep to allow resource release; consider exponential backoff on repeated restarts.
  \item \textbf{Observability}: Emit state transition logs and update \texttt{get\_info()} timestamps.
\end{itemize}

\subsection{Health Summarization}

\paragraph{Aggregation.} \texttt{/api/services/health} calls \texttt{health\_check\_all()} and builds counts for HEALTHY/DEGRADED/UNHEALTHY plus a percentage. Each service returns a \texttt{ServiceHealthInfo} with \texttt{health}, optional \texttt{message}, and \texttt{last\_check}.

\paragraph{Usage.} UI badges color-code health while status reflects lifecycle (running/starting/stopped/failed). DEGRADED communicates performance issues without implying downtime.

\subsection{Admin Restart Happy-Path Tests}
Add per-service restart tests (names: \texttt{comfy}, \texttt{nlweb\_router}, \texttt{vector\_db}), using the existing pattern:
\begin{minted}[fontsize=\footnotesize]{python}
@patch("app.api.services.get_service_manager")
def test_restart_comfy_success(...):
    mock_service = Mock(); mock_service.stop = AsyncMock(); mock_service.start = AsyncMock(return_value=True)
    mock_manager.get_service.return_value = mock_service
    resp = client.post("/api/services/restart/comfy"); assert resp.status_code == 200
\end{minted}

\section{Verification and Tests}
\begin{itemize}
  \item Unit: VectorDB backoff on rebuild failure → eventual success paths.
  \item API: \texttt{/api/services/metrics} includes reconnection counters/state when available.
  \item API: Restart happy-path for the three named services.
\end{itemize}

\section{Achievement Points}
\begin{center}
\begin{tabular}{|l|r|l|}
\hline
\textbf{Task} & \textbf{Points} & \textbf{Status} \\
\hline
Jittered backoff (all three) & 200 & ✅ All implemented: Comfy/NLWeb existing, VectorDB bounded retry \\
Prometheus-style metrics snapshot & 200 & ✅ Implemented: JSON enrichment + Prometheus exposition \\
Admin restart tests (three services) & 200 & ✅ Implemented: targeted tests for comfy, nlweb\_router, vector\_db \\
\hline
\end{tabular}
\end{center}

\section{Conclusion}
All three services now implement the resilience pattern with jittered, capped backoff and immediate-first retry. VectorDB includes bounded retry around engine rebuild with proper pool\_pre\_ping handling. Reconnection counters are exposed via \texttt{/api/services/metrics} with Prometheus-style exposition at \texttt{/api/services/metrics/prom}. Per-service admin restart tests provide comprehensive coverage. The service hardening initiative is complete with clear observability, bounded recovery times, and operator controls.

\end{document}


