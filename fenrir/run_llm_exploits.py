#!/usr/bin/env python3
"""
🐺 FENRIR - LLM Exploitation Runner

Main execution script for comprehensive LLM and AI service security testing.
This script provides a command-line interface for running specialized
AI security tests against the Reynard backend ecosystem.

Usage:
    python -m fenrir.run_llm_exploits --target http://localhost:8000
    python -m fenrir.run_llm_exploits --target https://api.reynard.dev --auth-token YOUR_JWT_TOKEN
    python -m fenrir.run_llm_exploits --config llm_exploit_config.json
"""

import argparse
import asyncio
import json
import logging
import sys
from typing import Any

# FENRIR LLM exploitation modules
from .llm_exploits.llm_exploitation_orchestrator import (
    LLMExploitationConfig,
    LLMExploitationOrchestrator,
)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("fenrir_llm_exploits.log"),
    ],
)
logger = logging.getLogger(__name__)


def print_fenrir_banner() -> None:
    """🐺 Display the FENRIR LLM exploitation banner."""
    banner = """
    🐺 ════════════════════════════════════════════════════════════════════════ 🐺

    ███████╗███████╗███╗   ██╗██████╗ ██╗██████╗      ██╗     ██╗     ███╗   ███╗
    ██╔════╝██╔════╝████╗  ██║██╔══██╗██║██╔══██╗     ██║     ██║     ████╗ ████║
    █████╗  █████╗  ██╔██╗ ██║██████╔╝██║██████╔╝     ██║     ██║     ██╔████╔██║
    ██╔══╝  ██╔══╝  ██║╚██╗██║██╔══██╗██║██╔══██╗     ██║     ██║     ██║╚██╔╝██║
    ██║     ███████╗██║ ╚████║██║  ██║██║██║  ██║     ███████╗███████╗██║ ╚═╝ ██║
    ╚═╝     ╚══════╝╚═╝  ╚═══╝╚═╝  ╚═╝╚═╝╚═╝  ╚═╝     ╚══════╝╚══════╝╚═╝     ╚═╝

    Framework for Exploitative Network Reconnaissance and Intrusion Research
    🦊 LLM & AI Service Security Testing Module 🦊

    🐺 ════════════════════════════════════════════════════════════════════════ 🐺

    *whiskers twitch with anticipation as the great wolf prepares to hunt*

    Welcome to FENRIR's most sophisticated hunting module - the LLM Exploitation Arsenal.
    This framework targets the emerging attack surface of AI-powered applications,
    focusing on prompt injection, streaming exploitation, and service chain attacks.

    ⚠️  WARNING: DESTRUCTIVE TESTING AHEAD ⚠️
    These exploits are designed to ACTUALLY BREAK your AI services!

    Target Services:
    🎯 Ollama (Local LLM inference and tool calling)
    🎯 NLWeb (Natural language web processing)
    🎯 ComfyUI (Image generation workflows)
    🎯 Diffusion LLM (Text generation and infilling)
    🎯 RAG (Retrieval-Augmented Generation)
    🎯 Caption (Image captioning services)
    🎯 Summarization (Document summarization)
    🎯 TTS (Text-to-Speech synthesis)

    🐺 The hunt begins... 🐺
    """
    print(banner)


def load_config_from_file(config_path: str) -> LLMExploitationConfig:
    """
    Load LLM exploitation configuration from JSON file.

    Args:
        config_path: Path to configuration file

    Returns:
        LLM exploitation configuration object
    """

    try:
        with open(config_path) as f:
            config_data = json.load(f)

        return LLMExploitationConfig(**config_data)

    except Exception as e:
        logger.error(f"Failed to load configuration from {config_path}: {e}")
        sys.exit(1)


def create_default_config(
    target_url: str, auth_token: str | None = None, test_type: str = "comprehensive"
) -> LLMExploitationConfig:
    """
    Create default LLM exploitation configuration.

    Args:
        target_url: Target URL for testing
        auth_token: Optional authentication token
        test_type: Type of testing (quick, comprehensive, targeted)

    Returns:
        LLM exploitation configuration object
    """

    if test_type == "quick":
        return LLMExploitationConfig(
            target_url=target_url,
            auth_token=auth_token,
            enable_prompt_injection=True,
            enable_streaming_exploits=False,
            enable_service_chains=False,
            enable_model_exploits=True,
            max_concurrent_attacks=2,
            attack_delay=1.0,
            max_test_duration=600,  # 10 minutes
        )
    if test_type == "targeted":
        return LLMExploitationConfig(
            target_url=target_url,
            auth_token=auth_token,
            enable_prompt_injection=True,
            enable_streaming_exploits=True,
            enable_service_chains=False,
            enable_model_exploits=True,
            max_concurrent_attacks=3,
            attack_delay=0.5,
            max_test_duration=900,  # 15 minutes
        )
    # comprehensive
    return LLMExploitationConfig(
        target_url=target_url,
        auth_token=auth_token,
        enable_prompt_injection=True,
        enable_streaming_exploits=True,
        enable_service_chains=True,
        enable_model_exploits=True,
        max_concurrent_attacks=5,
        attack_delay=0.5,
        max_test_duration=1800,  # 30 minutes
    )


async def run_llm_exploitation_tests(config: LLMExploitationConfig) -> dict[str, Any]:
    """
    Execute comprehensive LLM exploitation testing.

    Args:
        config: LLM exploitation configuration

    Returns:
        Complete test results and security assessment
    """

    logger.info("🐺 FENRIR LLM exploitation testing initiated")
    logger.info(f"🎯 Target: {config.target_url}")

    async with LLMExploitationOrchestrator(config) as orchestrator:
        # Execute comprehensive testing
        results = await orchestrator.execute_comprehensive_llm_security_test()

        # Export detailed report
        report_path = orchestrator.export_report()
        results["report_path"] = report_path

        return results


def display_test_results(results: dict[str, Any]) -> None:
    """
    Display test results in a formatted manner.

    Args:
        results: Test results from LLM exploitation
    """

    if "fenrir_master_report" not in results:
        print("❌ No valid test results found")
        return

    report = results["fenrir_master_report"]
    exec_summary = report["executive_summary"]
    vuln_breakdown = report["vulnerability_breakdown"]

    print("\n🐺 ═══════════════════════════════════════════════════════════════")
    print("🐺 FENRIR LLM EXPLOITATION RESULTS")
    print("🐺 ═══════════════════════════════════════════════════════════════")

    print(f"\n🎯 Target: {report['metadata']['target']}")
    print(f"⏱️  Execution Time: {report['metadata']['execution_time']}s")
    print(f"🛡️ Security Posture: {exec_summary['overall_security_posture']}")
    print(f"📊 Risk Score: {exec_summary['risk_score']}/100")

    print("\n📈 ATTACK STATISTICS")
    print(f"⚔️  Total Attacks: {exec_summary['total_attacks_executed']}")
    print(f"💥 Successful Exploits: {exec_summary['successful_exploits']}")
    print(f"📍 Success Rate: {exec_summary['success_rate']}%")

    print("\n🚨 VULNERABILITY BREAKDOWN")
    print(f"🔴 Critical: {vuln_breakdown['critical']}")
    print(f"🟠 High: {vuln_breakdown['high']}")
    print(f"🟡 Medium: {vuln_breakdown['medium']}")
    print(f"📊 Total: {vuln_breakdown['total']}")

    if exec_summary["services_compromised"]:
        print("\n🔥 COMPROMISED SERVICES:")
        for service in exec_summary["services_compromised"]:
            print(f"   💀 {service}")

    if exec_summary["data_exfiltration_risks"] > 0:
        print(
            f"\n📤 Data Exfiltration Risks: {exec_summary['data_exfiltration_risks']}"
        )

    if exec_summary["privilege_escalation_risks"] > 0:
        print(
            f"⬆️  Privilege Escalation Risks: {exec_summary['privilege_escalation_risks']}"
        )

    print("\n🔍 TOP RECOMMENDATIONS:")
    for i, rec in enumerate(report["top_recommendations"][:5], 1):
        print(f"   {i}. {rec}")

    if "report_path" in results:
        print(f"\n📄 Detailed Report: {results['report_path']}")

    print("\n🐺 ═══════════════════════════════════════════════════════════════")
    print("🐺 THE HUNT IS COMPLETE")
    print("🐺 ═══════════════════════════════════════════════════════════════")


def create_sample_config() -> None:
    """Create a sample configuration file for reference."""

    sample_config = {
        "target_url": "http://localhost:8000",
        "auth_token": None,
        "enable_prompt_injection": True,
        "enable_streaming_exploits": True,
        "enable_service_chains": True,
        "enable_auth_bypass": True,
        "enable_model_exploits": True,
        "max_concurrent_attacks": 5,
        "attack_delay": 0.5,
        "max_test_duration": 1800,
        "generate_detailed_report": True,
        "export_vulnerabilities": True,
        "include_remediation": True,
    }

    with open("llm_exploit_config_sample.json", "w") as f:
        json.dump(sample_config, f, indent=2)

    print("📝 Sample configuration file created: llm_exploit_config_sample.json")


def main() -> None:
    """Main entry point for FENRIR LLM exploitation testing."""

    parser = argparse.ArgumentParser(
        description="🐺 FENRIR LLM Exploitation Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  🐺 Quick test against local development server:
    python -m fenrir.run_llm_exploits --target http://localhost:8000 --test-type quick

  🐺 Comprehensive test with authentication:
    python -m fenrir.run_llm_exploits --target https://api.reynard.dev --auth-token YOUR_JWT_TOKEN

  🐺 Targeted testing of specific vulnerabilities:
    python -m fenrir.run_llm_exploits --target http://api.example.com --test-type targeted

  🐺 Load configuration from file:
    python -m fenrir.run_llm_exploits --config llm_exploit_config.json

  🐺 Generate sample configuration file:
    python -m fenrir.run_llm_exploits --create-sample-config

⚠️  WARNING: These are destructive security tests. Only run against systems you own!
        """,
    )

    parser.add_argument(
        "--target", "-t", help="Target URL for LLM exploitation testing"
    )

    parser.add_argument(
        "--auth-token", "-a", help="JWT authentication token for authenticated testing"
    )

    parser.add_argument(
        "--config", "-c", help="Path to LLM exploitation configuration file"
    )

    parser.add_argument(
        "--test-type",
        choices=["quick", "targeted", "comprehensive"],
        default="comprehensive",
        help="Type of testing to perform (default: comprehensive)",
    )

    parser.add_argument("--output", "-o", help="Output path for detailed report")

    parser.add_argument(
        "--create-sample-config",
        action="store_true",
        help="Create a sample configuration file and exit",
    )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="Suppress banner and verbose output"
    )

    args = parser.parse_args()

    # Handle sample config creation
    if args.create_sample_config:
        create_sample_config()
        return

    # Display banner unless quiet mode
    if not args.quiet:
        print_fenrir_banner()

    # Validate arguments
    if not args.config and not args.target:
        parser.error("Must specify either --target or --config")

    # Load configuration
    if args.config:
        config = load_config_from_file(args.config)
    else:
        config = create_default_config(args.target, args.auth_token, args.test_type)

    # Run LLM exploitation tests
    try:
        results = asyncio.run(run_llm_exploitation_tests(config))

        # Display results
        if not args.quiet:
            display_test_results(results)
        else:
            # Print minimal results for quiet mode
            report = results.get("fenrir_master_report", {})
            exec_summary = report.get("executive_summary", {})
            print(f"Risk Score: {exec_summary.get('risk_score', 0)}/100")
            print(
                f"Critical Vulnerabilities: {exec_summary.get('critical_findings', 0)}"
            )
            print(f"Success Rate: {exec_summary.get('success_rate', 0)}%")

            if "report_path" in results:
                print(f"Report: {results['report_path']}")

    except KeyboardInterrupt:
        print("\n🐺 Hunt interrupted by user. Exiting...")
        sys.exit(1)

    except Exception as e:
        logger.error(f"🐺 LLM exploitation testing failed: {e}")
        print(f"\n❌ Testing failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
