"""ðŸº Machine Learning Model Fuzzing Engine

*bares fangs with savage satisfaction* Specialized fuzzing engine for ML model endpoints.
Targets AI/ML specific vulnerabilities including parameter manipulation, adversarial inputs,
and resource exhaustion attacks.

Classes:
    MLFuzzer: ML model-specific fuzzing with AI attack vectors
"""

import asyncio
import random
import time
from typing import Any

import httpx

from ...core.analysis import VulnerabilityAnalyzer
from ...core.base_fuzzer import BaseFuzzer
from ...core.results import MLFuzzResult


class MLFuzzer(BaseFuzzer):
    """ðŸº Machine Learning Model Fuzzing Engine

    *snarls with predatory intelligence* Specialized fuzzing engine for
    machine learning model endpoints and AI/ML services. Targets ML-specific
    vulnerabilities including parameter manipulation, adversarial inputs,
    and resource exhaustion attacks.

    This fuzzer specializes in:
    - Parameter manipulation to break model behavior
    - Adversarial inputs to cause model failures
    - Resource exhaustion through oversized requests
    - Model poisoning attempts
    - Output manipulation and data corruption

    Attack Types:
    - Parameter Manipulation: Invalid model parameters
    - Adversarial Inputs: Malicious input data
    - Resource Exhaustion: Memory and CPU exhaustion
    - Model Poisoning: Training data manipulation
    - Output Manipulation: Response corruption

    Attributes:
        ml_attack_vectors (Dict[str, List[Dict[str, Any]]]): ML-specific attack vectors
        analyzer (VulnerabilityAnalyzer): Vulnerability detection engine

    Example:
        >>> fuzzer = MLFuzzer()
        >>> results = await fuzzer.fuzz_ml_endpoint("/api/embedding-visualization/reduce", "POST")
        >>> # Tests ML endpoint with parameter manipulation and adversarial inputs

    """

    def __init__(
        self, base_url: str = "http://localhost:8000", max_concurrent: int = 5,
    ):
        """Initialize the ML fuzzer.

        *whiskers twitch with intelligence* Sets up ML attack vectors
        and vulnerability analyzer for AI/ML specific testing.

        Args:
            base_url (str): Base URL for ML endpoints
            max_concurrent (int): Maximum concurrent requests

        """
        super().__init__(base_url, max_concurrent)
        self.analyzer = VulnerabilityAnalyzer()
        self.ml_attack_vectors = self._initialize_ml_attack_vectors()

    def _initialize_ml_attack_vectors(self) -> dict[str, list[dict[str, Any]]]:
        """Initialize ML-specific attack vectors.

        *bares fangs with savage satisfaction* Creates comprehensive
        ML-specific attack vectors for different vulnerability types.

        Returns:
            Dict[str, List[Dict[str, Any]]]: ML attack vectors organized by type

        """
        return {
            "parameter_manipulation": [
                {"temperature": -1.0},
                {"temperature": 0.0},
                {"temperature": 100.0},
                {"temperature": float("inf")},
                {"top_p": -0.1},
                {"top_p": 1.1},
                {"top_k": -1},
                {"top_k": 1000000},
                {"max_length": -1},
                {"max_length": 100000},
            ],
            "adversarial_inputs": [
                {"text": "A" * 100000},
                {"text": "æµ‹è¯•" * 10000},
                {"text": "ðŸš€ðŸ’€ðŸ”¥" * 1000},
                {"text": "!@#$%^&*()_+-=[]{}|;':\",./<>?" * 1000},
                {"text": "SELECT * FROM users WHERE id = '1' OR '1'='1'"},
                {"text": "<script>alert('XSS')</script>"},
                {"text": "; ls -la"},
                {"text": "| whoami"},
                {"text": "` cat /etc/passwd `"},
                {"text": "$(whoami)"},
            ],
            "resource_exhaustion": [
                {
                    "embeddings": [
                        [random.random() for _ in range(10000)] for _ in range(100)
                    ],
                },
                {"parameters": {"n_components": 100000}},
                {"parameters": {"perplexity": 100000}},
                {"max_samples": 1000000},
                {"max_samples": -1},
            ],
        }

    async def fuzz_endpoint(
        self,
        endpoint: str,
        method: str = "POST",
        attack_types: list[str] | None = None,
        **kwargs,
    ) -> list[MLFuzzResult]:
        """Fuzz ML endpoint with specialized attacks.

        *alpha wolf dominance radiates* Performs comprehensive ML
        fuzzing using multiple attack vectors and techniques.

        Args:
            endpoint (str): ML endpoint to fuzz
            method (str): HTTP method to use
            attack_types (Optional[List[str]]): Types of attacks to perform
            **kwargs: Additional fuzzing parameters

        Returns:
            List[MLFuzzResult]: Results from ML fuzzing

        """
        if attack_types is None:
            attack_types = list(self.ml_attack_vectors.keys())

        url = f"{self.base_url}{endpoint}"
        results = []

        for attack_type in attack_types:
            attack_payloads = self.ml_attack_vectors.get(attack_type, [])

            for payload in attack_payloads:
                try:
                    result = await self._send_ml_request(
                        url, method, payload, attack_type,
                    )
                    results.append(result)

                    # Small delay between requests
                    await asyncio.sleep(0.1)

                except Exception as e:
                    # Create error result
                    error_result = MLFuzzResult(
                        url=url,
                        method=method,
                        attack_type=attack_type,
                        payload=payload,
                        status_code=0,
                        response_time=0.0,
                        vulnerability_detected=True,
                        vulnerability_type="Exception",
                        model_response={"error": str(e)},
                    )
                    results.append(error_result)

        # Add results to collection
        self.results.extend(results)
        return results

    async def _send_ml_request(
        self, url: str, method: str, payload: dict[str, Any], attack_type: str,
    ) -> MLFuzzResult:
        """Send ML fuzzing request.

        *snarls with predatory glee* Creates realistic ML request
        with embedded attack and sends to target endpoint.

        Args:
            url (str): Target URL
            method (str): HTTP method
            payload (Dict[str, Any]): ML attack payload
            attack_type (str): Type of attack being performed

        Returns:
            MLFuzzResult: Result of the ML fuzzing attempt

        """
        start_time = time.time()

        try:
            # Create realistic ML request with embedded attack
            ml_request = self._create_ml_request(payload, attack_type)

            if method.upper() == "GET":
                response = await self.send_request(url, method="GET", params=ml_request)
            else:
                response = await self.send_request(
                    url,
                    method=method.upper(),
                    json=ml_request,
                    headers={"Content-Type": "application/json"},
                )

            response_time = time.time() - start_time

            # Parse model response
            model_response = None
            try:
                model_response = response.json()
            except:
                model_response = {"raw_response": response.text}

            # Detect vulnerabilities
            vulnerability_detected, vuln_type = self.analyzer.analyze_ml_response(
                response, model_response, payload, attack_type,
            )

            # Check for resource exhaustion
            resource_exhaustion = self.analyzer.detect_resource_exhaustion(
                response_time, model_response, attack_type,
            )

            return MLFuzzResult(
                url=url,
                method=method,
                attack_type=attack_type,
                payload=payload,
                status_code=response.status_code,
                response_time=response_time,
                model_response=model_response,
                vulnerability_detected=vulnerability_detected,
                vulnerability_type=vuln_type,
                resource_exhaustion=resource_exhaustion,
            )

        except Exception as e:
            response_time = time.time() - start_time
            return MLFuzzResult(
                url=url,
                method=method,
                attack_type=attack_type,
                payload=payload,
                status_code=0,
                response_time=response_time,
                vulnerability_detected=True,
                vulnerability_type="Exception",
                model_response={"error": str(e)},
            )

    def _create_ml_request(
        self, attack_payload: dict[str, Any], attack_type: str,
    ) -> dict[str, Any]:
        """Create realistic ML request with embedded attack.

        *bares fangs with cunning* Generates realistic-looking ML
        requests that hide malicious intent within normal-looking parameters.

        Args:
            attack_payload (Dict[str, Any]): The malicious attack payload
            attack_type (str): Type of ML attack being performed

        Returns:
            Dict[str, Any]: Realistic ML request with embedded attack

        """
        base_request = {
            "method": "pca",
            "filters": {},
            "parameters": {"n_components": 2, "random_state": 42},
            "max_samples": 1000,
            "use_cache": True,
            "random_seed": 42,
        }

        # Embed attack based on type
        if attack_type == "parameter_manipulation":
            base_request.update(attack_payload)
        elif attack_type == "adversarial_inputs":
            base_request["text"] = attack_payload["text"]
        elif attack_type == "resource_exhaustion":
            base_request.update(attack_payload)

        return base_request

    async def fuzz_model_poisoning(
        self, endpoint: str, method: str = "POST",
    ) -> list[MLFuzzResult]:
        """Perform model poisoning attacks.

        *circles with menacing intent* Attempts to poison ML models
        through malicious training data and parameter manipulation.

        Args:
            endpoint (str): ML endpoint to target
            method (str): HTTP method to use

        Returns:
            List[MLFuzzResult]: Results from model poisoning attempts

        """
        url = f"{self.base_url}{endpoint}"
        results = []

        # Model poisoning payloads
        poisoning_payloads = [
            {
                "training_data": [{"input": "normal", "output": "malicious"}],
                "model_parameters": {"learning_rate": 0.0},
            },
            {
                "training_data": [
                    {"input": "test", "output": "<script>alert('XSS')</script>"},
                ],
                "model_parameters": {"epochs": -1},
            },
            {
                "training_data": [{"input": "admin", "output": "root"}],
                "model_parameters": {"batch_size": 0},
            },
        ]

        for payload in poisoning_payloads:
            try:
                result = await self._send_ml_request(
                    url, method, payload, "model_poisoning",
                )
                results.append(result)
                await asyncio.sleep(0.1)
            except Exception as e:
                error_result = MLFuzzResult(
                    url=url,
                    method=method,
                    attack_type="model_poisoning",
                    payload=payload,
                    status_code=0,
                    response_time=0.0,
                    vulnerability_detected=True,
                    vulnerability_type="Exception",
                    model_response={"error": str(e)},
                )
                results.append(error_result)

        self.results.extend(results)
        return results

    def _analyze_response(
        self, response: httpx.Response, request_kwargs: dict,
    ) -> tuple[bool, str | None]:
        """Analyze response for ML-specific vulnerabilities.

        *fox intelligence analyzes* Checks for ML-specific vulnerabilities
        like model poisoning, data leakage, or inference attacks.

        Args:
            response: HTTP response to analyze
            request_kwargs: Original request parameters

        Returns:
            Tuple of (is_vulnerable, vulnerability_description)

        """
        try:
            # Check for ML-specific error responses
            if response.status_code in [400, 422, 500]:
                content = response.text.lower()

                # Model-related errors
                if any(
                    indicator in content
                    for indicator in [
                        "model",
                        "inference",
                        "prediction",
                        "tensor",
                        "neural",
                        "training",
                        "validation",
                        "accuracy",
                        "loss",
                    ]
                ):
                    return (
                        True,
                        f"ML model error (HTTP {response.status_code}) - potential vulnerability",
                    )

            # Check for successful responses that might leak model information
            elif response.status_code == 200:
                content = response.text.lower()

                # Check for model information leakage
                if any(
                    indicator in content
                    for indicator in [
                        "model_name",
                        "model_version",
                        "architecture",
                        "weights",
                        "parameters",
                        "training_data",
                        "dataset",
                    ]
                ):
                    return True, "ML model information leakage detected"

                # Check for inference results that might reveal model internals
                if any(
                    indicator in content
                    for indicator in [
                        "confidence",
                        "probability",
                        "logits",
                        "embeddings",
                        "feature_vector",
                        "activation",
                    ]
                ):
                    return True, "ML model internals exposed - potential data leakage"

            return False, None

        except Exception:
            return False, None
