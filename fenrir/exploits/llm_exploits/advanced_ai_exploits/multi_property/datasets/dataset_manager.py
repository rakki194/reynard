"""ğŸ¦Š Dataset Manager: Multi-Domain Dataset Management for Property Inference

Manages diverse datasets from multiple domains for comprehensive property inference attacks.
Supports news articles, medical data, financial information, legal documents, and more.
"""

import json
import logging
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import pandas as pd

from ..core.property_types import DomainType

logger = logging.getLogger(__name__)


@dataclass
class DatasetInfo:
    """Information about a dataset."""

    name: str
    domain: DomainType
    source: str  # "huggingface", "local", "synthetic"
    path: str
    size: int
    properties: list[str]
    description: str
    license: str = "unknown"
    download_url: str | None = None


class DatasetManager:
    """ğŸ¦Š Dataset Manager: Comprehensive dataset management for multi-property inference.

    Manages datasets from multiple domains including:
    - News articles (CNN, Reuters, BBC, etc.)
    - Medical data (patient records, clinical notes)
    - Financial data (market reports, company filings)
    - Legal documents (case law, regulations)
    - Technical documentation
    """

    def __init__(self, data_dir: str = "data/multi_property_datasets"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.datasets: dict[str, DatasetInfo] = {}
        self.loaded_datasets: dict[str, list[dict]] = {}

        # Initialize with known datasets
        self._initialize_known_datasets()

        logger.info("ğŸ¦Š Dataset Manager initialized with multi-domain support")

    def _initialize_known_datasets(self):
        """Initialize known datasets for property inference."""
        # News datasets
        self.datasets["cnn_news"] = DatasetInfo(
            name="CNN News Articles",
            domain=DomainType.NEWS,
            source="huggingface",
            path="cnn_dailymail",
            size=1000000,
            properties=["news_source", "sentiment_bias", "temporal_period"],
            description="CNN news articles for copyright and source inference",
            license="Apache 2.0",
        )

        self.datasets["reuters_news"] = DatasetInfo(
            name="Reuters News",
            domain=DomainType.NEWS,
            source="huggingface",
            path="reuters_news",
            size=500000,
            properties=["news_source", "geographic_region", "industry_focus"],
            description="Reuters news articles for business and geographic inference",
            license="Apache 2.0",
        )

        # Medical datasets
        self.datasets["medical_qa"] = DatasetInfo(
            name="Medical Q&A",
            domain=DomainType.MEDICAL,
            source="huggingface",
            path="medical_questions_pairs",
            size=100000,
            properties=[
                "gender_distribution",
                "age_demographics",
                "disease_prevalence",
            ],
            description="Medical question-answer pairs for demographic inference",
            license="MIT",
        )

        self.datasets["clinical_notes"] = DatasetInfo(
            name="Clinical Notes",
            domain=DomainType.MEDICAL,
            source="huggingface",
            path="clinical_notes",
            size=50000,
            properties=["specialty_focus", "treatment_approach", "patient_volume"],
            description="Clinical notes for medical specialty inference",
            license="MIT",
        )

        # Financial datasets
        self.datasets["financial_reports"] = DatasetInfo(
            name="Financial Reports",
            domain=DomainType.FINANCIAL,
            source="huggingface",
            path="financial_reports",
            size=100000,
            properties=["industry_focus", "company_size", "revenue_range"],
            description="Financial reports for business property inference",
            license="MIT",
        )

        # Legal datasets
        self.datasets["legal_cases"] = DatasetInfo(
            name="Legal Cases",
            domain=DomainType.LEGAL,
            source="huggingface",
            path="legal_case_law",
            size=200000,
            properties=["jurisdiction", "case_type", "court_level"],
            description="Legal case law for jurisdiction and case type inference",
            license="MIT",
        )

        # Technical datasets
        self.datasets["technical_docs"] = DatasetInfo(
            name="Technical Documentation",
            domain=DomainType.TECHNICAL,
            source="huggingface",
            path="technical_documentation",
            size=50000,
            properties=["technical_complexity", "formality_level", "readability_score"],
            description="Technical documentation for complexity inference",
            license="MIT",
        )

    def get_available_datasets(
        self, domain: DomainType | None = None,
    ) -> list[DatasetInfo]:
        """Get list of available datasets, optionally filtered by domain."""
        if domain is None:
            return list(self.datasets.values())

        return [
            dataset for dataset in self.datasets.values() if dataset.domain == domain
        ]

    def load_dataset(
        self, dataset_name: str, sample_size: int | None = None,
    ) -> list[dict]:
        """Load a dataset for property inference.

        Args:
            dataset_name: Name of the dataset to load
            sample_size: Optional sample size to limit loading

        """
        if dataset_name not in self.datasets:
            raise ValueError(f"Unknown dataset: {dataset_name}")

        dataset_info = self.datasets[dataset_name]

        # Check if already loaded
        if dataset_name in self.loaded_datasets:
            data = self.loaded_datasets[dataset_name]
            if sample_size:
                return random.sample(data, min(sample_size, len(data)))
            return data

        # Load dataset based on source
        if dataset_info.source == "huggingface":
            data = self._load_huggingface_dataset(dataset_info, sample_size)
        elif dataset_info.source == "local":
            data = self._load_local_dataset(dataset_info, sample_size)
        elif dataset_info.source == "synthetic":
            data = self._generate_synthetic_dataset(dataset_info, sample_size)
        else:
            raise ValueError(f"Unsupported dataset source: {dataset_info.source}")

        # Cache loaded dataset
        self.loaded_datasets[dataset_name] = data

        logger.info(f"ğŸ¦Š Loaded dataset {dataset_name}: {len(data)} samples")
        return data

    def _load_huggingface_dataset(
        self, dataset_info: DatasetInfo, sample_size: int | None,
    ) -> list[dict]:
        """Load dataset from HuggingFace Hub."""
        try:
            from datasets import load_dataset

            # Load dataset
            dataset = load_dataset(dataset_info.path, split="train")

            # Convert to list of dictionaries
            data = []
            for item in dataset:
                data.append(dict(item))

            # Sample if requested
            if sample_size and len(data) > sample_size:
                data = random.sample(data, sample_size)

            return data

        except Exception as e:
            logger.warning(
                f"âš ï¸ Failed to load HuggingFace dataset {dataset_info.path}: {e}",
            )
            # Fallback to synthetic generation
            return self._generate_synthetic_dataset(dataset_info, sample_size)

    def _load_local_dataset(
        self, dataset_info: DatasetInfo, sample_size: int | None,
    ) -> list[dict]:
        """Load dataset from local files."""
        dataset_path = self.data_dir / dataset_info.path

        if not dataset_path.exists():
            raise FileNotFoundError(f"Local dataset not found: {dataset_path}")

        # Load based on file extension
        if dataset_path.suffix == ".json":
            with open(dataset_path) as f:
                data = json.load(f)
        elif dataset_path.suffix == ".csv":
            df = pd.read_csv(dataset_path)
            data = df.to_dict("records")
        else:
            raise ValueError(f"Unsupported file format: {dataset_path.suffix}")

        # Sample if requested
        if sample_size and len(data) > sample_size:
            data = random.sample(data, sample_size)

        return data

    def _generate_synthetic_dataset(
        self, dataset_info: DatasetInfo, sample_size: int | None,
    ) -> list[dict]:
        """Generate synthetic dataset for testing."""
        size = sample_size or min(1000, dataset_info.size)

        if dataset_info.domain == DomainType.NEWS:
            return self._generate_news_dataset(size, dataset_info.properties)
        if dataset_info.domain == DomainType.MEDICAL:
            return self._generate_medical_dataset(size, dataset_info.properties)
        if dataset_info.domain == DomainType.FINANCIAL:
            return self._generate_financial_dataset(size, dataset_info.properties)
        if dataset_info.domain == DomainType.LEGAL:
            return self._generate_legal_dataset(size, dataset_info.properties)
        if dataset_info.domain == DomainType.TECHNICAL:
            return self._generate_technical_dataset(size, dataset_info.properties)
        return self._generate_generic_dataset(size)

    def _generate_news_dataset(self, size: int, properties: list[str]) -> list[dict]:
        """Generate synthetic news dataset."""
        news_sources = [
            "CNN",
            "Reuters",
            "BBC",
            "New York Times",
            "Wall Street Journal",
        ]
        topics = ["politics", "business", "technology", "health", "sports"]
        sentiments = ["positive", "negative", "neutral"]

        data = []
        for i in range(size):
            source = random.choice(news_sources)
            topic = random.choice(topics)
            sentiment = random.choice(sentiments)

            article = {
                "id": f"news_{i}",
                "title": f"{topic.title()} News Update",
                "content": f"This is a {sentiment} news article about {topic} from {source}.",
                "source": source,
                "topic": topic,
                "sentiment": sentiment,
                "timestamp": f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}",
            }

            data.append(article)

        return data

    def _generate_medical_dataset(self, size: int, properties: list[str]) -> list[dict]:
        """Generate synthetic medical dataset."""
        genders = ["male", "female"]
        ages = ["pediatric", "adolescent", "adult", "elderly"]
        conditions = ["diabetes", "hypertension", "cancer", "heart_disease", "asthma"]

        data = []
        for i in range(size):
            gender = random.choice(genders)
            age = random.choice(ages)
            condition = random.choice(conditions)

            record = {
                "id": f"medical_{i}",
                "question": f"What are the symptoms of {condition}?",
                "answer": "Common symptoms include various indicators that may vary by patient demographics.",
                "patient_gender": gender,
                "patient_age": age,
                "condition": condition,
                "specialty": "general_medicine",
            }

            data.append(record)

        return data

    def _generate_financial_dataset(
        self, size: int, properties: list[str],
    ) -> list[dict]:
        """Generate synthetic financial dataset."""
        industries = ["technology", "healthcare", "finance", "manufacturing"]
        company_sizes = ["startup", "small", "medium", "large", "enterprise"]
        revenue_ranges = ["<1M", "1M-10M", "10M-100M", "100M-1B", ">1B"]

        data = []
        for i in range(size):
            industry = random.choice(industries)
            size = random.choice(company_sizes)
            revenue = random.choice(revenue_ranges)

            report = {
                "id": f"financial_{i}",
                "title": f"{industry.title()} Company Analysis",
                "content": f"Analysis of a {size} {industry} company with revenue in the {revenue} range.",
                "industry": industry,
                "company_size": size,
                "revenue_range": revenue,
                "sector": "private",
            }

            data.append(report)

        return data

    def _generate_legal_dataset(self, size: int, properties: list[str]) -> list[dict]:
        """Generate synthetic legal dataset."""
        jurisdictions = ["federal", "state", "local"]
        case_types = ["civil", "criminal", "administrative", "constitutional"]
        courts = ["supreme", "appellate", "district", "municipal"]

        data = []
        for i in range(size):
            jurisdiction = random.choice(jurisdictions)
            case_type = random.choice(case_types)
            court = random.choice(courts)

            case = {
                "id": f"legal_{i}",
                "title": f"{case_type.title()} Case Summary",
                "content": f"This {case_type} case was heard in {court} court under {jurisdiction} jurisdiction.",
                "jurisdiction": jurisdiction,
                "case_type": case_type,
                "court_level": court,
                "year": random.randint(2010, 2024),
            }

            data.append(case)

        return data

    def _generate_technical_dataset(
        self, size: int, properties: list[str],
    ) -> list[dict]:
        """Generate synthetic technical dataset."""
        complexities = ["beginner", "intermediate", "advanced", "expert"]
        formality_levels = ["casual", "professional", "academic", "formal"]
        topics = ["programming", "algorithms", "databases", "networking", "security"]

        data = []
        for i in range(size):
            complexity = random.choice(complexities)
            formality = random.choice(formality_levels)
            topic = random.choice(topics)

            doc = {
                "id": f"technical_{i}",
                "title": f"{topic.title()} Documentation",
                "content": f"This {formality} documentation covers {complexity} level {topic} concepts.",
                "complexity": complexity,
                "formality": formality,
                "topic": topic,
                "readability_score": random.uniform(0.3, 0.9),
            }

            data.append(doc)

        return data

    def _generate_generic_dataset(self, size: int) -> list[dict]:
        """Generate generic dataset."""
        data = []
        for i in range(size):
            item = {
                "id": f"generic_{i}",
                "content": f"This is a generic data item number {i}.",
                "category": "general",
                "value": random.uniform(0, 100),
            }
            data.append(item)

        return data

    def create_property_dataset(
        self,
        base_dataset: str,
        target_properties: list[str],
        property_ratios: dict[str, float],
        sample_size: int = 1000,
    ) -> list[dict]:
        """Create a dataset with specific property ratios for shadow model training.

        Args:
            base_dataset: Base dataset to use
            target_properties: Properties to control
            property_ratios: Desired ratios for each property
            sample_size: Size of the generated dataset

        """
        # Load base dataset
        base_data = self.load_dataset(
            base_dataset, sample_size * 2,
        )  # Load extra for filtering

        # Filter data to achieve target property ratios
        filtered_data = self._filter_data_by_properties(
            base_data, target_properties, property_ratios, sample_size,
        )

        logger.info(f"ğŸ¦Š Created property dataset with {len(filtered_data)} samples")
        return filtered_data

    def _filter_data_by_properties(
        self,
        data: list[dict],
        properties: list[str],
        ratios: dict[str, float],
        target_size: int,
    ) -> list[dict]:
        """Filter data to achieve specific property ratios."""
        # This is a simplified implementation
        # In practice, you would implement more sophisticated filtering logic

        filtered_data = []
        for item in data:
            # Check if item matches property criteria
            matches = True
            for prop, ratio in ratios.items():
                # Simple property matching logic
                if prop in item:
                    # Add more sophisticated matching logic here
                    pass

            if matches:
                filtered_data.append(item)
                if len(filtered_data) >= target_size:
                    break

        return filtered_data

    def get_dataset_statistics(self, dataset_name: str) -> dict[str, Any]:
        """Get statistics about a dataset."""
        if dataset_name not in self.datasets:
            raise ValueError(f"Unknown dataset: {dataset_name}")

        dataset_info = self.datasets[dataset_name]

        stats = {
            "name": dataset_info.name,
            "domain": dataset_info.domain.value,
            "size": dataset_info.size,
            "properties": dataset_info.properties,
            "source": dataset_info.source,
            "loaded": dataset_name in self.loaded_datasets,
        }

        if dataset_name in self.loaded_datasets:
            data = self.loaded_datasets[dataset_name]
            stats["actual_size"] = len(data)
            stats["sample_keys"] = list(data[0].keys()) if data else []

        return stats

    def export_dataset(self, dataset_name: str, filepath: str):
        """Export a dataset to a file."""
        if dataset_name not in self.loaded_datasets:
            raise ValueError(f"Dataset not loaded: {dataset_name}")

        data = self.loaded_datasets[dataset_name]

        if filepath.endswith(".json"):
            with open(filepath, "w") as f:
                json.dump(data, f, indent=2)
        elif filepath.endswith(".csv"):
            df = pd.DataFrame(data)
            df.to_csv(filepath, index=False)
        else:
            raise ValueError("Unsupported export format. Use .json or .csv")

        logger.info(f"ğŸ¦Š Exported dataset {dataset_name} to {filepath}")

    def clear_cache(self):
        """Clear loaded dataset cache."""
        self.loaded_datasets.clear()
        logger.info("ğŸ¦Š Dataset cache cleared")
