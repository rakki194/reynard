""" PROMPT Library: Centralized Prompt Generation for Multi-Property Inference

PROMPT = Property Retrieval and Observation for Multi-Property Targeting

This library provides systematic prompt generation for property inference attacks
across diverse domains and property types.
"""

import logging
import random
from typing import Any

from .prompt_generators import (
    FinancialPromptGenerator,
    LegalPromptGenerator,
    MedicalPromptGenerator,
    NewsPromptGenerator,
    TechnicalPromptGenerator,
)

logger = logging.getLogger(__name__)


class PROMPTLibrary:
    """ PROMPT Library: Centralized prompt generation for property inference.

    Provides systematic prompt generation across multiple domains and property types
    for comprehensive multi-property inference attacks.
    """

    def __init__(self):
        self.generators = {
            "news": NewsPromptGenerator(),
            "medical": MedicalPromptGenerator(),
            "financial": FinancialPromptGenerator(),
            "legal": LegalPromptGenerator(),
            "technical": TechnicalPromptGenerator(),
        }

        self.prompt_cache: dict[str, list[str]] = {}
        self.effectiveness_scores: dict[str, float] = {}

        logger.info(" PROMPT Library initialized with multi-domain generators")

    def generate_property_prompts(
        self,
        property_types: list[str],
        domain: str,
        count: int = 10,
        strategy: str = "diverse",
    ) -> list[str]:
        """Generate prompts for inferring specific properties.

        Args:
            property_types: List of property types to target
            domain: Domain type (news, medical, financial, etc.)
            count: Number of prompts to generate
            strategy: Generation strategy ("diverse", "focused", "adversarial")

        """
        if domain not in self.generators:
            raise ValueError(f"Unsupported domain: {domain}")

        generator = self.generators[domain]

        # Generate prompts based on strategy
        if strategy == "diverse":
            prompts = generator.generate_diverse_prompts(property_types, count)
        elif strategy == "focused":
            prompts = generator.generate_focused_prompts(property_types, count)
        elif strategy == "adversarial":
            prompts = generator.generate_adversarial_prompts(property_types, count)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

        # Cache prompts for reuse
        cache_key = f"{domain}_{'_'.join(property_types)}_{strategy}"
        self.prompt_cache[cache_key] = prompts

        logger.info(
            f" Generated {len(prompts)} {strategy} prompts for {domain} domain",
        )
        return prompts

    def generate_multi_property_prompts(
        self,
        property_combinations: list[list[str]],
        domain: str,
        count_per_combination: int = 5,
    ) -> dict[str, list[str]]:
        """Generate prompts for multi-property inference.

        Args:
            property_combinations: List of property type combinations
            domain: Domain type
            count_per_combination: Number of prompts per combination

        """
        multi_prompts = {}

        for combination in property_combinations:
            combination_key = "_".join(combination)

            # Generate prompts that target multiple properties simultaneously
            prompts = self._generate_combination_prompts(
                combination,
                domain,
                count_per_combination,
            )

            multi_prompts[combination_key] = prompts

            logger.info(
                f" Generated {len(prompts)} multi-property prompts for {combination_key}",
            )

        return multi_prompts

    def _generate_combination_prompts(
        self,
        properties: list[str],
        domain: str,
        count: int,
    ) -> list[str]:
        """Generate prompts that target multiple properties simultaneously."""
        if domain not in self.generators:
            raise ValueError(f"Unsupported domain: {domain}")

        generator = self.generators[domain]
        return generator.generate_multi_property_prompts(properties, count)

    def get_domain_specific_prompts(
        self,
        domain: str,
        category: str = "general",
        count: int = 20,
    ) -> list[str]:
        """Get domain-specific prompts for general property inference."""
        if domain not in self.generators:
            raise ValueError(f"Unsupported domain: {domain}")

        generator = self.generators[domain]
        return generator.get_category_prompts(category, count)

    def optimize_prompts(
        self,
        prompts: list[str],
        target_properties: list[str],
        feedback_scores: list[float],
    ) -> list[str]:
        """Optimize prompts based on effectiveness feedback.

        Args:
            prompts: Original prompts
            target_properties: Properties being targeted
            feedback_scores: Effectiveness scores for each prompt

        """
        if len(prompts) != len(feedback_scores):
            raise ValueError("Prompts and feedback scores must have same length")

        # Sort prompts by effectiveness
        prompt_scores = list(zip(prompts, feedback_scores, strict=False))
        prompt_scores.sort(key=lambda x: x[1], reverse=True)

        # Keep top 70% of prompts and generate variations
        top_count = max(1, int(len(prompts) * 0.7))
        top_prompts = [prompt for prompt, _ in prompt_scores[:top_count]]

        # Generate variations of top prompts
        optimized_prompts = []
        for prompt in top_prompts:
            variations = self._generate_prompt_variations(prompt)
            optimized_prompts.extend(variations)

        # Add some random new prompts for diversity
        new_prompts = self._generate_random_prompts(
            target_properties,
            len(prompts) - len(optimized_prompts),
        )
        optimized_prompts.extend(new_prompts)

        logger.info(
            f" Optimized {len(prompts)} prompts to {len(optimized_prompts)} optimized prompts",
        )
        return optimized_prompts[: len(prompts)]

    def _generate_prompt_variations(self, base_prompt: str) -> list[str]:
        """Generate variations of a base prompt."""
        variations = [base_prompt]  # Include original

        # Simple variations
        if "?" in base_prompt:
            variations.append(base_prompt.replace("?", "."))
        if "." in base_prompt:
            variations.append(base_prompt.replace(".", "?"))

        # Add context variations
        context_prefixes = [
            "In your experience, ",
            "Based on your training, ",
            "From your knowledge, ",
            "According to your data, ",
        ]

        for prefix in context_prefixes[:2]:  # Limit variations
            if not base_prompt.startswith(prefix):
                variations.append(prefix + base_prompt.lower())

        return variations

    def _generate_random_prompts(self, properties: list[str], count: int) -> list[str]:
        """Generate random prompts for diversity."""
        random_prompts = []

        base_templates = [
            "What can you tell me about {property}?",
            "Describe the characteristics of {property}.",
            "How would you analyze {property}?",
            "What patterns do you see in {property}?",
            "Explain the distribution of {property}.",
        ]

        for _ in range(count):
            template = random.choice(base_templates)
            property_name = random.choice(properties)
            prompt = template.format(property=property_name)
            random_prompts.append(prompt)

        return random_prompts

    def get_prompt_statistics(self) -> dict[str, Any]:
        """Get statistics about prompt generation and effectiveness."""
        total_prompts = sum(len(prompts) for prompts in self.prompt_cache.values())

        return {
            "total_cached_prompts": total_prompts,
            "cached_combinations": len(self.prompt_cache),
            "available_domains": list(self.generators.keys()),
            "effectiveness_scores": self.effectiveness_scores,
            "cache_keys": list(self.prompt_cache.keys()),
        }

    def clear_cache(self):
        """Clear the prompt cache."""
        self.prompt_cache.clear()
        self.effectiveness_scores.clear()
        logger.info(" PROMPT Library cache cleared")

    def export_prompts(self, filepath: str):
        """Export all cached prompts to a file."""
        import json

        export_data = {
            "prompts": self.prompt_cache,
            "effectiveness_scores": self.effectiveness_scores,
            "statistics": self.get_prompt_statistics(),
        }

        with open(filepath, "w") as f:
            json.dump(export_data, f, indent=2)

        logger.info(f" Exported prompts to {filepath}")

    def import_prompts(self, filepath: str):
        """Import prompts from a file."""
        import json

        with open(filepath) as f:
            import_data = json.load(f)

        self.prompt_cache = import_data.get("prompts", {})
        self.effectiveness_scores = import_data.get("effectiveness_scores", {})

        logger.info(f" Imported prompts from {filepath}")


# Global PROMPT Library instance
prompt_library = PROMPTLibrary()
