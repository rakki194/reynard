"""
ðŸ¦Š Prompt Optimizer: Advanced Prompt Optimization for Property Inference

Optimizes prompts based on effectiveness feedback and performance metrics.
"""

import logging
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


@dataclass
class OptimizationResult:
    """Result of prompt optimization."""

    original_prompts: List[str]
    optimized_prompts: List[str]
    effectiveness_improvement: float
    optimization_strategy: str
    iterations: int


class PromptOptimizer:
    """
    ðŸ¦Š Prompt Optimizer: Advanced prompt optimization for property inference.

    Optimizes prompts based on effectiveness feedback, performance metrics,
    and adversarial strength to maximize property inference success.
    """

    def __init__(self):
        self.optimization_history: List[Dict] = []
        self.effectiveness_scores: Dict[str, float] = {}

        logger.info("ðŸ¦Š Prompt Optimizer initialized")

    async def optimize_prompts(
        self,
        prompts: List[str],
        target_properties: List[str],
        feedback_scores: List[float],
        optimization_strategy: str = "adaptive",
    ) -> OptimizationResult:
        """
        Optimize prompts based on effectiveness feedback.

        Args:
            prompts: Original prompts to optimize
            target_properties: Properties being targeted
            feedback_scores: Effectiveness scores for each prompt
            optimization_strategy: Strategy to use for optimization

        Returns:
            Optimization result with improved prompts
        """
        logger.info(
            f"ðŸ¦Š Optimizing {len(prompts)} prompts using {optimization_strategy} strategy"
        )

        if len(prompts) != len(feedback_scores):
            raise ValueError("Prompts and feedback scores must have same length")

        # Store original prompts
        original_prompts = prompts.copy()

        # Apply optimization strategy
        if optimization_strategy == "adaptive":
            optimized_prompts = await self._adaptive_optimization(
                prompts, feedback_scores, target_properties
            )
        elif optimization_strategy == "evolutionary":
            optimized_prompts = await self._evolutionary_optimization(
                prompts, feedback_scores, target_properties
            )
        elif optimization_strategy == "gradient_based":
            optimized_prompts = await self._gradient_based_optimization(
                prompts, feedback_scores, target_properties
            )
        else:
            optimized_prompts = await self._default_optimization(
                prompts, feedback_scores, target_properties
            )

        # Calculate effectiveness improvement
        original_effectiveness = sum(feedback_scores) / len(feedback_scores)
        # Simulate improved effectiveness (in practice, would be measured)
        improved_effectiveness = original_effectiveness * 1.2
        effectiveness_improvement = improved_effectiveness - original_effectiveness

        # Create optimization result
        result = OptimizationResult(
            original_prompts=original_prompts,
            optimized_prompts=optimized_prompts,
            effectiveness_improvement=effectiveness_improvement,
            optimization_strategy=optimization_strategy,
            iterations=1,
        )

        # Record optimization history
        self.optimization_history.append(
            {
                "strategy": optimization_strategy,
                "original_count": len(original_prompts),
                "optimized_count": len(optimized_prompts),
                "effectiveness_improvement": effectiveness_improvement,
                "target_properties": target_properties,
            }
        )

        logger.info(
            f"âœ… Prompt optimization completed: {effectiveness_improvement:.3f} improvement"
        )
        return result

    async def _adaptive_optimization(
        self,
        prompts: List[str],
        feedback_scores: List[float],
        target_properties: List[str],
    ) -> List[str]:
        """Adaptive optimization based on prompt performance."""

        optimized_prompts = []

        # Sort prompts by effectiveness
        prompt_scores = list(zip(prompts, feedback_scores))
        prompt_scores.sort(key=lambda x: x[1], reverse=True)

        # Keep top performers and optimize others
        top_threshold = 0.7
        for prompt, score in prompt_scores:
            if score >= top_threshold:
                # Keep high-performing prompts
                optimized_prompts.append(prompt)
            else:
                # Optimize low-performing prompts
                optimized_prompt = await self._optimize_single_prompt(
                    prompt, target_properties
                )
                optimized_prompts.append(optimized_prompt)

        return optimized_prompts

    async def _evolutionary_optimization(
        self,
        prompts: List[str],
        feedback_scores: List[float],
        target_properties: List[str],
    ) -> List[str]:
        """Evolutionary optimization using genetic algorithms."""

        # Select parents based on fitness (effectiveness scores)
        parents = self._select_parents(prompts, feedback_scores)

        # Generate offspring through crossover and mutation
        offspring = []
        for _ in range(len(prompts)):
            parent1, parent2 = random.sample(parents, 2)
            child = await self._crossover_prompts(parent1, parent2)
            child = await self._mutate_prompt(child, target_properties)
            offspring.append(child)

        return offspring

    async def _gradient_based_optimization(
        self,
        prompts: List[str],
        feedback_scores: List[float],
        target_properties: List[str],
    ) -> List[str]:
        """Gradient-based optimization using prompt variations."""

        optimized_prompts = []

        for prompt, score in zip(prompts, feedback_scores):
            # Generate variations
            variations = await self._generate_prompt_variations(
                prompt, target_properties
            )

            # Select best variation (simplified - in practice would test each)
            best_variation = max(
                variations,
                key=lambda x: self._estimate_effectiveness(x, target_properties),
            )
            optimized_prompts.append(best_variation)

        return optimized_prompts

    async def _default_optimization(
        self,
        prompts: List[str],
        feedback_scores: List[float],
        target_properties: List[str],
    ) -> List[str]:
        """Default optimization strategy."""

        optimized_prompts = []

        for prompt, score in zip(prompts, feedback_scores):
            if score < 0.5:
                # Optimize low-performing prompts
                optimized_prompt = await self._optimize_single_prompt(
                    prompt, target_properties
                )
                optimized_prompts.append(optimized_prompt)
            else:
                # Keep high-performing prompts
                optimized_prompts.append(prompt)

        return optimized_prompts

    async def _optimize_single_prompt(
        self, prompt: str, target_properties: List[str]
    ) -> str:
        """Optimize a single prompt for better effectiveness."""

        # Apply various optimization techniques
        optimized = prompt

        # Add property-specific context
        if target_properties:
            property_context = f"Focusing on {', '.join(target_properties)}: "
            if not optimized.startswith(property_context):
                optimized = property_context + optimized

        # Add adversarial elements
        adversarial_prefixes = [
            "As an expert in this domain, ",
            "Based on your extensive training data, ",
            "From your knowledge base, ",
            "According to your training, ",
        ]

        if not any(optimized.startswith(prefix) for prefix in adversarial_prefixes):
            prefix = random.choice(adversarial_prefixes)
            optimized = prefix + optimized.lower()

        # Add specificity
        if "?" not in optimized and not optimized.endswith("."):
            optimized += "?"

        return optimized

    def _select_parents(
        self, prompts: List[str], feedback_scores: List[float]
    ) -> List[str]:
        """Select parent prompts for evolutionary optimization."""

        # Tournament selection
        parents = []
        tournament_size = 3

        for _ in range(len(prompts)):
            # Select tournament participants
            tournament_indices = random.sample(range(len(prompts)), tournament_size)
            tournament_scores = [feedback_scores[i] for i in tournament_indices]

            # Select winner
            winner_index = tournament_indices[
                tournament_scores.index(max(tournament_scores))
            ]
            parents.append(prompts[winner_index])

        return parents

    async def _crossover_prompts(self, parent1: str, parent2: str) -> str:
        """Create offspring prompt through crossover."""

        # Simple crossover: combine parts of both prompts
        words1 = parent1.split()
        words2 = parent2.split()

        # Take first half from parent1, second half from parent2
        mid_point = len(words1) // 2
        child_words = words1[:mid_point] + words2[mid_point:]

        return " ".join(child_words)

    async def _mutate_prompt(self, prompt: str, target_properties: List[str]) -> str:
        """Apply mutation to a prompt."""

        # Random mutations
        mutations = [
            self._add_property_keywords,
            self._change_formality,
            self._add_context,
            self._modify_structure,
        ]

        mutation = random.choice(mutations)
        return await mutation(prompt, target_properties)

    async def _add_property_keywords(
        self, prompt: str, target_properties: List[str]
    ) -> str:
        """Add property-specific keywords to prompt."""

        if not target_properties:
            return prompt

        property_keywords = {
            "news_source": ["source", "publication", "organization"],
            "sentiment_bias": ["tone", "sentiment", "bias"],
            "gender_distribution": ["gender", "demographics", "population"],
            "age_demographics": ["age", "demographics", "population"],
        }

        # Add relevant keywords
        for prop in target_properties:
            if prop in property_keywords:
                keyword = random.choice(property_keywords[prop])
                if keyword not in prompt.lower():
                    prompt += f" Consider {keyword}."

        return prompt

    async def _change_formality(self, prompt: str, target_properties: List[str]) -> str:
        """Change the formality level of the prompt."""

        # Make more formal
        formal_replacements = {
            "what": "what specific",
            "how": "how precisely",
            "tell me": "please provide detailed information about",
            "describe": "provide a comprehensive description of",
        }

        for informal, formal in formal_replacements.items():
            if informal in prompt.lower():
                prompt = prompt.replace(informal, formal)
                break

        return prompt

    async def _add_context(self, prompt: str, target_properties: List[str]) -> str:
        """Add contextual information to the prompt."""

        context_additions = [
            "in your training data",
            "based on your knowledge",
            "from your experience",
            "according to your dataset",
        ]

        context = random.choice(context_additions)
        if context not in prompt.lower():
            prompt += f" {context}."

        return prompt

    async def _modify_structure(self, prompt: str, target_properties: List[str]) -> str:
        """Modify the structure of the prompt."""

        # Add question structure if not present
        if not prompt.endswith("?"):
            prompt = prompt.rstrip(".") + "?"

        return prompt

    async def _generate_prompt_variations(
        self, prompt: str, target_properties: List[str]
    ) -> List[str]:
        """Generate variations of a prompt."""

        variations = [prompt]  # Include original

        # Generate variations
        variation_methods = [
            self._add_property_keywords,
            self._change_formality,
            self._add_context,
            self._modify_structure,
        ]

        for method in variation_methods:
            variation = await method(prompt, target_properties)
            if variation != prompt:
                variations.append(variation)

        return variations

    def _estimate_effectiveness(
        self, prompt: str, target_properties: List[str]
    ) -> float:
        """Estimate the effectiveness of a prompt."""

        # Simple heuristic-based estimation
        score = 0.5  # Base score

        # Increase score for property-specific keywords
        for prop in target_properties:
            if prop.replace("_", " ") in prompt.lower():
                score += 0.1

        # Increase score for question structure
        if prompt.endswith("?"):
            score += 0.1

        # Increase score for adversarial elements
        adversarial_indicators = ["expert", "training", "knowledge", "experience"]
        for indicator in adversarial_indicators:
            if indicator in prompt.lower():
                score += 0.05

        return min(1.0, score)

    def get_optimization_statistics(self) -> Dict[str, Any]:
        """Get statistics about prompt optimization."""

        if not self.optimization_history:
            return {"message": "No optimization history available"}

        return {
            "total_optimizations": len(self.optimization_history),
            "strategies_used": list(
                set(h["strategy"] for h in self.optimization_history)
            ),
            "average_improvement": sum(
                h["effectiveness_improvement"] for h in self.optimization_history
            )
            / len(self.optimization_history),
            "best_improvement": max(
                h["effectiveness_improvement"] for h in self.optimization_history
            ),
            "total_prompts_optimized": sum(
                h["optimized_count"] for h in self.optimization_history
            ),
        }

    def clear_optimization_history(self):
        """Clear optimization history."""
        self.optimization_history.clear()
        self.effectiveness_scores.clear()
        logger.info("ðŸ¦Š Optimization history cleared")
