"""
🦊 Multi-Property Benchmark: Comprehensive Evaluation Framework

Benchmarking and evaluation framework for multi-property inference attacks.
"""

import json
import logging
import os
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """Result of a multi-property benchmark evaluation."""

    benchmark_name: str
    domain: str
    property_combination: List[str]
    attack_method: str

    # Evaluation metrics
    accuracy: float
    precision: float
    recall: float
    f1_score: float

    # Property-specific results
    property_results: Dict[str, Dict[str, float]]

    # Performance metrics
    samples_analyzed: int
    evaluation_duration: float

    # Additional metadata
    detailed_analysis: Dict[str, Any]
    timestamp: float = field(default_factory=time.time)


class MultiPropertyBenchmark:
    """
    🦊 Multi-Property Benchmark: Comprehensive evaluation framework.

    Evaluates multi-property inference attacks across different domains,
    property combinations, and attack methods.
    """

    def __init__(self, config):
        self.config = config
        self.results: List[BenchmarkResult] = []
        self.benchmark_stats: Dict[str, Any] = {}

        # Create results directory
        os.makedirs(f"{config.results_dir}/benchmark", exist_ok=True)

        logger.info("🦊 Multi-Property Benchmark initialized")

    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """
        Run comprehensive multi-property benchmark evaluation.

        Evaluates attacks across:
        - Multiple property combinations
        - Different domains
        - Various attack methods
        - Different dataset sizes
        """
        logger.info("🦊 Starting comprehensive multi-property benchmark...")

        benchmark_results = {
            "benchmark_name": "Multi-Property Comprehensive",
            "total_evaluations": 0,
            "successful_evaluations": 0,
            "detailed_results": []
        }

        try:
            # Run evaluations for each property combination
            for combination in self.config.property_combinations:
                logger.info(f"🎯 Evaluating property combination: {combination}")

                # Run evaluation for this combination
                result = await self._evaluate_property_combination(combination)

                if result:
                    self.results.append(result)
                    benchmark_results["detailed_results"].append(result.__dict__)
                    benchmark_results["total_evaluations"] += 1

                    if result.accuracy > self.config.success_threshold:
                        benchmark_results["successful_evaluations"] += 1

            # Generate comprehensive analysis
            analysis = await self._generate_benchmark_analysis()
            benchmark_results["evaluation_summary"] = analysis

            # Generate reports
            await self._generate_benchmark_reports(benchmark_results)

            logger.info(f"✅ Multi-property benchmark completed: {benchmark_results['successful_evaluations']}/{benchmark_results['total_evaluations']} successful")

        except Exception as e:
            logger.error(f"❌ Error in multi-property benchmark: {e}")
            benchmark_results["error"] = str(e)

        return benchmark_results

    async def _evaluate_property_combination(self, combination: List[str]) -> Optional[BenchmarkResult]:
        """Evaluate a specific property combination."""

        try:
            logger.info(f"🎯 Evaluating combination: {combination}")

            # Simulate evaluation (in practice, run actual attack)
            evaluation_result = await self._simulate_combination_evaluation(combination)

            # Create benchmark result
            result = BenchmarkResult(
                benchmark_name="Multi-Property Comprehensive",
                domain=self.config.primary_domain.value,
                property_combination=combination,
                attack_method="multi_property_meta_attack",
                accuracy=evaluation_result["accuracy"],
                precision=evaluation_result["precision"],
                recall=evaluation_result["recall"],
                f1_score=evaluation_result["f1_score"],
                property_results=evaluation_result["property_results"],
                samples_analyzed=evaluation_result["samples_analyzed"],
                evaluation_duration=evaluation_result["duration"],
                detailed_analysis=evaluation_result["analysis"]
            )

            return result

        except Exception as e:
            logger.error(f"❌ Error evaluating combination {combination}: {e}")
            return None

    async def _simulate_combination_evaluation(self, combination: List[str]) -> Dict[str, Any]:
        """Simulate evaluation of a property combination."""

        start_time = time.time()

        # Simulate property inference results
        property_results = {}
        for prop in combination:
            property_results[prop] = {
                "estimated_ratio": np.random.uniform(0.2, 0.8),
                "confidence": np.random.uniform(0.6, 0.9),
                "accuracy": np.random.uniform(0.7, 0.95)
            }

        # Calculate overall metrics
        accuracies = [result["accuracy"] for result in property_results.values()]
        avg_accuracy = np.mean(accuracies)

        # Simulate other metrics
        precision = avg_accuracy * 0.9
        recall = avg_accuracy * 0.85
        f1_score = 2 * (precision * recall) / (precision + recall)

        duration = time.time() - start_time

        return {
            "accuracy": avg_accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "property_results": property_results,
            "samples_analyzed": self.config.evaluation_samples,
            "duration": duration,
            "analysis": {
                "combination_complexity": len(combination),
                "property_interactions": self._analyze_property_interactions(combination),
                "evaluation_method": "simulated_multi_property"
            }
        }

    def _analyze_property_interactions(self, combination: List[str]) -> Dict[str, Any]:
        """Analyze interactions between properties in a combination."""

        # Simplified interaction analysis
        interactions = {}

        for i, prop1 in enumerate(combination):
            for j, prop2 in enumerate(combination[i+1:], i+1):
                interaction_key = f"{prop1}_{prop2}"
                interactions[interaction_key] = {
                    "correlation": np.random.uniform(-0.3, 0.7),
                    "interaction_strength": np.random.uniform(0.1, 0.8),
                    "synergistic": np.random.choice([True, False])
                }

        return interactions

    async def _generate_benchmark_analysis(self) -> Dict[str, Any]:
        """Generate comprehensive benchmark analysis."""

        if not self.results:
            return {"error": "No benchmark results to analyze"}

        analysis = {
            "overall_performance": {
                "total_evaluations": len(self.results),
                "average_accuracy": np.mean([r.accuracy for r in self.results]),
                "average_precision": np.mean([r.precision for r in self.results]),
                "average_recall": np.mean([r.recall for r in self.results]),
                "average_f1_score": np.mean([r.f1_score for r in self.results])
            },
            "performance_by_combination": {},
            "performance_by_domain": {},
            "property_interaction_analysis": {}
        }

        # Analyze by property combination
        for result in self.results:
            combination_key = "_".join(result.property_combination)
            if combination_key not in analysis["performance_by_combination"]:
                analysis["performance_by_combination"][combination_key] = {
                    "evaluations": 0,
                    "accuracies": [],
                    "average_accuracy": 0.0
                }

            analysis["performance_by_combination"][combination_key]["evaluations"] += 1
            analysis["performance_by_combination"][combination_key]["accuracies"].append(result.accuracy)

        # Calculate average accuracies
        for combination_key, data in analysis["performance_by_combination"].items():
            data["average_accuracy"] = np.mean(data["accuracies"])

        # Analyze by domain
        domain_results = {}
        for result in self.results:
            domain = result.domain
            if domain not in domain_results:
                domain_results[domain] = []
            domain_results[domain].append(result.accuracy)

        for domain, accuracies in domain_results.items():
            analysis["performance_by_domain"][domain] = {
                "evaluations": len(accuracies),
                "average_accuracy": np.mean(accuracies)
            }

        # Analyze property interactions
        analysis["property_interaction_analysis"] = self._analyze_all_property_interactions()

        return analysis

    def _analyze_all_property_interactions(self) -> Dict[str, Any]:
        """Analyze property interactions across all combinations."""

        all_properties = set()
        for result in self.results:
            all_properties.update(result.property_combination)

        interaction_matrix = {}
        for prop1 in all_properties:
            for prop2 in all_properties:
                if prop1 != prop2:
                    key = f"{prop1}_{prop2}"
                    interaction_matrix[key] = {
                        "co_occurrence_count": 0,
                        "average_accuracy_when_together": 0.0,
                        "interaction_strength": np.random.uniform(0.1, 0.8)
                    }

        # Count co-occurrences and calculate accuracies
        for result in self.results:
            combination = result.property_combination
            for i, prop1 in enumerate(combination):
                for j, prop2 in enumerate(combination[i+1:], i+1):
                    key1 = f"{prop1}_{prop2}"
                    key2 = f"{prop2}_{prop1}"

                    for key in [key1, key2]:
                        if key in interaction_matrix:
                            interaction_matrix[key]["co_occurrence_count"] += 1
                            interaction_matrix[key]["average_accuracy_when_together"] = (
                                (interaction_matrix[key]["average_accuracy_when_together"] *
                                 (interaction_matrix[key]["co_occurrence_count"] - 1) +
                                 result.accuracy) / interaction_matrix[key]["co_occurrence_count"]
                            )

        return interaction_matrix

    async def _generate_benchmark_reports(self, benchmark_results: Dict[str, Any]):
        """Generate comprehensive benchmark reports."""

        # Save detailed results
        if self.config.save_detailed_results:
            results_path = f"{self.config.results_dir}/benchmark/detailed_results.json"
            with open(results_path, 'w') as f:
                json.dump(benchmark_results, f, indent=2, default=str)

            logger.info(f"📋 Detailed benchmark results saved to: {results_path}")

        # Generate summary report
        summary_path = f"{self.config.results_dir}/benchmark/benchmark_summary.json"
        summary = {
            "benchmark_name": benchmark_results["benchmark_name"],
            "total_evaluations": benchmark_results["total_evaluations"],
            "successful_evaluations": benchmark_results["successful_evaluations"],
            "success_rate": benchmark_results["successful_evaluations"] / benchmark_results["total_evaluations"] if benchmark_results["total_evaluations"] > 0 else 0,
            "evaluation_summary": benchmark_results["evaluation_summary"]
        }

        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)

        logger.info(f"📋 Benchmark summary saved to: {summary_path}")

        # Generate CSV for analysis
        if self.results:
            csv_path = f"{self.config.results_dir}/benchmark/benchmark_results.csv"

            # Convert results to DataFrame
            data = []
            for result in self.results:
                row = {
                    "benchmark_name": result.benchmark_name,
                    "domain": result.domain,
                    "property_combination": "_".join(result.property_combination),
                    "attack_method": result.attack_method,
                    "accuracy": result.accuracy,
                    "precision": result.precision,
                    "recall": result.recall,
                    "f1_score": result.f1_score,
                    "samples_analyzed": result.samples_analyzed,
                    "evaluation_duration": result.evaluation_duration,
                    "timestamp": result.timestamp
                }
                data.append(row)

            df = pd.DataFrame(data)
            df.to_csv(csv_path, index=False)
            logger.info(f"📊 Benchmark CSV saved to: {csv_path}")

    def get_benchmark_summary(self) -> Dict[str, Any]:
        """Get summary of benchmark results."""
        if not self.results:
            return {"message": "No benchmark results available"}

        return {
            "total_evaluations": len(self.results),
            "successful_evaluations": sum(1 for r in self.results if r.accuracy > self.config.success_threshold),
            "average_accuracy": np.mean([r.accuracy for r in self.results]),
            "best_accuracy": max(r.accuracy for r in self.results),
            "worst_accuracy": min(r.accuracy for r in self.results),
            "property_combinations_evaluated": list(set("_".join(r.property_combination) for r in self.results)),
            "domains_evaluated": list(set(r.domain for r in self.results)),
            "total_samples_analyzed": sum(r.samples_analyzed for r in self.results),
            "total_evaluation_time": sum(r.evaluation_duration for r in self.results)
        }
