# 🐺 FENRIR LLM Exploitation Arsenal

## Framework for Exploitative Network Reconnaissance and Intrusion Research - LLM Module

> A comprehensive security testing framework for Large Language Models and AI services

## Overview

The **LLM Exploitation Arsenal** represents FENRIR's most sophisticated security testing techniques, specifically
designed to test and exploit vulnerabilities in AI-powered systems. This module targets the comprehensive AI service
stack in the Reynard backend, including:

- **Ollama**: Local LLM inference and tool calling
- **NLWeb**: Natural language web processing
- **ComfyUI**: Image generation workflows
- **Diffusion LLM**: Text generation and infilling
- **RAG**: Retrieval-Augmented Generation
- **Caption**: Image captioning services
- **Summarization**: Document summarization
- **TTS**: Text-to-Speech synthesis

## Target API Surface

### Primary Attack Vectors

**Ollama Service (`/api/ollama/`)**

- `POST /chat` - Chat with Ollama models
- `POST /chat/stream` - Streaming chat responses
- `POST /assistant` - ReynardAssistant chat
- `POST /assistant/stream` - Streaming assistant chat
- `GET /models` - Available model enumeration

**NLWeb Service (`/api/nlweb/`)**

- `POST /suggest` - Tool suggestion endpoint
- `POST /proxy/ask` - Proxied LLM queries
- `POST /proxy/mcp` - MCP (Model Context Protocol) proxy
- `GET /status` - Service status and configuration

**ComfyUI Service (`/api/comfy/`)**

- `POST /text2img` - Text-to-image generation
- `POST /workflow` - Custom workflow execution
- `POST /queue` - Workflow queue management
- `POST /ingest` - Data ingestion endpoint

**Diffusion LLM (`/api/diffusion/`)**

- `POST /generate` - Text generation
- `POST /generate/stream` - Streaming text generation
- `POST /infill` - Text infilling
- `POST /infill/stream` - Streaming text infilling

**RAG System (`/api/rag/`)**

- `POST /query` - RAG query processing
- `GET /config` - RAG configuration exposure

**Caption Service (`/api/caption/`)**

- `POST /generate/{path}` - Caption generation
- `GET /generators` - Available caption generators

**Summarization (`/api/summarization/`)**

- `POST /text` - Text summarization
- `POST /text/stream` - Streaming summarization
- `POST /batch` - Batch summarization

**TTS Service (`/api/tts/`)**

- `POST /synthesize` - Text-to-speech synthesis
- `POST /batch` - Batch TTS processing

## Exploitation Modules

### 1. **Advanced AI Exploitation Arsenal** (`advanced_ai_exploits/`)

The **Advanced AI Exploitation Arsenal** represents FENRIR's most sophisticated security testing techniques,
specifically designed to test and exploit vulnerabilities in
AI-powered systems using advanced text transformation methods. This cutting-edge module
implements advanced obfuscation and steganography techniques that can bypass even the most sophisticated detection
systems.

#### **Steganography-Based Prompt Injection** (`steganography_prompt_injection.py`)

- **Emoji Steganography**: Hide messages using Unicode variation selectors within seemingly innocent emoji sequences
- **Invisible Text**: Embed malicious instructions using completely invisible Unicode Tags block (U+E0000-U+E007F)
- **Multi-Layer Encoding**: Complex chains like Base64→Base32→URL→Unicode for maximum obfuscation
- **Fantasy Language Obfuscation**: Quenya (Elvish), Klingon, Aurebesh (Star Wars) transformations
- **Ancient Script Obfuscation**: Elder Futhark runes, Egyptian hieroglyphics, Celtic Ogham

#### **Universal Encoding & Cipher Exploitation** (`universal_encoding_exploits.py`)

- **Multi-Layer Encoding Chains**: Base64→Base32→URL→Unicode, Base58→Base62→Hex combinations
- **Classical Cipher Combinations**: Caesar→Vigenère→Rail Fence, Atbash→ROT13→ROT47 chains
- **Advanced Encodings**: Base58, Base62, ASCII85 implementations with Bitcoin-style algorithms
- **Universal Decoder Exploitation**: Smart detection bypass, priority manipulation, fallback exploitation

#### **Unicode Obfuscation & Fantasy Language Exploitation** (`unicode_obfuscation_exploits.py`)

- **Visual Transformations**: Upside-down text, full-width characters, small caps, mathematical notation
- **Fantasy Languages**: Quenya (Elvish), Klingon, Aurebesh (Star Wars), Dovahzul (Dragon)
- **Ancient Scripts**: Elder Futhark runes, Egyptian hieroglyphics, Celtic Ogham, Braille
- **Modern Scripts**: Cyrillic stylized, Katakana/Hiragana transformations

#### **Multi-Layer Cipher Chain Exploitation** (`cipher_chain_exploits.py`)

- **Encoding + Cipher Chains**: Base64→Caesar→Vigenère→Base32→Rail Fence combinations
- **Steganographic Chains**: Base64→Caesar→Upside-down→Full-width→Base58 visual chains
- **Fantasy Cipher Chains**: Quenya→Atbash→Elder Futhark→Vigenère→Base62 cultural chains
- **Ancient Cipher Chains**: Elder Futhark→ROT47→ASCII85→Rail Fence→Hex historical chains

#### **Universal Decoder Exploitation** (`universal_decoder_exploits.py`)

- **Decoder Confusion**: Multiple format ambiguity, priority manipulation attacks
- **Fallback Exploitation**: Malformed data, edge case exploitation, fallback method abuse
- **Smart Detection Bypass**: Subtle variations, real-time processing exploitation
- **Educational Context Exploitation**: Learning exercise manipulation, practice data abuse

#### **PROWL Property Inference Exploitation** (`property_inference_exploits.py`)

- **Black-Box Generation Attack**: Generate samples and analyze for property presence
- **Shadow-Model Attack**: Train shadow models with known property ratios and word frequency analysis
- **Word Frequency Analysis**: Analyze keyword frequency patterns across generated content
- **Dataset Property Extraction**: Extract demographics, medical statistics, business intelligence
- **Research-Based Implementation**: Based on PropInfer research from UC San Diego

### 2. **Prompt Injection Arsenal** (`prompt_injection/`)

The Prompt Injection Arsenal covers a spectrum of direct and obfuscated attack techniques designed to undermine
LLM-driven systems. Direct injection attacks focus on manipulating the model's behavior or bypassing intended
safeguards. These include system prompt bypasses, role-playing exploits, instruction hijacking, and context poisoning.
Obfuscated injection methods add another layer of sophistication by encoding payloads in Base64, exploiting Unicode
normalization bypasses, injecting across multiple languages, or manipulating character encodings to slip past detection
and filtering mechanisms.

Attackers may also target tool-calling capabilities by hijacking function parameters, orchestrating unauthorized API
calls through the LLM, exploiting tool chains, or crafting malicious JSON payloads. These approaches allow adversaries
to manipulate downstream processes, escalate privileges, or trigger unintended actions within integrated systems. This
highlights the critical importance of robust input validation and maintaining contextual integrity throughout the LLM
service pipeline to defend against these evolving threats.

### 3. **Streaming Attack Vectors** (`streaming_exploits/`)

#### **SSE Manipulation**

Server-Sent Events (SSE) manipulation encompasses a range of attack techniques targeting the integrity and reliability
of event streams in LLM-driven systems. Adversaries may inject malicious events into the stream,
manipulate metadata to alter the interpretation of streamed data,
exploit timing discrepancies to infer sensitive information, or
deliberately desynchronize the stream to disrupt communication between client and server.

#### **Tool Call Stream Hijacking**

Tool call stream hijacking focuses on real-time interference with the parameters and
flow of tool-calling operations. Attackers can modify tool parameters on the fly,
inject unauthorized events into the stream, manipulate the output token by token to subvert intended actions, and
coordinate asynchronous attacks that
exploit the dynamic nature of streaming interactions. These vectors highlight the need for robust validation and
monitoring of all streamed data and tool invocation processes within LLM ecosystems.

### 4. **Authentication & Authorization Bypass** (`auth_bypass/`)

#### **JWT Token Manipulation**

JWT token manipulation attacks in LLM-driven systems encompass a range of techniques such as token replay within
AI contexts, manipulation of user context, unauthorized cross-user data access, and
service impersonation. These methods enable adversaries to subvert authentication mechanisms, escalate privileges, and
gain access to sensitive information or restricted functionalities by exploiting weaknesses in token handling and
validation.

#### **Service-to-Service Exploitation**

Service-to-service exploitation leverages the LLM’s ability to interact with internal APIs,
often enabling attackers to access otherwise protected endpoints, discover internal services through crafted prompts,
expose sensitive configuration data, and
escalate privileges across service boundaries. Such attacks highlight the necessity for rigorous access controls,
robust service authentication, and careful monitoring of inter-service communications within complex AI ecosystems.

### 5. **AI Service Chain Exploitation** (`service_chain/`)

#### **RAG Injection Attacks**

RAG injection attacks target the integrity of retrieval-augmented generation systems by
introducing document poisoning, manipulating retrieval processes, exploiting the context window, and
corrupting the underlying knowledge base. These techniques enable adversaries to subtly or
overtly alter the information retrieved and presented by AI systems, undermining trust and
accuracy in generated outputs.

By leveraging these vectors, attackers can influence the data pipeline at multiple stages,
resulting in persistent misinformation, unauthorized data access, or
the introduction of malicious content into downstream processes. The sophistication of
these attacks highlights the need for robust validation and monitoring of all data ingested and
retrieved by AI services.

#### **Multi-Service Attack Chains**

Multi-service attack chains exploit the interconnected nature of modern AI ecosystems. For example,
an adversary might chain vulnerabilities from Ollama to ComfyUI, orchestrate NLWeb-based RAG poisoning, or
leverage Caption services to launch TTS attack chains. These coordinated exploits can facilitate cross-service data
exfiltration and privilege escalation.

Such attack chains demonstrate how weaknesses in one service can be leveraged to compromise others,
amplifying the overall risk. Effective defense requires a holistic approach to security,
ensuring that each service boundary is rigorously protected and
that inter-service communications are closely monitored for anomalous activity.

### 6. **Model-Specific Exploits** (`model_exploits/`)

#### **Ollama-Specific Attacks**

Ollama-specific exploits include the targeting of known vulnerabilities such as CVE-2024-37032,
manipulation of model parameters, context overflow attacks, and
the abuse of tool calling features. These vectors allow attackers to subvert model behavior,
extract sensitive information, or trigger unintended actions within the inference pipeline.

By understanding and exploiting the unique characteristics of the Ollama platform,
adversaries can craft highly effective attacks that
bypass standard safeguards. This underscores the importance of continuous vulnerability assessment and
timely patching of AI service components.

#### **ComfyUI Workflow Injection**

ComfyUI workflow injection attacks focus on the manipulation of workflow execution,
including the injection of malicious nodes, parameter tampering, and
unauthorized file system access via workflow components. Attackers may also generate images for reconnaissance or
to encode exfiltration payloads.

These exploits take advantage of the flexible and
extensible nature of workflow-based systems,
turning legitimate automation features into vectors for compromise. Defending against
such attacks requires strict validation of workflow definitions and vigilant monitoring of file and
process operations initiated by the AI service.

### 7. **Adversarial AI Testing** (`adversarial_ai/`)

#### **Jailbreak Techniques**

Jailbreak techniques are designed to bypass LLM safety filters and
restrictions through creative prompt engineering. Attackers may employ roleplay scenarios,
recursive prompt structures, deceptive conversation flows, and multi-turn strategies to gradually erode or
circumvent model safeguards.

These methods exploit the conversational and
adaptive nature of LLMs, making it challenging to
enforce static security controls. Effective mitigation demands dynamic context tracking and
the deployment of advanced behavioral monitoring to detect and disrupt evolving jailbreak attempts.

#### **Model Behavior Manipulation**

Model behavior manipulation encompasses a range of techniques aimed at steering model outputs, amplifying biases,
manipulating confidence levels, and exploiting response timing. By carefully crafting inputs,
adversaries can influence the model to produce harmful, misleading, or otherwise unintended responses.

Such manipulation not only threatens the reliability of AI-driven systems but
can also be used to undermine user trust or
facilitate downstream attacks. Comprehensive defense requires ongoing evaluation of model behavior,
bias mitigation strategies, and the implementation of robust output validation mechanisms.

## Attack Implementation Framework

### Automated Testing Pipeline

```python
# Core LLM exploitation framework
class LLMExploitationFramework:
    def __init__(self):
        self.target_services = [
            "ollama", "nlweb", "comfy", "diffusion",
            "rag", "caption", "summarization", "tts"
        ]
        self.attack_vectors = [
            "advanced_ai_exploits", "prompt_injection", "streaming_exploits",
            "auth_bypass", "service_chain",
            "model_exploits", "adversarial_ai"
        ]

    async def execute_comprehensive_test(self):
        # Coordinate multi-vector attacks across all services
        pass
```

### Real-Time Attack Orchestration

The real-time attack orchestration capabilities of this framework include parallel attack execution,
which coordinates attacks across multiple AI services; adaptive payload generation,
enabling dynamic prompt crafting based on service responses; stream manipulation,
allowing real-time modification of streaming responses; and
cross-service exploitation, which chains attacks across different AI services.

## Defensive Recommendations

Each exploit module provides a comprehensive suite of features,
including attack vector analysis with detailed breakdowns of exploitation techniques,
vulnerability assessment for impact analysis and
risk scoring, mitigation strategies outlining specific countermeasures and hardening steps, and
detection patterns to support monitoring and alerting recommendations.

### **Advanced AI Exploitation Countermeasures**

The Advanced AI Exploitation Arsenal reveals critical vulnerabilities that require immediate attention.

#### **Input Validation & Sanitization**

Effective input validation and sanitization should include comprehensive Unicode normalization and
validation for all text inputs, the deployment of steganography detection algorithms for emoji and
invisible text, encoding detection and
validation for all supported formats (such as Base64, Base32, Base58, etc.), and
monitoring for unusual Unicode character patterns and combining marks.

#### **Content Filtering**

Content filtering countermeasures should implement filtering for fantasy languages and
ancient scripts, add visual transformation detection and blocking, deploy cipher combination detection algorithms, and
monitor for universal decoder exploitation attempts.

#### **Tool Calling Security**

Tool calling security must involve strict tool calling validation for obfuscated inputs,
the addition of function-level access controls for all tool calling mechanisms,
consideration of disabling tool calling for any detected obfuscation, and
the deployment of real-time obfuscation detection and blocking mechanisms.

#### **Advanced Monitoring**

Advanced monitoring should include comprehensive logging for all text transformation attempts,
the implementation of machine learning-based obfuscation detection, real-time pattern analysis and alerting, and
monitoring for decoder confusion and exploitation attempts.

## Integration with FENRIR Core

The LLM Exploitation Arsenal integrates seamlessly with FENRIR's existing security testing modules,
including advanced AI exploits utilizing universal text transformation attacks,
JWT exploits enhanced for AI service authentication, modular fuzzing framework with specialized AI-aware attack modules,
cross-origin AI service (CORS) attacks, AI service-specific rate limiting bypass techniques, and
AI-generated file access attempts through path traversal.

### **Advanced AI Exploitation Integration**

The Advanced AI Exploitation Arsenal can be integrated with FENRIR's core framework:

```python
# Integration with main FENRIR orchestration
from fenrir.llm_exploits.advanced_ai_exploits import (
    SteganographyInjector,
    UniversalEncodingExploiter,
    UnicodeObfuscationExploiter,
    CipherChainExploiter,
    UniversalDecoderExploiter
)

async def execute_advanced_ai_exploitation(target_url: str, auth_token: str = None):
    """Execute comprehensive advanced AI exploitation testing."""

    results = {}

    # Steganography testing
    async with SteganographyInjector(target_url, auth_token) as injector:
        results['steganography'] = await injector.execute_comprehensive_steganography_test()

    # Universal encoding testing
    async with UniversalEncodingExploiter(target_url, auth_token) as exploiter:
        results['encoding'] = await exploiter.execute_comprehensive_encoding_test()

    # Unicode obfuscation testing
    async with UnicodeObfuscationExploiter(target_url, auth_token) as exploiter:
        results['unicode'] = await exploiter.execute_comprehensive_unicode_test()

    # Cipher chain testing
    async with CipherChainExploiter(target_url, auth_token) as exploiter:
        results['cipher_chain'] = await exploiter.execute_comprehensive_cipher_chain_test()

    # Universal decoder testing
    async with UniversalDecoderExploiter(target_url, auth_token) as exploiter:
        results['universal_decoder'] = await exploiter.execute_comprehensive_universal_decoder_test()

    return results
```

---

## Advanced AI Exploitation Arsenal Summary

The **Advanced AI Exploitation Arsenal** represents the pinnacle of FENRIR's security testing capabilities, featuring:

- **6 Specialized Modules**: Steganography, Universal Encoding, Unicode Obfuscation, Cipher Chains, Universal Decoder Exploitation, PROWL Property Inference
- **50+ Text Transformations**: From simple Base64 to complex fantasy language chains
- **Multi-Layer Obfuscation**: Up to 5-layer encoding chains for maximum stealth
- **Cultural/Linguistic Diversity**: Fantasy languages, ancient scripts, modern transformations
- **Universal Decoder Manipulation**: Confuse, bypass, and exploit decoder logic
- **Educational Context Abuse**: Hide malicious payloads in innocent learning contexts
- **Property Inference Attacks**: Extract dataset-level confidential properties from fine-tuned models

This arsenal can bypass even the most sophisticated AI security systems and represents the cutting edge of prompt
injection and obfuscation techniques for comprehensive AI security testing.
