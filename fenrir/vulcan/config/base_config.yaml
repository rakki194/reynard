# VULCAN Base Configuration
# Versatile Unified Learning Capability And Neural Training

# Model Configuration
model:
  name: 'Qwen/Qwen3-8B'
  max_length: 2048
  temperature: 0.7
  enable_thinking: true

# Training Configuration
training:
  learning_rate: 2e-4
  batch_size: 4
  gradient_accumulation_steps: 8
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0

# LoRA Configuration
lora:
  rank: 8
  alpha: 16
  dropout: 0.1
  target_modules:
    - 'q_proj'
    - 'v_proj'
    - 'k_proj'
    - 'o_proj'
    - 'gate_proj'
    - 'up_proj'
    - 'down_proj'

# Data Configuration
data:
  train_split: 0.9
  val_split: 0.1
  max_samples: null # null for all samples
  shuffle: true
  seed: 42

# Logging Configuration
logging:
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: 'eval_loss'
  greater_is_better: false

# Output Configuration
output:
  output_dir: 'models/checkpoints'
  overwrite_output_dir: true
  save_strategy: 'steps'
  evaluation_strategy: 'steps'

# Hardware Configuration
hardware:
  device: 'auto' # auto, cuda, cpu
  mixed_precision: 'bf16' # bf16, fp16, no
  dataloader_num_workers: 4
  dataloader_pin_memory: true

# Advanced Configuration
advanced:
  gradient_checkpointing: true
  ddp_find_unused_parameters: false
  remove_unused_columns: false
  dataloader_drop_last: true
