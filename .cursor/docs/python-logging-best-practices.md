# Python Logging Best Practices Guide

_Efficient logging patterns with lazy formatting and proper error handling_

## Overview

Python logging is a critical component of production applications, but
improper usage can lead to performance issues and
poor debugging experiences. This guide covers best practices for logging, including lazy formatting,
proper exception handling, and performance optimization techniques.

## The Problem: Inefficient Logging

### F-String Logging Anti-Pattern

**‚ùå Inefficient Logging:**

```python
import logging

logger = logging.getLogger(__name__)

def process_user_data(user_id: str, data: dict) -> bool:
    # F-strings are ALWAYS evaluated, even when logging level prevents output
    logger.debug(f"Processing user {user_id} with {len(data)} items")
    logger.info(f"User {user_id} data: {data}")
    logger.warning(f"Validation failed for user {user_id}: {data}")

    try:
        result = complex_operation(data)
        logger.info(f"Successfully processed user {user_id}: {result}")
        return True
    except Exception as e:
        logger.error(f"Failed to process user {user_id}: {e}")
        return False
```

**Problems:**

- F-strings are evaluated even when log level prevents output
- String formatting happens regardless of logging configuration
- Performance impact in production with high log levels
- Memory usage for unused formatted strings

### Lazy Logging Solution

**‚úÖ Efficient Logging:**

```python
import logging

logger = logging.getLogger(__name__)

def process_user_data(user_id: str, data: dict) -> bool:
    # Lazy formatting - only evaluated when logging level allows output
    logger.debug("Processing user %s with %d items", user_id, len(data))
    logger.info("User %s data: %s", user_id, data)
    logger.warning("Validation failed for user %s: %s", user_id, data)

    try:
        result = complex_operation(data)
        logger.info("Successfully processed user %s: %s", user_id, result)
        return True
    except Exception as e:
        logger.error("Failed to process user %s: %s", user_id, e)
        return False
```

**Benefits:**

- String formatting only occurs when log level allows output
- Significant performance improvement in production
- Reduced memory usage
- Better debugging with `exc_info=True`

## Logging Performance Comparison

### Benchmark Results

```python
import logging
import time
import sys

# Configure logging
logging.basicConfig(level=logging.WARNING)  # Only WARNING and above
logger = logging.getLogger(__name__)

def benchmark_f_string_logging():
    """Benchmark f-string logging performance."""
    start_time = time.time()

    for i in range(10000):
        # This will be evaluated 10,000 times but never output
        logger.debug(f"Processing item {i} with data {list(range(100))}")

    end_time = time.time()
    return end_time - start_time

def benchmark_lazy_logging():
    """Benchmark lazy logging performance."""
    start_time = time.time()

    for i in range(10000):
        # This will be evaluated 0 times (no output at DEBUG level)
        logger.debug("Processing item %d with data %s", i, list(range(100)))

    end_time = time.time()
    return end_time - start_time

# Results (typical):
# F-string logging: ~2.5 seconds
# Lazy logging: ~0.1 seconds
# Performance improvement: 25x faster
```

## Logging Levels and Usage

### Standard Logging Levels

```python
import logging

logger = logging.getLogger(__name__)

# DEBUG: Detailed information for diagnosing problems
logger.debug("Entering function with parameters: %s", params)

# INFO: General information about program execution
logger.info("User %s logged in successfully", username)

# WARNING: Something unexpected happened, but program continues
logger.warning("Deprecated API endpoint used: %s", endpoint)

# ERROR: A serious problem occurred, but program continues
logger.error("Failed to connect to database: %s", error_message)

# CRITICAL: A very serious error occurred, program may not continue
logger.critical("System running out of memory: %d%% used", memory_percent)
```

### Context-Aware Logging

```python
import logging
from contextvars import ContextVar

# Context variables for request tracking
request_id: ContextVar[str] = ContextVar('request_id')
user_id: ContextVar[str] = ContextVar('user_id')

class ContextualFormatter(logging.Formatter):
    """Custom formatter that includes context information."""

    def format(self, record):
        # Add context information to log record
        record.request_id = request_id.get('unknown', '')
        record.user_id = user_id.get('unknown', '')
        return super().format(record)

def setup_logging():
    """Setup logging with context-aware formatting."""
    formatter = ContextualFormatter(
        '%(asctime)s - %(name)s - %(levelname)s - '
        '[%(request_id)s] [%(user_id)s] - %(message)s'
    )

    handler = logging.StreamHandler()
    handler.setFormatter(formatter)

    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

def process_request(req_id: str, user: str, data: dict):
    """Process request with contextual logging."""
    # Set context for this request
    request_id.set(req_id)
    user_id.set(user)

    logger = logging.getLogger(__name__)
    logger.info("Processing request with %d items", len(data))

    try:
        result = process_data(data)
        logger.info("Request completed successfully")
        return result
    except Exception as e:
        logger.error("Request failed: %s", e, exc_info=True)
        raise
```

## Exception Logging Best Practices

### Proper Exception Logging

```python
import logging
import traceback

logger = logging.getLogger(__name__)

def robust_data_processing(data: list) -> list:
    """Process data with comprehensive exception logging."""
    processed_items = []

    for i, item in enumerate(data):
        try:
            result = process_item(item)
            processed_items.append(result)
            logger.debug("Processed item %d successfully", i)

        except ValueError as e:
            # Log specific exception with context
            logger.warning(
                "Invalid data format for item %d: %s. Skipping item.",
                i, e
            )
            continue

        except ProcessingError as e:
            # Log business logic errors
            logger.error(
                "Processing failed for item %d: %s. Item: %s",
                i, e, item
            )
            continue

        except Exception as e:
            # Log unexpected errors with full traceback
            logger.error(
                "Unexpected error processing item %d: %s",
                i, e, exc_info=True
            )
            continue

    logger.info("Data processing completed. %d/%d items processed",
                len(processed_items), len(data))
    return processed_items
```

### Exception Chaining in Logs

```python
def handle_api_request(url: str, data: dict) -> dict:
    """Handle API request with proper exception chaining in logs."""
    logger = logging.getLogger(__name__)

    try:
        response = requests.post(url, json=data, timeout=10)
        response.raise_for_status()
        return response.json()

    except requests.Timeout as e:
        logger.error("API request timeout for %s: %s", url, e)
        raise APIError(f"Request timeout for {url}") from e

    except requests.ConnectionError as e:
        logger.error("API connection error for %s: %s", url, e)
        raise APIError(f"Connection failed for {url}") from e

    except requests.HTTPError as e:
        logger.error("API HTTP error for %s: %d - %s",
                    url, e.response.status_code, e.response.text)
        raise APIError(f"HTTP error {e.response.status_code} for {url}") from e

    except Exception as e:
        logger.error("Unexpected API error for %s: %s", url, e, exc_info=True)
        raise APIError(f"Unexpected error for {url}") from e
```

## Structured Logging

### JSON Logging for Production

```python
import json
import logging
from datetime import datetime
from typing import Any, Dict

class JSONFormatter(logging.Formatter):
    """JSON formatter for structured logging."""

    def format(self, record: logging.LogRecord) -> str:
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno,
        }

        # Add exception information if present
        if record.exc_info:
            log_entry['exception'] = {
                'type': record.exc_info[0].__name__,
                'message': str(record.exc_info[1]),
                'traceback': self.formatException(record.exc_info)
            }

        # Add extra fields from record
        for key, value in record.__dict__.items():
            if key not in log_entry and not key.startswith('_'):
                log_entry[key] = value

        return json.dumps(log_entry)

def setup_structured_logging():
    """Setup structured JSON logging."""
    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())

    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# Usage with extra context
def log_user_action(user_id: str, action: str, details: Dict[str, Any]):
    """Log user action with structured data."""
    logger = logging.getLogger(__name__)
    logger.info(
        "User action performed",
        extra={
            'user_id': user_id,
            'action': action,
            'details': details,
            'category': 'user_action'
        }
    )
```

### Contextual Logging with Extra Fields

```python
import logging
from functools import wraps

def log_function_call(func):
    """Decorator to log function calls with context."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)

        # Log function entry
        logger.debug(
            "Entering %s with args=%s, kwargs=%s",
            func.__name__, args, kwargs
        )

        try:
            result = func(*args, **kwargs)
            logger.debug("Exiting %s with result=%s", func.__name__, result)
            return result

        except Exception as e:
            logger.error(
                "Exception in %s: %s",
                func.__name__, e,
                exc_info=True,
                extra={
                    'function': func.__name__,
                    'args': str(args),
                    'kwargs': str(kwargs)
                }
            )
            raise

    return wrapper

@log_function_call
def process_payment(amount: float, user_id: str) -> bool:
    """Process payment with automatic logging."""
    # Function implementation
    return True
```

## Performance Optimization

### Conditional Logging

```python
import logging

logger = logging.getLogger(__name__)

def expensive_operation(data: list) -> list:
    """Operation with expensive logging preparation."""
    # Only prepare expensive data if logging level allows it
    if logger.isEnabledFor(logging.DEBUG):
        # This expensive operation only runs if DEBUG logging is enabled
        debug_info = {
            'data_size': len(data),
            'data_summary': summarize_data(data),  # Expensive operation
            'memory_usage': get_memory_usage()
        }
        logger.debug("Processing data: %s", debug_info)

    # Regular processing
    return [item * 2 for item in data]

def summarize_data(data: list) -> dict:
    """Expensive operation to summarize data."""
    # Simulate expensive operation
    import time
    time.sleep(0.1)  # Simulate processing time
    return {'min': min(data), 'max': max(data), 'avg': sum(data) / len(data)}
```

### Logging Configuration for Performance

```python
import logging
import logging.config

# Production logging configuration
PRODUCTION_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
        'json': {
            '()': 'JSONFormatter',
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'standard',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'DEBUG',
            'formatter': 'json',
            'filename': 'app.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        }
    },
    'loggers': {
        '': {  # Root logger
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False
        },
        'app.database': {
            'handlers': ['file'],
            'level': 'DEBUG',
            'propagate': False
        },
        'app.api': {
            'handlers': ['console'],
            'level': 'INFO',
            'propagate': False
        }
    }
}

def setup_production_logging():
    """Setup production logging configuration."""
    logging.config.dictConfig(PRODUCTION_CONFIG)
```

## Security Considerations

### Sensitive Data Logging

```python
import logging
import re
from typing import Any, Dict

class SecureFormatter(logging.Formatter):
    """Formatter that redacts sensitive information."""

    SENSITIVE_PATTERNS = [
        (r'password["\']?\s*[:=]\s*["\']?([^"\'\s]+)', r'password="***"'),
        (r'token["\']?\s*[:=]\s*["\']?([^"\'\s]+)', r'token="***"'),
        (r'api_key["\']?\s*[:=]\s*["\']?([^"\'\s]+)', r'api_key="***"'),
        (r'secret["\']?\s*[:=]\s*["\']?([^"\'\s]+)', r'secret="***"'),
    ]

    def format(self, record: logging.LogRecord) -> str:
        message = super().format(record)

        # Redact sensitive information
        for pattern, replacement in self.SENSITIVE_PATTERNS:
            message = re.sub(pattern, replacement, message, flags=re.IGNORECASE)

        return message

def log_user_data(user_data: Dict[str, Any]):
    """Log user data with sensitive information redacted."""
    logger = logging.getLogger(__name__)

    # Create a copy to avoid modifying original data
    safe_data = user_data.copy()

    # Remove sensitive fields
    sensitive_fields = ['password', 'token', 'api_key', 'secret', 'ssn']
    for field in sensitive_fields:
        if field in safe_data:
            safe_data[field] = '***'

    logger.info("User data: %s", safe_data)
```

### Audit Logging

```python
import logging
from datetime import datetime
from typing import Any, Dict

class AuditLogger:
    """Specialized logger for audit events."""

    def __init__(self):
        self.logger = logging.getLogger('audit')
        self.logger.setLevel(logging.INFO)

        # Separate handler for audit logs
        handler = logging.FileHandler('audit.log')
        formatter = logging.Formatter(
            '%(asctime)s - AUDIT - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log_user_action(self, user_id: str, action: str,
                       resource: str, details: Dict[str, Any] = None):
        """Log user action for audit purposes."""
        audit_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'user_id': user_id,
            'action': action,
            'resource': resource,
            'details': details or {},
            'ip_address': get_client_ip(),  # Implement this
            'user_agent': get_user_agent()  # Implement this
        }

        self.logger.info("User action: %s", json.dumps(audit_entry))

    def log_system_event(self, event_type: str, details: Dict[str, Any]):
        """Log system events for audit purposes."""
        audit_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'details': details,
            'system': 'application'
        }

        self.logger.info("System event: %s", json.dumps(audit_entry))

# Usage
audit_logger = AuditLogger()

def login_user(user_id: str, ip_address: str):
    """Login user with audit logging."""
    try:
        # Login logic
        result = perform_login(user_id)

        # Log successful login
        audit_logger.log_user_action(
            user_id=user_id,
            action='login',
            resource='authentication',
            details={'ip_address': ip_address, 'success': True}
        )

        return result

    except Exception as e:
        # Log failed login attempt
        audit_logger.log_user_action(
            user_id=user_id,
            action='login_failed',
            resource='authentication',
            details={'ip_address': ip_address, 'error': str(e)}
        )
        raise
```

## Testing Logging

### Logging in Tests

```python
import logging
import pytest
from unittest.mock import Mock, patch

def test_logging_output():
    """Test that logging produces expected output."""
    with patch('logging.getLogger') as mock_get_logger:
        mock_logger = Mock()
        mock_get_logger.return_value = mock_logger

        # Import and use the module
        from mymodule import process_data

        # Call function that should log
        process_data([1, 2, 3])

        # Verify logging calls
        mock_logger.info.assert_called_with(
            "Processing %d items", 3
        )

def test_exception_logging():
    """Test that exceptions are properly logged."""
    with patch('logging.getLogger') as mock_get_logger:
        mock_logger = Mock()
        mock_get_logger.return_value = mock_logger

        from mymodule import risky_operation

        # Call function that raises exception
        with pytest.raises(ValueError):
            risky_operation("invalid")

        # Verify error was logged with exception info
        mock_logger.error.assert_called()
        call_args = mock_logger.error.call_args
        assert call_args[1]['exc_info'] is True

class TestLoggingConfiguration:
    """Test logging configuration."""

    def test_log_level_configuration(self):
        """Test that log levels are configured correctly."""
        logger = logging.getLogger('test_module')

        # Test different log levels
        assert logger.isEnabledFor(logging.INFO)
        assert logger.isEnabledFor(logging.WARNING)
        assert logger.isEnabledFor(logging.ERROR)

        # DEBUG might be disabled in production
        # assert logger.isEnabledFor(logging.DEBUG)
```

## Best Practices Summary

### 1. Use Lazy Formatting

```python
# ‚úÖ Good: Lazy formatting
logger.info("Processing user %s with %d items", user_id, item_count)

# ‚ùå Bad: F-string formatting
logger.info(f"Processing user {user_id} with {item_count} items")
```

### 2. Log Exceptions Properly

```python
# ‚úÖ Good: Log with exception info
try:
    risky_operation()
except Exception as e:
    logger.error("Operation failed: %s", e, exc_info=True)

# ‚ùå Bad: Log without context
try:
    risky_operation()
except Exception as e:
    logger.error("Operation failed")
```

### 3. Use Appropriate Log Levels

```python
# ‚úÖ Good: Appropriate levels
logger.debug("Detailed debugging info")
logger.info("Normal operation info")
logger.warning("Unexpected but recoverable")
logger.error("Error that needs attention")
logger.critical("System-level problem")
```

### 4. Include Context

```python
# ‚úÖ Good: Rich context
logger.info("User %s performed action %s on resource %s",
           user_id, action, resource)

# ‚ùå Bad: Minimal context
logger.info("Action performed")
```

### 5. Secure Sensitive Data

```python
# ‚úÖ Good: Redact sensitive data
logger.info("User data: %s", redact_sensitive_data(user_data))

# ‚ùå Bad: Log sensitive data
logger.info("User data: %s", user_data)  # May contain passwords
```

## Conclusion

ü¶ä _Effective logging requires the cunning of a fox - knowing what to log, when to log it, and
how to do it efficiently without impacting performance._

Proper logging practices are essential for:

- **Debugging**: Comprehensive error information and context
- **Monitoring**: Understanding application behavior and performance
- **Security**: Audit trails and sensitive data protection
- **Performance**: Efficient logging that doesn't impact application speed

Key principles:

- **Use Lazy Formatting**: Avoid f-strings in logging calls
- **Log Exceptions Properly**: Include `exc_info=True` for full tracebacks
- **Choose Appropriate Levels**: Use the right log level for each message
- **Include Rich Context**: Provide enough information for debugging
- **Secure Sensitive Data**: Redact or exclude sensitive information
- **Test Logging**: Verify that logging works as expected

By following these best practices, you'll create logging that provides excellent debugging information while
maintaining performance and security.

_Build logging that outfoxes complexity and provides clarity._ ü¶ä
