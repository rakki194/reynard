<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>## CLIPA - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>## CLIPA</h1>
      <div class="markdown-content"><h2>CLIPA</h2>
<p>In this work, we present a surprising finding that there exists an <em>inverse</em> scaling law for CLIP training,<br>whereby the larger the image/text encoders used, the shorter the sequence length of image/text tokens that can be applied in training.<br>Moreover, we showcase that the strategy for reducing image/text token length plays a crucial role in determining the quality of this scaling law.</p>
<p><img src="/docs/inverse_scaling_law.png" alt=""></p>
<p>As a result of this finding, we are able to successfully train CLIP even by using academic resources.<br>For example, on an A100 eight-GPU server, our CLIP models achieve zero-shot top-1 ImageNet accuracies of <strong>63.2%</strong> in about <strong>2 days</strong>,<br><strong>67.8%</strong> in about <strong>3 days</strong>, and <strong>69.3%</strong> in about <strong>4 days</strong>.</p>
<p>Moreover, We find that CLIPA at scale leads to state-of-the-art performance. For example, our CLIPA-v2 H/14 achieves a zero-shot top-1 ImageNet accuracy of <strong>81.8%</strong>,<br>with a budget less than <strong>$15000</strong>.</p>
<p><img src="/docs/clipa_acc_compute.png" alt=""></p>
<p>For more details, please see our paper <a href="https://arxiv.org/abs/2305.07017">An Inverse Scaling Law for CLIP Training</a> and<br><a href="https://arxiv.org/abs/2306.15658">CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy</a>.</p>
<p>Eight token length reduction strategies are investigated in this work, detailed as follows.</p>
<h2>Image token length reduction</h2>
<p><img src="/docs/clipa_reduce_image_token.png" alt=""></p>
<ul>
<li><p><code>resize</code>: use <code>--force-image-size</code> to specify the image size you want to adopt. We find this strategy generally works the best as it retains full image information.</p>
</li>
<li><p><code>random mask</code>: Randomly mask out image patches. use <code>--force-patch-dropout</code> to specify the mask ratio you want to adopt. </p>
</li>
<li><p><code>grid mask</code>: Preserve one patch in each 2 × 2 grid window. We do not provide implementation for grid masking, as it is only experimental and we generally find resizing works better.</p>
</li>
<li><p><code>block mask</code>: Keep a single block and remove other patches. We do not provide implementation for block masking, as it is only experimental and we generally find resizing works better.</p>
</li>
</ul>
<h2>Text token length reduction</h2>
<ul>
<li><p><code>syntax mask</code>: Assign different masking priorities to parts of speech. Specify <code>&quot;text_mask&quot;: syntax</code> in <code>&quot;tokenizer_kwargs&quot;</code> in <code>&quot;text_cfg&quot;</code> of model config <code>json</code> file to use.<br>Specifically, we prioritize retaining nouns, followed by adjectives, and then other words.<br>We find this strategy generally works the best as it retains critical information for contrastive learning.</p>
</li>
<li><p><code>truncate</code>: Truncation selects the first N text tokens and discards the rest. This is the default setting of <code>open_clip</code>. </p>
</li>
<li><p><code>random mask</code>: Randomly drops a portion of the text tokens. Specify <code>&quot;text_mask&quot;: random</code> in <code>&quot;tokenizer_kwargs&quot;</code> in <code>&quot;text_cfg&quot;</code> of model config <code>json</code> file to use. </p>
</li>
<li><p><code>block mask</code>: Randomly preserves consecutive text sequences. Specify <code>&quot;text_mask&quot;: block</code> in <code>&quot;tokenizer_kwargs&quot;</code> in <code>&quot;text_cfg&quot;</code> of model config <code>json</code> file to use.</p>
</li>
</ul>
<h2>Installation</h2>
<p>The installation is really the same as <code>open_clip</code>, except for the usage of Natural Language Toolkit (NLTK) in <code>syntax mask</code> of text token length reduction.<br>Please follow the <a href="https://www.nltk.org/">official doc</a> to install NLTK.</p>
<p>Note that the the usage of NLTK brings two constraints:</p>
<ul>
<li>Because certain functions like <code>nltk.pos_tag</code> from NLTK only support English and Russian for now, the <code>syntax mask</code> only works for English.<br>we have not tested it on Russian or any other language. Theoretically, it should work the same, given a proper language processing toolkit for other languages.<br>If you still want to apply <code>syntax mask</code> on other languages, try finding the right toolkit. Otherwise, use other text token length reduction strategies</li>
<li>some modules of NLTK like <code>punkt</code> or <code>averaged_perceptron_tagger</code> need to be downloaded first before using NLTK.<br>We have included the downloading code in <code>tokenizer.py</code>, but this might cause trouble in certain cases.<br>You may want to manually download those modules first, by <code>nltk.download(&#39;punkt&#39;)</code> and <code>nltk.download(&#39;averaged_perceptron_tagger&#39;)</code>,<br>and then setup the environmental variable before running the script <code>export NLTK_DATA=cache</code>.<br>Note that this is a one-time effort. Remember to comment out those <code>nltk.download</code> lines in <code>tokenizer.py</code> afterwards.</li>
</ul>
<h2>Training</h2>
<p>We provide example scripts to reproduce our CLIPA results on an A100 eight-GPU machine under path <code>docs/script_examples/clipa</code>.</p>
<p>For instance, to reproduce the CLIPA-L16(I37,T8) results, first run the pre-training script</p>
<pre><code class="hljs">bash docs<span class="hljs-regexp">/script_examples/</span>clipa<span class="hljs-regexp">/vit_l16/i</span>37_t8_pretrain.sh</code></pre><p>and fine-tune the pre-trained checkpoint with</p>
<pre><code class="hljs">bash docs<span class="hljs-regexp">/script_examples/</span>clipa<span class="hljs-regexp">/vit_l16/i</span>37_t8_finetune.sh</code></pre><ul>
<li>Remember to change the path to dataset to your own path.</li>
<li>This is a two-stage training pipeline. Remember to change the path to pre-trained checkpoint to your own when fine-tuning.</li>
<li>The training time is ~3 days for pre-training and ~1 day for fine-tuning on an A100 eight-GPU machine.</li>
</ul>
<h2>Model Weights</h2>
<p>Below are CLIPA trained weights on LAION-400M with an A100 eight-GPU machine.<br>All models are pre-trained for 6 epochs with reduced input token lengths and subsequently fine-tuned for 0.36 epoch with full input token lengths.</p>
<table>
<thead>
<tr>
<th></th>
<th align="center">Pre-trained Weights</th>
<th align="center">zero-shot IN-1K</th>
</tr>
</thead>
<tbody><tr>
<td>CLIPA-B/16(I50,T16)</td>
<td align="center"><a href="https://drive.google.com/file/d/1MDpz8gV2Vjaazk16rBhLxU8811U7_cGL/view?usp=sharing">download</a></td>
<td align="center">59.7</td>
</tr>
<tr>
<td>CLIPA-L/16(I17,T16)</td>
<td align="center"><a href="https://drive.google.com/file/d/1Tr2GYiKAaMH6EGIn5l7eX_1K20eaA3WA/view?usp=sharing">download</a></td>
<td align="center">60.3</td>
</tr>
<tr>
<td>CLIPA_L/16(I37,T8)</td>
<td align="center"><a href="https://drive.google.com/file/d/1EM1ChRNARpLckkJjf6m7njCY3xyvpGBu/view?usp=sharing">download</a></td>
<td align="center">57.9</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th></th>
<th align="center">Fine-tuned Weights</th>
<th align="center">zero-shot IN-1K</th>
</tr>
</thead>
<tbody><tr>
<td>CLIPA-B/16(I50,T16)</td>
<td align="center"><a href="https://drive.google.com/file/d/1fURK0K_a3-83jVEI4PVEbnEJb_V6UbGv/view?usp=sharing">download</a></td>
<td align="center">63.2</td>
</tr>
<tr>
<td>CLIPA-L/16(I17,T16)</td>
<td align="center"><a href="https://drive.google.com/file/d/18qqZGOTGOgb3I3JWONuat6qObsgLq7sR/view?usp=sharing">download</a></td>
<td align="center">67.8</td>
</tr>
<tr>
<td>CLIPA_L/16(I37,T8)</td>
<td align="center"><a href="https://drive.google.com/file/d/1lV7pLORUK04T9QKKx9TpYtMws-AZrib0/view?usp=sharing">download</a></td>
<td align="center">69.3</td>
</tr>
</tbody></table>
<h2>CLIPA-v2</h2>
<p>We also provide example scripts to reproduce our CLIPA-v2 H/14 results under path <code>docs/script_examples/clipav2</code>.<br>Note that the original results are obtained with <a href="https://github.com/UCSC-VLAA/CLIPA/tree/master/clipa_jax">our JAX implementation</a>.<br>These scripts are written after manually scanning the JAX config files.<br>As it is infeasible for us to retrain those models again with pytorch, its correctness cannot be verified with 100% confidence. Use them at your own discretion.</p>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>