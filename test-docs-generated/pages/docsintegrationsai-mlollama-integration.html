<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ollama Integration & YipYap Assistant 🦊 - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>Ollama Integration & YipYap Assistant 🦊</h1>
      <div class="markdown-content"><h1>Ollama Integration &amp; YipYap Assistant 🦊</h1>
<p>This document covers the Ollama integration in YipYap, including the custom yipyap assistant that helps with dataset management tasks.</p>
<h2>Overview</h2>
<p>YipYap now includes a built-in AI assistant (🦊) powered by Ollama that can help you with:</p>
<ul>
<li>Image dataset organization and management</li>
<li>Tagging and captioning workflows</li>
<li>Dataset cleaning and preparation</li>
<li>Understanding YipYap features and functionality</li>
<li>Git-based dataset version control</li>
<li>General dataset best practices</li>
</ul>
<h2>Prerequisites</h2>
<h3>Installing Ollama</h3>
<ol>
<li><p><strong>Download and Install Ollama</strong></p>
<ul>
<li>Visit <a href="https://ollama.ai">ollama.ai</a> and download for your platform</li>
<li>Follow the installation instructions for your operating system</li>
</ul>
</li>
<li><p><strong>Start Ollama Service</strong></p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># The service usually starts automatically, but you can also run:</span>
ollama serve</code></pre></li>
<li><p><strong>Pull a Model</strong> (recommended)</p>
<pre><code class="hljs language-bash">ollama pull qwen3:8b</code></pre></li>
</ol>
<h3>Environment Configuration</h3>
<p>Configure your Ollama settings by setting these environment variables:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Ollama server URL (default: http://localhost:11434)</span>
OLLAMA_BASE_URL=http://localhost:11434

<span class="hljs-comment"># Default model for the yipyap assistant (default: qwen3:8b)</span>
YIPYAP_ASSISTANT_MODEL=qwen3:8b</code></pre><p>You can also set these in your <code>.env</code> file in the project root.</p>
<h2>Using the YipYap Assistant</h2>
<h3>Accessing the Assistant</h3>
<p>The yipyap assistant (🦊) appears as a floating chat interface in the bottom-right corner of the application when Ollama is connected and running.</p>
<h3>Features</h3>
<ol>
<li><p><strong>Context-Aware Assistance</strong></p>
<ul>
<li>The assistant automatically receives context about your current directory</li>
<li>It knows about selected images and current dataset state</li>
<li>Git repository status is included when available</li>
</ul>
</li>
<li><p><strong>Streaming Responses</strong></p>
<ul>
<li>Real-time response streaming for immediate feedback</li>
<li>Ability to stop generation if needed</li>
</ul>
</li>
<li><p><strong>Conversation History</strong></p>
<ul>
<li>Maintains conversation context within the session</li>
<li>Clear history option for starting fresh</li>
</ul>
</li>
<li><p><strong>Model Selection</strong></p>
<ul>
<li>Switch between different Ollama models</li>
<li>Automatic model availability detection</li>
</ul>
</li>
</ol>
<h3>Example Use Cases</h3>
<p><strong>Dataset Organization:</strong></p>
<blockquote>
<p>&quot;How should I organize my training images for a Stable Diffusion model?&quot;</p>
</blockquote>
<p><strong>Tagging Help:</strong></p>
<blockquote>
<p>&quot;What tags would be appropriate for this fantasy art collection?&quot;</p>
</blockquote>
<p><strong>Git Workflow:</strong></p>
<blockquote>
<p>&quot;I have 500 new images in my dataset. What&#39;s the best way to commit them?&quot;</p>
</blockquote>
<p><strong>Caption Generation:</strong></p>
<blockquote>
<p>&quot;Should I use JTP2 or WDv3 for tagging anime-style artwork?&quot;</p>
</blockquote>
<p><strong>Technical Issues:</strong></p>
<blockquote>
<p>&quot;My thumbnails are generating slowly. How can I speed this up?&quot;</p>
</blockquote>
<h2>API Endpoints</h2>
<p>The Ollama integration provides several API endpoints:</p>
<h3>Status and Health</h3>
<ul>
<li><code>GET /api/ollama/status</code> - Get connection status and availability</li>
<li><code>GET /api/ollama/health</code> - Health check endpoint</li>
</ul>
<h3>Model Management</h3>
<ul>
<li><code>GET /api/ollama/models</code> - List available models</li>
<li><code>POST /api/ollama/models/pull</code> - Pull a new model</li>
<li><code>GET /api/ollama/assistant/models</code> - Get models available for assistant</li>
<li><code>POST /api/ollama/assistant/ensure-model</code> - Ensure model is available</li>
</ul>
<h3>Chat Interface</h3>
<ul>
<li><code>POST /api/ollama/chat</code> - Chat with the assistant (streaming)</li>
<li><code>GET /api/ollama/assistant/context/{path}</code> - Get context for path</li>
</ul>
<h2>Frontend Integration</h2>
<h3>Using the Composable</h3>
<pre><code class="hljs language-typescript"><span class="hljs-keyword">import</span> { useOllama } <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;../composables/useOllama&quot;</span>;

<span class="hljs-keyword">function</span> <span class="hljs-title function_">MyComponent</span>(<span class="hljs-params"></span>) {
  <span class="hljs-keyword">const</span> {
    isConnected,
    isStreaming,
    chatWithAssistant,
    conversationHistory,
    <span class="hljs-comment">// ... other methods</span>
  } = <span class="hljs-title function_">useOllama</span>();

  <span class="hljs-comment">// Send a message to the assistant</span>
  <span class="hljs-keyword">const</span> <span class="hljs-title function_">handleChat</span> = <span class="hljs-keyword">async</span> (<span class="hljs-params"></span>) =&gt; {
    <span class="hljs-keyword">await</span> <span class="hljs-title function_">chatWithAssistant</span>(<span class="hljs-string">&quot;How do I organize my dataset?&quot;</span>, {
      <span class="hljs-attr">current_path</span>: <span class="hljs-string">&quot;/my/dataset/path&quot;</span>,
      <span class="hljs-attr">selected_images</span>: [<span class="hljs-string">&quot;image1.jpg&quot;</span>, <span class="hljs-string">&quot;image2.png&quot;</span>]
    });
  };

  <span class="hljs-keyword">return</span> (
    <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">div</span>&gt;</span>
      {/* Your component UI */}
    <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span>
  );
}</code></pre><h3>Adding the Assistant Component</h3>
<pre><code class="hljs language-typescript"><span class="hljs-keyword">import</span> { <span class="hljs-title class_">YipYapAssistant</span> } <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;../components/YipYapAssistant&quot;</span>;

<span class="hljs-keyword">function</span> <span class="hljs-title function_">MyPage</span>(<span class="hljs-params"></span>) {
  <span class="hljs-keyword">return</span> (
    <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">div</span>&gt;</span>
      {/* Your page content */}

      <span class="hljs-tag">&lt;<span class="hljs-name">YipYapAssistant</span>
        <span class="hljs-attr">currentPath</span>=<span class="hljs-string">&quot;/current/dataset/path&quot;</span>
        <span class="hljs-attr">selectedImages</span>=<span class="hljs-string">{[</span>&quot;<span class="hljs-attr">selected1.jpg</span>&quot;, &quot;<span class="hljs-attr">selected2.png</span>&quot;]}
      /&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span>
  );
}</code></pre><h2>Configuration</h2>
<h3>Assistant Personality</h3>
<p>The yipyap assistant is configured with a custom system prompt that:</p>
<ul>
<li>Identifies as 🦊 (fox emoji)</li>
<li>Specializes in dataset management tasks</li>
<li>Provides helpful, technical guidance</li>
<li>Maintains a friendly but professional tone</li>
<li>Focuses on yipyap-specific workflows</li>
</ul>
<h3>Model Recommendations</h3>
<p><strong>For Development/Testing:</strong></p>
<ul>
<li><code>qwen3:8b</code> (8B) - Fast, good for all queries</li>
</ul>
<h2>Troubleshooting</h2>
<h3>Common Issues</h3>
<p><strong>Assistant Shows as Offline:</strong></p>
<ol>
<li>Verify Ollama is running: <code>ollama list</code></li>
<li>Check the service: <code>ollama serve</code></li>
<li>Verify the URL in environment variables</li>
<li>Check firewall settings for port 11434</li>
</ol>
<p><strong>Slow Responses:</strong></p>
<ol>
<li>Use a smaller model (llama3.2 vs llama3.1:8b)</li>
<li>Ensure adequate system resources</li>
<li>Check if other applications are using GPU/CPU</li>
</ol>
<p><strong>Model Not Found:</strong></p>
<ol>
<li>Pull the model: <code>ollama pull model-name</code></li>
<li>Verify model name in settings</li>
<li>Refresh the models list in the UI</li>
</ol>
<p><strong>Connection Errors:</strong></p>
<ol>
<li>Verify Ollama server is accessible</li>
<li>Check network settings and firewall</li>
<li>Ensure correct OLLAMA_BASE_URL</li>
</ol>
<h3>Performance Tips</h3>
<ol>
<li><p><strong>Memory Management:</strong></p>
<ul>
<li>Ollama loads models into memory</li>
<li>Larger models require more RAM</li>
<li>Only keep needed models pulled</li>
</ul>
</li>
<li><p><strong>GPU Acceleration:</strong></p>
<ul>
<li>Ollama automatically uses GPU if available</li>
<li>Ensure proper GPU drivers are installed</li>
<li>Monitor GPU memory usage</li>
</ul>
</li>
<li><p><strong>Response Speed:</strong></p>
<ul>
<li>Smaller models respond faster</li>
<li>Local models are always faster than API calls</li>
<li>Consider model size vs. quality tradeoffs</li>
</ul>
</li>
</ol>
<h2>Development</h2>
<h3>Adding New Features</h3>
<p>The Ollama integration is modular and extensible:</p>
<ol>
<li><strong>Backend:</strong> Add new endpoints in <code>app/api/ollama.py</code></li>
<li><strong>Frontend:</strong> Extend the composable in <code>src/composables/useOllama.ts</code></li>
<li><strong>Assistant:</strong> Modify system prompt in <code>app/ollama_integration.py</code></li>
</ol>
<h3>Custom Prompts</h3>
<p>You can create custom prompts for specific use cases:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># In YipYapAssistant class</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">create_specialized_prompt</span>(<span class="hljs-params">self, task_type: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:
    base_prompt = <span class="hljs-variable language_">self</span>.system_prompt

    <span class="hljs-keyword">if</span> task_type == <span class="hljs-string">&quot;tagging&quot;</span>:
        <span class="hljs-keyword">return</span> base_prompt + <span class="hljs-string">&quot;\n\nFocus specifically on image tagging best practices...&quot;</span>
    <span class="hljs-keyword">elif</span> task_type == <span class="hljs-string">&quot;organization&quot;</span>:
        <span class="hljs-keyword">return</span> base_prompt + <span class="hljs-string">&quot;\n\nProvide detailed dataset organization strategies...&quot;</span>

    <span class="hljs-keyword">return</span> base_prompt</code></pre><h2>Security Considerations</h2>
<ol>
<li><strong>Local Processing:</strong> All data stays on your local machine</li>
<li><strong>No API Keys:</strong> No external service dependencies</li>
<li><strong>Privacy:</strong> Conversations are not logged or transmitted</li>
<li><strong>Access Control:</strong> Uses existing yipyap authentication</li>
</ol>
<h2>Future Enhancements</h2>
<p>Planned improvements include:</p>
<ul>
<li>Multi-modal support (image + text input)</li>
<li>Integration with caption generation models</li>
<li>Custom tool/function calling</li>
<li>Dataset analysis and insights</li>
<li>Automated workflow suggestions</li>
<li>Integration with tagging systems</li>
</ul>
<h2>Contributing</h2>
<p>To contribute to the Ollama integration:</p>
<ol>
<li>Follow the existing code patterns</li>
<li>Add appropriate error handling</li>
<li>Include type definitions</li>
<li>Update documentation</li>
<li>Test with multiple models</li>
<li>Consider performance implications</li>
</ol>
<h2>License</h2>
<p>The Ollama integration follows the same MIT license as the rest of YipYap.</p>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>