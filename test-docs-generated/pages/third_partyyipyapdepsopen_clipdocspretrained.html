<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>## Pretrained model results - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>## Pretrained model results</h1>
      <div class="markdown-content"><h2>Pretrained model results</h2>
<p>We evaluate the full collection of available models on a suite of 38 datasets in a zero-shot setting (i.e., without fine-tuning), following <a href="https://arxiv.org/abs/2304.14108">Gadre et al., 2023</a>.<br>Click below to see the full results.</p>
<ul>
<li><a href="openclip_results.csv">Full results (English)</a></li>
<li><a href="openclip_classification_results.csv">Classification-only results</a></li>
<li><a href="openclip_retrieval_results.csv">Retrieval results</a></li>
<li><a href="openclip_multilingual_retrieval_results.csv">Multilingual retrieval results</a></li>
</ul>
<h2>Pretrained model details</h2>
<p>Below are details for several of our pretrained models.</p>
<h3>LAION-400M - <a href="https://laion.ai/laion-400-open-dataset">https://laion.ai/laion-400-open-dataset</a></h3>
<p>We ran experiments in an attempt to reproduce OpenAI&#39;s ViT results with the comparably sized (and open) LAION-400M dataset. Trained<br>weights can be found in release <a href="https://github.com/mlfoundations/open_clip/releases/tag/v0.2-weights">v0.2</a>.</p>
<p>The LAION400M weights have been trained on the JUWELS supercomputer (see acknowledgements section below).</p>
<h4>ViT-B/32 224x224</h4>
<p>We replicate OpenAI&#39;s results on ViT-B/32, reaching a top-1 ImageNet-1k zero-shot accuracy of 62.96%.</p>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot.png" width="700">

<p><strong>Zero-shot comparison (courtesy of Andreas Fürst)</strong><br><img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_openai_compare_b32.jpg" width="700"></p>
<p>ViT-B/32 was trained with 128 A100 (40 GB) GPUs for <del>36 hours, 4600 GPU-hours. The per-GPU batch size was 256 for a global batch size of 32768. 256 is much lower than it could have been (</del>320-384) due to being sized initially before moving to &#39;local&#39; contrastive loss.</p>
<h4>ViT-B/16 224x224</h4>
<p>The B/16 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 67.07.</p>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16.png" width="700">

<p>This was the first major train session using the updated webdataset 0.2.x code. A bug was found that prevented shards from being shuffled properly between nodes/workers each epoch. This was fixed part way through training (epoch 26) but likely had an impact.</p>
<p>ViT-B/16 was trained with 176 A100 (40 GB) GPUS for ~61 hours, 10700 GPU-hours. Batch size per GPU was 192 for a global batch size of 33792.</p>
<h4>ViT-B/16+ 240x240</h4>
<p>The B/16+ 240x240 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 69.21.</p>
<p>This model is the same depth as the B/16, but increases the</p>
<ul>
<li>vision width from 768 -&gt; 896</li>
<li>text width from 512 -&gt; 640</li>
<li>the resolution 224x224 -&gt; 240x240 (196 -&gt; 225 tokens)</li>
</ul>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16_plus_240.png" width="700">

<p>Unlike the B/16 run above, this model was a clean run with no dataset shuffling issues.</p>
<p>ViT-B/16+ was trained with 224 A100 (40 GB) GPUS for ~61 hours, 13620 GPU-hours. Batch size per GPU was 160 for a global batch size of 35840.</p>
<h4>ViT-L/14 224x224</h4>
<p>The L/14 LAION-400M training reached a top-1 ImageNet-1k zero-shot validation score of 72.77.</p>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_l14.png" width="700">

<p>ViT-L/14 was trained with 400 A100 (40 GB) GPUS for ~127 hours, 50800 GPU-hours. Batch size per GPU was 96 for a global batch size of 38400. Grad checkpointing was enabled.</p>
<h3>LAION-2B (en) - <a href="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/">https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/</a></h3>
<p>A ~2B sample subset of LAION-5B with english captions (<a href="https://huggingface.co/datasets/laion/laion2B-en">https://huggingface.co/datasets/laion/laion2B-en</a>)</p>
<h4>ViT-B/32 224x224</h4>
<p>A ViT-B/32 trained on LAION-2B, reaching a top-1 ImageNet-1k zero-shot accuracy of 65.62%.</p>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion2b_clip_zeroshot_b32.png" width="700">

<p>ViT-B/32 was trained with 112 A100 (40 GB) GPUs. The per-GPU batch size was 416 for a global batch size of 46592. Compute generously provided by <a href="https://stability.ai/">stability.ai</a>.</p>
<p>A second iteration of B/32 was trained on stability.ai cluster with a larger global batch size and learning rate, hitting 66.6% top-1. See <a href="https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K">https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K</a></p>
<h4>ViT-L/14 224x224</h4>
<p>A ViT-L/14 with a 75.3% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K">https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K</a></p>
<p>These weights use a different dataset mean and std than others. Instead of using the OpenAI mean &amp; std, inception style normalization <code>[-1, 1]</code> is used via a mean and std of <code>[0.5, 0.5, 0.5]</code>. This is handled automatically if using <code>open_clip.create_model_and_transforms</code> from pretrained weights.</p>
<h4>ViT-H/14 224x224</h4>
<p>A ViT-H/14 with a 78.0% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K">https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K</a></p>
<h4>ViT-g/14 224x224</h4>
<p>A ViT-g/14 with a 76.6% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K">https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K</a></p>
<p>This model was trained with a shorted schedule than other LAION-2B models with 12B samples seen instead of 32+B. It matches LAION-400M training in samples seen. Many zero-shot results are lower as a result, but despite this it performs very well in some OOD zero-shot and retrieval tasks.</p>
<h4>ViT-B/32 roberta base</h4>
<p>A ViT-B/32 with roberta base encoder with a 61.7% top-1 ImageNet-1k zero-shot was trained on stability. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k">https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k</a><br>This is the first openclip model using a HF text tower. It has better performance on a range of tasks compared to the standard text encoder, see <a href="https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k/blob/main/unknown.png">metrics</a></p>
<h4>ViT-B/32 xlm roberta base</h4>
<p>A ViT-B/32 with xlm roberta base encoder with a 62.33% top-1 ImageNet-1k zero-shot was trained on stability. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k">https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k</a><br>This is the first openclip model trained on the full laion5B dataset; hence the first multilingual clip trained with openclip. It has better performance on a range of tasks compared to the standard text encoder, see <a href="https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k/blob/main/metrics.png">metrics</a><br>A preliminary multilingual evaluation was run: 43% on imagenet1k italian (vs 21% for english B/32), 37% for imagenet1k japanese (vs 1% for english B/32 and 50% for B/16 clip japanese). It shows the multilingual property is indeed there as expected. Larger models will get even better performance.</p>
<h4>ViT-H/14 xlm roberta large</h4>
<p>A ViT-H/14 with xlm roberta large encoder with a 77.0% (vs 78% for the english equivalent) top-1 ImageNet-1k zero-shot was trained on stability. See model details here <a href="https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k">https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k</a></p>
<p>This model was trained following the <a href="https://arxiv.org/abs/2111.07991">LiT</a> methodology: the image tower was frozen (initialized from english openclip ViT-H/14), the text tower was initialized from <a href="https://huggingface.co/xlm-roberta-large">xlm roberta large</a> and unfrozen. This reduced training cost by a 3x factor.</p>
<p>See full english <a href="https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/resolve/main/results_xlm_roberta_large.png">metrics</a></p>
<p>On zero shot classification on imagenet with translated prompts this model reaches:</p>
<ul>
<li>56% in italian (vs 21% for <a href="https://github.com/clip-italian/clip-italian">https://github.com/clip-italian/clip-italian</a>)</li>
<li>53% in japanese (vs 54.6% for <a href="https://github.com/rinnakk/japanese-clip">https://github.com/rinnakk/japanese-clip</a>)</li>
<li>55.7% in chinese (to be compared with <a href="https://github.com/OFA-Sys/Chinese-CLIP">https://github.com/OFA-Sys/Chinese-CLIP</a>)</li>
</ul>
<h4>YFCC-15M</h4>
<p>Below are checkpoints of models trained on YFCC-15M, along with their zero-shot top-1 accuracies on ImageNet and ImageNetV2. These models were trained using 8 GPUs and the same hyperparameters described in the &quot;Sample running code&quot; section, with the exception of <code>lr=5e-4</code> and <code>epochs=32</code>.</p>
<ul>
<li><a href="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt">ResNet-50</a> (32.7% / 27.9%)</li>
<li><a href="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt">ResNet-101</a> (34.8% / 30.0%)</li>
</ul>
<h4>CC12M - <a href="https://github.com/google-research-datasets/conceptual-12m">https://github.com/google-research-datasets/conceptual-12m</a></h4>
<ul>
<li><a href="https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt">ResNet-50</a> (36.45%)</li>
</ul>
<h3>CommonPool and DataComp models</h3>
<p>As part of <a href="https://github.com/mlfoundations/datacomp">DataComp</a>, we trained models on CommonPool using various data filtering strategies.</p>
<p>The best performing models are specified below for the xlarge scale, see our paper <a href="https://arxiv.org/abs/2304.14108">DataComp: In seearch of the next generation of multimodal datasets</a> for more details.</p>
<p>Additional models and more information can be found at <a href="/docs/datacomp_models.md">/docs/datacomp_models.md</a>.</p>
<ul>
<li><p><code>datacomp_xl_s13b_b90k</code>: A ViT-L/14 trained on DataComp-1B for 12.8B steps and batch size 90k. Achieves 79.2% zero-shot accuracy on ImageNet. Available at <a href="https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K">https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K</a>.</p>
</li>
<li><p><code>commonpool_xl_clip_s13b_b90k</code>: A ViT-L/14 trained on CommonPool-XL filtered using CLIP scores, for 12.8B steps and batch size 90k. Achieves 76.4% zero-shot accuracy on ImageNet. Available at <a href="https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K">https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K</a>.</p>
</li>
<li><p><code>commonpool_xl_laion_s13b_b90k</code>: A ViT-L/14 trained on CommonPool-XL filtered using the LAION-2B filtering scheme, for 12.8B steps and batch size 90k. Achieves 75.5% zero-shot accuracy on ImageNet. Available at <a href="https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K">https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K</a>.</p>
</li>
<li><p><code>commonpool_xl_s13b_b90k</code>: A ViT-L/14 trained on CommonPool-XL without any filtering, for 12.8B steps and batch size 90k. Achieves 72.3% zero-shot accuracy on ImageNet. Available at <a href="https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K">https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K</a>.</p>
</li>
</ul>
<p>If you use models trained on DataComp-1B or CommonPool variations, please consider citing the following:</p>
<pre><code class="hljs">@article{datacomp,
  title={DataComp: In search of the next generation of <span class="hljs-keyword">multimodal </span>datasets},
  author={Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, <span class="hljs-keyword">Jonathan </span>Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, <span class="hljs-keyword">Jieyu </span>Zhang, Eyal <span class="hljs-keyword">Orgad, </span>Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan <span class="hljs-keyword">Bitton, </span>Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, <span class="hljs-keyword">Shuran </span>Song, Hannaneh Hajishirzi, Ali Farhadi, Romain <span class="hljs-keyword">Beaumont, </span>Sewoong Oh, Alex <span class="hljs-keyword">Dimakis, </span><span class="hljs-keyword">Jenia </span><span class="hljs-keyword">Jitsev, </span>Yair Carmon, Vaishaal <span class="hljs-keyword">Shankar, </span>Ludwig <span class="hljs-keyword">Schmidt},
</span>  <span class="hljs-keyword">journal={arXiv </span>preprint arXiv:<span class="hljs-number">2304</span>.<span class="hljs-number">14108</span>},
  year={<span class="hljs-number">2023</span>}
}</code></pre><h3>MetaCLIP</h3>
<p>MetaCLIP models are described in the paper <a href="https://arxiv.org/abs/2309.16671">Demystifying CLIP Data</a>.<br>These models were developed by Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer from Meta, New York University and the University of Washington.</p>
<p>Models are licensed under CC-BY-NC.<br>More details are available at <a href="https://github.com/facebookresearch/MetaCLIP">https://github.com/facebookresearch/MetaCLIP</a>.</p>
<p>If you use MetaCLIP models, please cite the following:</p>
<pre><code class="hljs"><span class="language-xml">@inproceedings</span><span class="hljs-template-variable">{xu2023metaclip,
   title={Demystifying CLIP Data}</span><span class="language-xml">,
   author=</span><span class="hljs-template-variable">{Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu, Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer and Christoph Feichtenhofer}</span><span class="language-xml">,
   journal=</span><span class="hljs-template-variable">{arXiv preprint arXiv:2309.16671}</span><span class="language-xml">,
   year=</span><span class="hljs-template-variable">{2023}</span><span class="language-xml">
}</span></code></pre><h3>EVA-CLIP</h3>
<p>EVA-CLIP models are described in the paper <a href="https://arxiv.org/abs/2303.15389">EVA-CLIP: Improved Training Techniques for CLIP at Scale</a>.<br>These models were developed by Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang and Yue Cao from BAAI and HUST.</p>
<p>Models are licensed under the MIT License.<br>More details are available at <a href="https://github.com/baaivision/EVA/tree/master/EVA-CLIP">https://github.com/baaivision/EVA/tree/master/EVA-CLIP</a>.</p>
<p>If you use EVA models, please cite the following:</p>
<pre><code class="hljs">@article{EVA-CLIP,
  title={EVA-CLIP: Improved Training Techniques for CLIP <span class="hljs-built_in">at</span> <span class="hljs-keyword">Scale},
</span>  author={Sun, Quan <span class="hljs-keyword">and </span>Fang, Yuxin <span class="hljs-keyword">and </span>Wu, Ledell <span class="hljs-keyword">and </span>Wang, Xinlong <span class="hljs-keyword">and </span>Cao, Yue},
  <span class="hljs-keyword">journal={arXiv </span>preprint arXiv:<span class="hljs-number">2303</span>.<span class="hljs-number">15389</span>},
  year={<span class="hljs-number">2023</span>}
}</code></pre><h3>NLLB-CLIP</h3>
<p>NLLB-CLIP models are described in the paper <a href="https://arxiv.org/abs/2309.01859">NLLB-CLIP - train performant multilingual image retrieval model on a budget</a> by Alexander Visheratin.</p>
<p>The model was trained following the <a href="https://arxiv.org/abs/2111.07991">LiT</a> methodology: the image tower was frozen, the text tower was initialized from the <a href="https://arxiv.org/abs/2207.04672">NLLB</a> encoder and unfrozen.</p>
<p>The model was trained on the <a href="https://huggingface.co/datasets/visheratin/laion-coco-nllb">LAION-COCO-NLLB</a> dataset.</p>
<p>The first version of the model (<code>nllb-clip</code>) described in the paper was trained using the OpenAI CLIP image encoder.</p>
<p>The second version of the model (<code>nllb-clip-siglip</code>) was trained using the <a href="https://arxiv.org/abs/2303.15343">SigLIP</a> image encoder.</p>
<p>Models are licensed under CC-BY-NC.</p>
<p>If you use NLLB-CLIP models, please cite the following:</p>
<pre><code class="hljs">@article{visheratin2023nllb,
  title={NLLB-CLIP<span class="hljs-comment">--train performant multilingual image retrieval model on a budget},</span>
  author={Visheratin, Alexander},
  journal={arXiv preprint arXiv:<span class="hljs-number">2309.01859</span>},
  <span class="hljs-built_in">year</span>={<span class="hljs-number">2023</span>}
}</code></pre><h3>CLIPA</h3>
<p>CLIPA models are described in the following papers by Xianhang Li, Zeyu Wang, Cihang Xie from UC Santa Cruz:</p>
<ul>
<li><a href="https://arxiv.org/abs/2305.07017">An Inverse Scaling Law for CLIP Training</a></li>
<li><a href="https://arxiv.org/abs/2306.15658">CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy</a></li>
</ul>
<p>Models are licensed under Apache 2.0.<br>More details are available at <a href="https://github.com/UCSC-VLAA/CLIPA">https://github.com/UCSC-VLAA/CLIPA</a> and <a href="clipa.md">here</a>.</p>
<p>If you use CLIPA models, please cite the following:</p>
<pre><code class="hljs"><span class="language-xml">@inproceedings</span><span class="hljs-template-variable">{li2023clipa,
      title={An Inverse Scaling Law for CLIP Training}</span><span class="language-xml">,
      author=</span><span class="hljs-template-variable">{Xianhang Li and Zeyu Wang and Cihang Xie}</span><span class="language-xml">,
      booktitle=</span><span class="hljs-template-variable">{NeurIPS}</span><span class="language-xml">,
      year=</span><span class="hljs-template-variable">{2023}</span><span class="language-xml">,
}</span></code></pre><pre><code class="hljs">@article{li2023clipav2,
      title={CLIPA-v2: <span class="hljs-keyword">Scaling </span>CLIP Training with <span class="hljs-number">81</span>.<span class="hljs-number">1</span>% <span class="hljs-built_in">Zero</span>-<span class="hljs-keyword">shot </span>ImageNet Accuracy within a $<span class="hljs-number">10</span>,<span class="hljs-number">000</span> <span class="hljs-keyword">Budget; </span>An <span class="hljs-keyword">Extra </span>$<span class="hljs-number">4</span>,<span class="hljs-number">000</span> Unlocks <span class="hljs-number">81</span>.<span class="hljs-number">8</span>% Accuracy},
      author={Xianhang Li <span class="hljs-keyword">and </span>Zeyu Wang <span class="hljs-keyword">and </span>Cihang Xie},
      <span class="hljs-keyword">journal={arXiv </span>preprint arXiv:<span class="hljs-number">2306</span>.<span class="hljs-number">15658</span>},
      year={<span class="hljs-number">2023</span>},
}</code></pre><h3>SigLIP</h3>
<p>SigLIP models are described in the paper <a href="https://arxiv.org/abs/2303.15343">Sigmoid Loss for Language Image Pre-Training</a>.<br>These models were developed by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer from Google Deepmind.</p>
<p>Models are licensed under the Apache 2 license.<br>More details are available at h<a href="https://github.com/google-research/big_vision">https://github.com/google-research/big_vision</a>.</p>
<p>If you use SigLIP models, please cite the following:</p>
<pre><code class="hljs">@article{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua <span class="hljs-keyword">and </span>Mustafa, <span class="hljs-keyword">Basil </span><span class="hljs-keyword">and </span>Kolesnikov, Alexander <span class="hljs-keyword">and </span><span class="hljs-keyword">Beyer, </span>Lucas},
  <span class="hljs-keyword">journal={arXiv </span>preprint arXiv:<span class="hljs-number">2303</span>.<span class="hljs-number">15343</span>},
  year={<span class="hljs-number">2023</span>}
}</code></pre><h3>DFN</h3>
<p>Data Filtering Network models are described in <a href="https://arxiv.org/abs/2309.17425">https://arxiv.org/abs/2309.17425</a>.<br>These models were developed by Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev and Vaishaal Shankar from Apple and the University of Washington.</p>
<p>Models are licensed under the following: <a href="https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-384/blob/main/LICENSE">https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-384/blob/main/LICENSE</a>.</p>
<p>If you use DFN models, please cite the following:</p>
<pre><code class="hljs">@article{fang2023data,
  title={Data Filtering Networks},
  author={Fang, Alex <span class="hljs-keyword">and </span><span class="hljs-keyword">Jose, </span>Albin Madappally <span class="hljs-keyword">and </span><span class="hljs-keyword">Jain, </span>Amit <span class="hljs-keyword">and </span><span class="hljs-keyword">Schmidt, </span>Ludwig <span class="hljs-keyword">and </span>Toshev, Alexander <span class="hljs-keyword">and </span><span class="hljs-keyword">Shankar, </span>Vaishaal},
  <span class="hljs-keyword">journal={arXiv </span>preprint arXiv:<span class="hljs-number">2309</span>.<span class="hljs-number">17425</span>},
  year={<span class="hljs-number">2023</span>}
}</code></pre></div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>