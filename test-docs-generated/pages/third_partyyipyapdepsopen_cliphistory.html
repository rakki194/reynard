<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>## 2.24.0 - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>## 2.24.0</h1>
      <div class="markdown-content"><h2>2.24.0</h2>
<ul>
<li>Fix missing space in error message</li>
<li>use model flag for normalizing embeddings</li>
<li>init logit_bias for non siglip pretrained models</li>
<li>Fix logit_bias load_checkpoint addition </li>
<li>Make CoCa model match CLIP models for logit scale/bias init</li>
<li>Fix missing return of &quot;logit_bias&quot; in CoCa.forward</li>
<li>Add NLLB-CLIP with SigLIP models</li>
<li>Add get_logits method and NLLB tokenizer</li>
<li>Remove the empty file src/open_clip/generation_utils.py</li>
<li>Update params.py: &quot;BatchNorm&quot; -&gt; &quot;LayerNorm&quot; in the description string for &quot;--lock-text-freeze-layer-norm&quot;</li>
</ul>
<h2>2.23.0</h2>
<ul>
<li>Add CLIPA-v2 models</li>
<li>Add SigLIP models</li>
<li>Add MetaCLIP models</li>
<li>Add NLLB-CLIP models</li>
<li>CLIPA train code</li>
<li>Minor changes/fixes<ul>
<li>Remove protobuf version limit</li>
<li>Stop checking model name when loading CoCa models</li>
<li>Log native wandb step</li>
<li>Use bool instead of long masks</li>
</ul>
</li>
</ul>
<h2>2.21.0</h2>
<ul>
<li>Add SigLIP loss + training support</li>
<li>Add more DataComp models (B/16, B/32 and B/32@256)</li>
<li>Update default num workers</li>
<li>Update CoCa generation for <code>transformers&gt;=4.31</code></li>
<li>PyTorch 2.0 <code>state_dict()</code> compatibility fix for compiled models</li>
<li>Fix padding in <code>ResizeMaxSize</code></li>
<li>Convert JIT model on state dict load for <code>pretrained=&#39;filename…&#39;</code></li>
<li>Other minor changes and fixes (typos, README, dependencies, CI)</li>
</ul>
<h2>2.20.0</h2>
<ul>
<li>Add EVA models</li>
<li>Support serial worker training</li>
<li>Fix Python 3.7 compatibility</li>
</ul>
<h2>2.19.0</h2>
<ul>
<li>Add DataComp models</li>
</ul>
<h2>2.18.0</h2>
<ul>
<li>Enable int8 inference without <code>.weight</code> attribute</li>
</ul>
<h2>2.17.2</h2>
<ul>
<li>Update push_to_hf_hub</li>
</ul>
<h2>2.17.0</h2>
<ul>
<li>Add int8 support</li>
<li>Update notebook demo</li>
<li>Refactor zero-shot classification code</li>
</ul>
<h2>2.16.2</h2>
<ul>
<li>Fixes for context_length and vocab_size attributes</li>
</ul>
<h2>2.16.1</h2>
<ul>
<li>Fixes for context_length and vocab_size attributes </li>
<li>Fix --train-num-samples logic</li>
<li>Add HF BERT configs for PubMed CLIP model</li>
</ul>
<h2>2.16.0</h2>
<ul>
<li>Add improved g-14 weights</li>
<li>Update protobuf version</li>
</ul>
<h2>2.15.0</h2>
<ul>
<li>Add convnext_xxlarge weights</li>
<li>Fixed import in readme</li>
<li>Add samples per second per gpu logging</li>
<li>Fix slurm example</li>
</ul>
<h2>2.14.0</h2>
<ul>
<li>Move dataset mixtures logic to shard level</li>
<li>Fix CoCa accum-grad training</li>
<li>Safer transformers import guard</li>
<li>get_labels refactoring</li>
</ul>
<h2>2.13.0</h2>
<ul>
<li>Add support for dataset mixtures with different sampling weights</li>
<li>Make transformers optional again</li>
</ul>
<h2>2.12.0</h2>
<ul>
<li>Updated convnext configs for consistency</li>
<li>Added input_patchnorm option</li>
<li>Clean and improve CoCa generation</li>
<li>Support model distillation</li>
<li>Add ConvNeXt-Large 320x320 fine-tune weights</li>
</ul>
<h2>2.11.1</h2>
<ul>
<li>Make transformers optional</li>
<li>Add MSCOCO CoCa finetunes to pretrained models</li>
</ul>
<h2>2.11.0</h2>
<ul>
<li>coca support and weights</li>
<li>ConvNeXt-Large weights</li>
</ul>
<h2>2.10.1</h2>
<ul>
<li><code>hf-hub:org/model_id</code> support for loading models w/ config and weights in Hugging Face Hub</li>
</ul>
<h2>2.10.0</h2>
<ul>
<li>Added a ViT-bigG-14 model.</li>
<li>Added an up-to-date example slurm script for large training jobs.</li>
<li>Added a option to sync logs and checkpoints to S3 during training.</li>
<li>New options for LR schedulers, constant and constant with cooldown</li>
<li>Fix wandb autoresuming when resume is not set</li>
<li>ConvNeXt <code>base</code> &amp; <code>base_w</code> pretrained models added</li>
<li><code>timm-</code> model prefix removed from configs</li>
<li><code>timm</code> augmentation + regularization (dropout / drop-path) supported</li>
</ul>
<h2>2.9.3</h2>
<ul>
<li>Fix wandb collapsing multiple parallel runs into a single one</li>
</ul>
<h2>2.9.2</h2>
<ul>
<li>Fix braceexpand memory explosion for complex webdataset urls</li>
</ul>
<h2>2.9.1</h2>
<ul>
<li>Fix release</li>
</ul>
<h2>2.9.0</h2>
<ul>
<li>Add training feature to auto-resume from the latest checkpoint on restart via <code>--resume latest</code></li>
<li>Allow webp in webdataset</li>
<li>Fix logging for number of samples when using gradient accumulation</li>
<li>Add model configs for convnext xxlarge</li>
</ul>
<h2>2.8.2</h2>
<ul>
<li>wrapped patchdropout in a torch.nn.Module</li>
</ul>
<h2>2.8.1</h2>
<ul>
<li>relax protobuf dependency</li>
<li>override the default patch dropout value in &#39;vision_cfg&#39;</li>
</ul>
<h2>2.8.0</h2>
<ul>
<li>better support for HF models</li>
<li>add support for gradient accumulation</li>
<li>CI fixes</li>
<li>add support for patch dropout</li>
<li>add convnext configs</li>
</ul>
<h2>2.7.0</h2>
<ul>
<li>add multilingual H/14 xlm roberta large</li>
</ul>
<h2>2.6.1</h2>
<ul>
<li>fix setup.py _read_reqs</li>
</ul>
<h2>2.6.0</h2>
<ul>
<li>Make openclip training usable from pypi.</li>
<li>Add xlm roberta large vit h 14 config.</li>
</ul>
<h2>2.5.0</h2>
<ul>
<li>pretrained B/32 xlm roberta base: first multilingual clip trained on laion5B</li>
<li>pretrained B/32 roberta base: first clip trained using an HF text encoder</li>
</ul>
<h2>2.4.1</h2>
<ul>
<li>Add missing hf_tokenizer_name in CLIPTextCfg.</li>
</ul>
<h2>2.4.0</h2>
<ul>
<li>Fix #211, missing RN50x64 config. Fix type of dropout param for ResNet models</li>
<li>Bring back LayerNorm impl that casts to input for non bf16/fp16 </li>
<li>zero_shot.py: set correct tokenizer based on args</li>
<li>training/params.py: remove hf params and get them from model config</li>
</ul>
<h2>2.3.1</h2>
<ul>
<li>Implement grad checkpointing for hf model.</li>
<li>custom_text: True if hf_model_name is set</li>
<li>Disable hf tokenizer parallelism</li>
</ul>
<h2>2.3.0</h2>
<ul>
<li>Generalizable Text Transformer with HuggingFace Models (@iejMac)</li>
</ul>
<h2>2.2.0</h2>
<ul>
<li>Support for custom text tower</li>
<li>Add checksum verification for pretrained model weights</li>
</ul>
<h2>2.1.0</h2>
<ul>
<li>lot including sota models, bfloat16 option, better loading, better metrics</li>
</ul>
<h2>1.2.0</h2>
<ul>
<li>ViT-B/32 trained on Laion2B-en</li>
<li>add missing openai RN50x64 model</li>
</ul>
<h2>1.1.1</h2>
<ul>
<li>ViT-B/16+</li>
<li>Add grad checkpointing support</li>
<li>more robust data loader</li>
</ul>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>