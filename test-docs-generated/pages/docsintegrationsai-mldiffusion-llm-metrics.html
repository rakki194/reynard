<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Diffusion LLM Metrics - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>Diffusion LLM Metrics</h1>
      <div class="markdown-content"><h1>Diffusion LLM Metrics</h1>
<p>This document describes the metrics collection and export functionality for the Diffusion LLM integration in YipYap.</p>
<h2>Overview</h2>
<p>The Diffusion LLM integration includes comprehensive metrics collection for monitoring performance, usage patterns, and error rates. Metrics are stored in the SQLite metrics database and can be exported in multiple formats for integration with external monitoring systems.</p>
<h2>Metrics Collected</h2>
<h3>Performance Metrics</h3>
<ul>
<li><strong>Request Latency</strong>: Time taken for each request (generate, infill, generate/stream, infill/stream)</li>
<li><strong>Stream Duration</strong>: Total time for streaming operations</li>
<li><strong>Token Count</strong>: Estimated number of tokens generated (approximation based on character count)</li>
<li><strong>Generation Time</strong>: Internal generation time reported by the model</li>
</ul>
<h3>Usage Metrics</h3>
<ul>
<li><strong>Request Counts</strong>: Total number of requests per endpoint</li>
<li><strong>Error Rates</strong>: Success/failure rates per endpoint</li>
<li><strong>Model Usage</strong>: Which models are being used and their performance</li>
</ul>
<h3>Error Tracking</h3>
<ul>
<li><strong>Error Counts</strong>: Number of errors per endpoint type</li>
<li><strong>Error Types</strong>: Categorized errors (timeout, network, validation, etc.)</li>
<li><strong>Success Rates</strong>: Percentage of successful requests</li>
</ul>
<h2>API Endpoints</h2>
<h3>Get Metrics Summary</h3>
<pre><code class="hljs"><span class="hljs-built_in">GET</span> /api/diffusion/metrics</code></pre><p>Returns a comprehensive summary of all diffusion LLM metrics including:</p>
<ul>
<li>Overall statistics (total requests, success rate, average latency)</li>
<li>Per-endpoint error rates and request counts</li>
<li>Average latencies by event type</li>
<li>Token counts by event type</li>
<li>Recent performance events</li>
</ul>
<h3>Export Metrics</h3>
<pre><code class="hljs"><span class="hljs-keyword">GET</span> /api/diffusion/metrics/export?<span class="hljs-keyword">format</span>={<span class="hljs-keyword">format</span>}</code></pre><p>Exports metrics in various formats for external monitoring systems:</p>
<ul>
<li><code>format=json</code> (default): JSON format with full metrics data</li>
<li><code>format=prometheus</code>: Prometheus-compatible metrics format</li>
<li><code>format=influxdb</code>: InfluxDB line protocol format</li>
</ul>
<h2>Metrics Database Schema</h2>
<p>Metrics are stored in the <code>performance_events</code> table with the following structure:</p>
<pre><code class="hljs language-sql"><span class="hljs-keyword">CREATE TABLE</span> performance_events (
    id <span class="hljs-type">INTEGER</span> <span class="hljs-keyword">PRIMARY KEY</span> AUTOINCREMENT,
    model_id TEXT <span class="hljs-keyword">NOT NULL</span>,
    user_id TEXT <span class="hljs-keyword">NOT NULL</span>,
    event_type TEXT <span class="hljs-keyword">NOT NULL</span>,
    duration <span class="hljs-type">REAL</span> <span class="hljs-keyword">NOT NULL</span>,
    <span class="hljs-type">timestamp</span> <span class="hljs-type">REAL</span> <span class="hljs-keyword">NOT NULL</span>,
    metadata TEXT
);</code></pre><h3>Event Types</h3>
<ul>
<li><code>diffusion_generate</code>: Non-streaming text generation</li>
<li><code>diffusion_infill</code>: Non-streaming text infilling</li>
<li><code>diffusion_generate_stream</code>: Streaming text generation</li>
<li><code>diffusion_infill_stream</code>: Streaming text infilling</li>
</ul>
<h3>Metadata Fields</h3>
<p>The <code>metadata</code> field contains JSON data with additional context:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;token_count&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">25</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;max_new_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">64</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;temperature&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.2</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;top_p&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.9</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;alg&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;entropy&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;generation_time&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.02</span>
<span class="hljs-punctuation">}</span></code></pre><h2>Prometheus Format</h2>
<p>When exporting to Prometheus format, the following metrics are available:</p>
<pre><code class="hljs"><span class="hljs-comment"># HELP diffusion_llm_requests_total Total number of requests</span>
<span class="hljs-comment"># TYPE diffusion_llm_requests_total counter</span>
<span class="hljs-attribute">diffusion_llm_requests_total</span>{endpoint=<span class="hljs-string">&quot;generate&quot;</span>} <span class="hljs-number">10</span>
<span class="hljs-attribute">diffusion_llm_requests_total</span>{endpoint=<span class="hljs-string">&quot;infill&quot;</span>} <span class="hljs-number">5</span>

<span class="hljs-comment"># HELP diffusion_llm_errors_total Total number of errors</span>
<span class="hljs-comment"># TYPE diffusion_llm_errors_total counter</span>
<span class="hljs-attribute">diffusion_llm_errors_total</span>{endpoint=<span class="hljs-string">&quot;generate&quot;</span>} <span class="hljs-number">2</span>
<span class="hljs-attribute">diffusion_llm_errors_total</span>{endpoint=<span class="hljs-string">&quot;infill&quot;</span>} <span class="hljs-number">1</span>

<span class="hljs-comment"># HELP diffusion_llm_latency_seconds Average latency in seconds</span>
<span class="hljs-comment"># TYPE diffusion_llm_latency_seconds gauge</span>
<span class="hljs-attribute">diffusion_llm_latency_seconds</span>{event_type=<span class="hljs-string">&quot;diffusion_generate&quot;</span>} <span class="hljs-number">1</span>.<span class="hljs-number">5</span>
<span class="hljs-attribute">diffusion_llm_latency_seconds</span>{event_type=<span class="hljs-string">&quot;diffusion_infill&quot;</span>} <span class="hljs-number">2</span>.<span class="hljs-number">0</span>

<span class="hljs-comment"># HELP diffusion_llm_tokens_total Total tokens generated</span>
<span class="hljs-comment"># TYPE diffusion_llm_tokens_total counter</span>
<span class="hljs-attribute">diffusion_llm_tokens_total</span>{event_type=<span class="hljs-string">&quot;diffusion_generate&quot;</span>} <span class="hljs-number">25</span>
<span class="hljs-attribute">diffusion_llm_tokens_total</span>{event_type=<span class="hljs-string">&quot;diffusion_infill&quot;</span>} <span class="hljs-number">30</span></code></pre><h2>InfluxDB Format</h2>
<p>When exporting to InfluxDB format, metrics are formatted as line protocol:</p>
<pre><code class="hljs">diffusion_llm,<span class="hljs-attribute">type</span>=requests,endpoint=generate <span class="hljs-attribute">count</span>=10 1234567890000000000
diffusion_llm,<span class="hljs-attribute">type</span>=requests,endpoint=infill <span class="hljs-attribute">count</span>=5 1234567890000000000
diffusion_llm,<span class="hljs-attribute">type</span>=latency,event_type=diffusion_generate <span class="hljs-attribute">value</span>=1.5 1234567890000000000
diffusion_llm,<span class="hljs-attribute">type</span>=latency,event_type=diffusion_infill <span class="hljs-attribute">value</span>=2.0 1234567890000000000
diffusion_llm,<span class="hljs-attribute">type</span>=tokens,event_type=diffusion_generate <span class="hljs-attribute">count</span>=25 1234567890000000000
diffusion_llm,<span class="hljs-attribute">type</span>=tokens,event_type=diffusion_infill <span class="hljs-attribute">count</span>=30 1234567890000000000</code></pre><h2>Error Rate Calculation</h2>
<p>Error rates are calculated as success percentages:</p>
<pre><code class="hljs"><span class="hljs-attr">success_rate</span> = ((total_requests - error_count) / total_requests) * <span class="hljs-number">100</span></code></pre><p>For example:</p>
<ul>
<li>10 total requests, 2 errors = 80% success rate</li>
<li>5 total requests, 1 error = 80% success rate</li>
</ul>
<h2>Token Count Estimation</h2>
<p>Token counts are estimated using a simple character-based approximation:</p>
<pre><code class="hljs">token_count = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(<span class="hljs-keyword">text</span>)<span class="hljs-comment"> // 4)</span></code></pre><p>This provides a rough approximation where 1 token ≈ 4 characters for English text. In a production environment, you would use the actual model&#39;s tokenizer for more accurate counts.</p>
<h2>Monitoring Integration</h2>
<h3>Prometheus</h3>
<p>To integrate with Prometheus, configure a scrape job:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">scrape_configs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">job_name:</span> <span class="hljs-string">&quot;yipyap-diffusion-llm&quot;</span>
    <span class="hljs-attr">static_configs:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">targets:</span> [<span class="hljs-string">&quot;localhost:7000&quot;</span>]
    <span class="hljs-attr">metrics_path:</span> <span class="hljs-string">&quot;/api/diffusion/metrics/export&quot;</span>
    <span class="hljs-attr">params:</span>
      <span class="hljs-attr">format:</span> [<span class="hljs-string">&quot;prometheus&quot;</span>]</code></pre><h3>InfluxDB</h3>
<p>To send metrics to InfluxDB, use the line protocol endpoint:</p>
<pre><code class="hljs language-bash">curl <span class="hljs-string">&quot;http://localhost:7000/api/diffusion/metrics/export?format=influxdb&quot;</span> | \
  curl -i -XPOST <span class="hljs-string">&quot;http://influxdb:8086/write?db=yipyap&quot;</span> --data-binary @-</code></pre><h3>Grafana</h3>
<p>Create dashboards using the Prometheus or InfluxDB data sources to visualize:</p>
<ul>
<li>Request rates and error rates</li>
<li>Latency percentiles</li>
<li>Token generation rates</li>
<li>Model usage patterns</li>
</ul>
<h2>Testing</h2>
<p>Unit tests for the metrics functionality are available in:</p>
<ul>
<li><code>app/tests/test_diffusion_metrics_unit.py</code>: Core metrics logic tests</li>
<li><code>app/tests/test_diffusion_metrics.py</code>: API endpoint tests (requires diffusion LLM enabled)</li>
</ul>
<p>Run tests with:</p>
<pre><code class="hljs language-bash">python -m pytest app/tests/test_diffusion_metrics_unit.py -v</code></pre><h2>Configuration</h2>
<p>Metrics collection is enabled by default when the diffusion LLM service is active. No additional configuration is required.</p>
<p>The metrics database is stored in <code>./metrics.db</code> by default and can be configured via the <code>MetricsDatabase</code> class.</p>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>