<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Crawl and Summarize - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>Crawl and Summarize</h1>
      <div class="markdown-content"><h1>Crawl and Summarize</h1>
<p>Fetch markdown content via Firecrawl and summarize it with Ollama. Both systems provide streaming endpoints and cached storage for results.</p>
<h2>Crawl (Firecrawl)</h2>
<p>Feature‑flagged by <code>crawl_enabled</code> with base URL <code>firecrawl_base_url</code> and cache dir <code>crawl_cache_dir</code>. The service caches results per‑URL with a TTL (max_age_days), and exposes helpers to submit jobs, poll status, and direct fetch with polling.</p>
<ul>
<li><p>Endpoints (prefix <code>/api/crawl</code>):</p>
<ul>
<li><code>POST /fetch</code>: <code>{ url, max_age_days }</code> → <code>{ job_id }</code></li>
<li><code>GET /status/{job_id}</code>: returns Firecrawl status JSON</li>
<li><code>GET /direct?url=&amp;max_age_days=</code>: direct fetch; returns <code>{ url, markdown, cached?, metadata? }</code></li>
<li><code>POST /purge-cache</code>: optional <code>url</code> to purge one; otherwise purge all</li>
<li><code>GET /stream</code>: SSE; accepts <code>url</code> or an existing <code>job_id</code>; emits <code>status</code>, <code>submitted</code>, <code>done</code>, <code>error</code></li>
</ul>
</li>
<li><p>Files:</p>
<ul>
<li><code>app/api/crawl.py</code></li>
<li><code>app/services/integration/crawl_service.py</code></li>
</ul>
</li>
</ul>
<h2>Summarize</h2>
<p>Summarize markdown with Ollama, providing both non‑streaming and streaming variants. Results are normalized and persisted for later retrieval.</p>
<ul>
<li><p>Endpoints (prefix <code>/api/summarize</code>):</p>
<ul>
<li><code>POST /url</code>: <code>{ url, max_age_days, include_outline?, include_highlights? }</code> → normalized summary payload with <code>summary_id</code></li>
<li><code>GET /{summary_id}</code>: returns persisted summary</li>
<li><code>GET /stream?url=...</code>: SSE stream with events: <code>crawl_progress</code>, <code>cleaning</code>, <code>llm_tokens</code>, <code>done</code>, <code>error</code></li>
</ul>
</li>
<li><p>Files:</p>
<ul>
<li><code>app/api/summarize.py</code></li>
<li><code>app/services/integration/summarize_service.py</code></li>
</ul>
</li>
</ul>
<h2>Notes</h2>
<ul>
<li>Summarize streaming path first emits crawl progress, then streams LLM tokens as they arrive, and finally emits a structured <code>done</code> payload persisted to disk under <code>cache/summaries/</code>.</li>
<li>TTS API supports <code>speak-summary</code> using the <code>summary_id</code> payload.</li>
</ul>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>