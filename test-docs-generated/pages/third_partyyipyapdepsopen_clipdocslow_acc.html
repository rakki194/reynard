<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>As we describe in more detail below, CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow reliable scaling laws. - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>As we describe in more detail below, CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow reliable scaling laws.</h1>
      <div class="markdown-content"><p>As we describe in more detail below, CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow reliable scaling laws.</p>
<p><a href="https://arxiv.org/abs/2212.07143">Cherti et al., 2022</a> and <a href="https://arxiv.org/abs/2304.14108">Gadre et al., 2023</a> show additional discussions about the scaling behavior of CLIP models.</p>
<h2>Scaling trends</h2>
<p>The plot below shows how zero-shot performance of CLIP models varies as we scale the number of samples used for training. Zero-shot performance increases steadily for both ImageNet and <a href="https://arxiv.org/abs/1902.10811">ImageNetV2</a>, and is far from saturated at ~15M samples.</p>
<img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/scaling.png" width="700">

<h2>Why are low-accuracy CLIP models interesting?</h2>
<p><strong>TL;DR:</strong> CLIP models have high effective robustness, even at small scales.</p>
<p>CLIP models are particularly intriguing because they are more robust to natural distribution shifts (see Section 3.3 in the <a href="https://arxiv.org/abs/2103.00020">CLIP paper</a>).<br>This phenomena is illustrated by the figure below, with ImageNet accuracy on the x-axis<br>and <a href="https://arxiv.org/abs/1902.10811">ImageNetV2</a> (a reproduction of the ImageNet validation set with distribution shift) accuracy on the y-axis.<br>Standard training denotes training on the ImageNet train set and the CLIP zero-shot models<br>are shown as stars.</p>
<p><img src="https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/effective_robustness.png" alt="CLIP scatter plot"></p>
<p>As observed by <a href="https://arxiv.org/abs/2007.00644">Taori et al., 2020</a> and <a href="https://arxiv.org/abs/2107.04649">Miller et al., 2021</a>, the in-distribution<br>and out-of-distribution accuracies of models trained on ImageNet follow a predictable linear trend (the red line in the above plot). <em>Effective robustness</em><br>quantifies robustness as accuracy beyond this baseline, i.e., how far a model lies above the red line. Ideally a model would not suffer from distribution shift and fall on the y = x line (<a href="http://proceedings.mlr.press/v119/shankar20c.html">trained human labelers are within a percentage point of the y = x line</a>).</p>
<p>Even though the CLIP models trained with<br>this codebase achieve much lower accuracy than those trained by OpenAI, our models still lie on the same<br>trend of improved effective robustness (the purple line). Therefore, we can study what makes<br>CLIP robust without requiring industrial-scale compute.</p>
<p>For more information on effective robustness, please see:</p>
<ul>
<li><a href="https://arxiv.org/abs/1902.10811">Recht et al., 2019</a>.</li>
<li><a href="https://arxiv.org/abs/2007.00644">Taori et al., 2020</a>.</li>
<li><a href="https://arxiv.org/abs/2107.04649">Miller et al., 2021</a>.</li>
</ul>
<p>To know more about the factors that contribute to CLIP&#39;s robustness refer to <a href="https://arxiv.org/abs/2205.01397">Fang et al., 2022</a>.</p>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>