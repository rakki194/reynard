<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>yipyap - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>yipyap</h1>
      <div class="markdown-content"><h1>yipyap</h1>
<p><img src="docs/yipyap_demo_optimized.gif" alt="Yipyap Demo"></p>
<hr>
<p><a href="#installation">➡ Jump to installation and usage ⬅</a></p>
<h2>Introduction</h2>
<p><strong>Y</strong>our <strong>I</strong>ntuitive <strong>P</strong>latform for <strong>Y</strong>ielding, <strong>A</strong>nnotating, and <strong>P</strong>rocessing or 🦊 <code>yipyap</code> for short is a web application for uploading, browsing and managing image, audio and video dataset directories with caption support, generating and caching thumbnails, running various tagging and captioning models, editing dataset configuration and sample prompts, built with Python and SolidJS.</p>
<p>The frontend of yipyap is built with SolidJS, a reactive JavaScript framework that emphasizes fine-grained reactivity and performance, using Vite as the build tool for fast development and optimized production builds. The application follows a component-based architecture with a central app context managing global state. The main entry point is <code>/src/main.tsx</code>, which sets up routing and the app context, while routes live in <code>/src/router.ts</code>. The core application state management resides in <code>/src/contexts/app.tsx</code>, which handles theme management, locale/translation management, settings persistence, notification system, and various feature flags and configurations. The icon map with all the fluent ui and other custom icons including the favicon is in <code>/src/icons/index.tsx</code>.</p>
<p>Components are organized in feature-based directories under <code>/src/components/</code>, with CSS modules or shared stylesheets for styling. Global styles are defined in <code>/src/styles.css</code>, while theme-specific styles are in <code>/src/themes.css</code>. All tests are located in the same directory as the component they are testing, with the test utilities and setup located in <code>/src/test/</code>. Backend Python tests are located in <code>/app/tests/</code>.</p>
<h2>Table of Contents</h2>
<ul>
<li><a href="#yipyap">yipyap</a><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#features">Features</a><ul>
<li><a href="#core-features">Core Features</a></li>
<li><a href="#image-viewing">Image Viewing</a></li>
<li><a href="#text-and-code-viewing">Text and Code Viewing</a></li>
<li><a href="#file-management">File Management</a></li>
<li><a href="#captions--tags">Captions &amp; Tags</a></li>
<li><a href="#languages">Languages</a></li>
<li><a href="#object-detection-and-bounding-box-annotation">Object Detection and Bounding Box Annotation</a><ul>
<li><a href="#core-annotation-features">Core Annotation Features</a></li>
<li><a href="#supported-detection-models">Supported Detection Models</a><ul>
<li><a href="#yolo-based-object-detection">YOLO-based Object Detection</a></li>
<li><a href="#watermark-detection-joycaption">Watermark Detection (Joycaption)</a></li>
<li><a href="#florence-2-vision-language-models">Florence-2 Vision-Language Models</a></li>
</ul>
</li>
<li><a href="#florence-2-model-variants">Florence-2 Model Variants</a><ul>
<li><a href="#microsoft-official-models">Microsoft Official Models</a></li>
<li><a href="#specialized-models">Specialized Models</a></li>
</ul>
</li>
<li><a href="#advanced-features">Advanced Features</a><ul>
<li><a href="#draggable-panel-interface">Draggable Panel Interface</a></li>
<li><a href="#prediction-workflow">Prediction Workflow</a></li>
<li><a href="#custom-model-support">Custom Model Support</a></li>
<li><a href="#smart-labeling-system">Smart Labeling System</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#development">Development</a><ul>
<li><a href="#environment-variables">Environment Variables</a></li>
</ul>
</li>
<li><a href="#developer-documentation">Developer Documentation</a><ul>
<li><a href="#project-structure">Project Structure</a></li>
<li><a href="#key-components">Key Components</a></li>
</ul>
</li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgements">Acknowledgements</a><ul>
<li><a href="#caption-generation-and-tagging-models">Caption Generation and Tagging Models</a><ul>
<li><a href="#joint-tagger-project-jtp2">Joint Tagger Project (JTP2)</a></li>
<li><a href="#wd-14-tagger-wdv3">WD-1.4 Tagger (WDv3)</a></li>
</ul>
</li>
<li><a href="#object-detection-and-vision-language-models">Object Detection and Vision-Language Models</a><ul>
<li><a href="#florence-2">Florence-2</a></li>
<li><a href="#furrence-2-large">Furrence-2-Large</a></li>
<li><a href="#owlv2-open-world-localization">OWLv2 (Open-World Localization)</a></li>
<li><a href="#yolo-you-only-look-once">YOLO (You Only Look Once)</a></li>
<li><a href="#joycaption-watermark-detection">Joycaption Watermark Detection</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#getting-help">Getting Help</a></li>
<li><a href="#backend-architecture">Backend Architecture</a></li>
<li><a href="#test-organization">Test Organization</a></li>
</ul>
</li>
</ul>
<h2>Features</h2>
<h3>Core Features</h3>
<ul>
<li>Browse directories with breadcrumbs</li>
<li>View images with thumbnails and captions</li>
<li>Search and sort files easily (TODO)</li>
</ul>
<h3>Image Viewing</h3>
<ul>
<li>Support for multiple caption formats</li>
<li>View and edit image metadata</li>
<li>Keyboard shortcuts</li>
<li>Zoom and pan smoothly (Experimental toggle)</li>
<li>Navigate with minimap (Experimental toggle)</li>
</ul>
<h3>Text and Code Viewing</h3>
<ul>
<li>Monaco-based text viewer/editor for dataset files</li>
<li>Code analysis and editor utilities</li>
</ul>
<h3>File Management</h3>
<ul>
<li>Drag and drop files to upload, with progress tracking</li>
<li>Upload entire folders at once</li>
<li>Perform batch operations</li>
<li>Quick folder navigation</li>
</ul>
<h3>Captions &amp; Tags</h3>
<ul>
<li>Add captions and tags</li>
<li>Generate captions automatically</li>
<li>Auto-save your changes</li>
<li>Beautiful tag colors that match your theme</li>
<li>Edit multiple files at once (TODO)</li>
</ul>
<h3>Languages</h3>
<ul>
<li><p>Available in multiple languages</p>
</li>
<li><p>Right-to-left support</p>
</li>
<li><p>Locale-aware formatting</p>
</li>
<li><p><strong>Browse and organize</strong> your image collection with an intuitive web interface</p>
</li>
<li><p><strong>Powerful search</strong> capabilities with tag filtering and smart suggestions</p>
</li>
<li><p><strong>Batch operations</strong> for moving, deleting, and organizing images efficiently</p>
</li>
<li><p><strong>Advanced image editing</strong> with cropping, rotation, and format conversion</p>
</li>
<li><p><strong>Smart captioning</strong> with multiple AI model support (JTP2, WDv3, Florence-2)</p>
</li>
<li><p><strong>Tagging system</strong> with autocomplete and color-coded tags</p>
</li>
<li><p><strong>Bounding box labeling</strong> with object detection models including Florence-2</p>
</li>
<li><p><strong>Thumbnail generation</strong> and preview optimization</p>
</li>
<li><p><strong>Responsive design</strong> that works on desktop and mobile devices</p>
</li>
</ul>
<h3>Object Detection and Bounding Box Annotation</h3>
<p>YipYap includes a sophisticated bounding box editor with an intuitive interface for creating, editing, and managing object annotations. The system supports multiple detection models and provides a comprehensive annotation workflow.</p>
<h4>Core Annotation Features</h4>
<ul>
<li><strong>Interactive Bounding Box Creation</strong>: Click and drag to create new bounding boxes with real-time visual feedback</li>
<li><strong>Advanced Editing with Fabric.js</strong>: Interactive resize and move operations with corner handles and drag functionality  </li>
<li><strong>Label Management</strong>: Create, edit, and organize custom label classes with persistent storage</li>
<li><strong>Color-Coded Labels</strong>: Automatic generation of perceptually uniform colors using OKLCH color space</li>
<li><strong>Fabric.js Integration</strong>: Professional-grade editing experience with smooth animations and precise controls</li>
<li><strong>Keyboard Shortcuts</strong>: Escape to cancel operations, efficient navigation between elements</li>
<li><strong>Model Predictions</strong>: AI-powered suggestion system with confidence thresholds</li>
<li><strong>Batch Operations</strong>: Accept all predictions or individual selections for efficient annotation</li>
</ul>
<h4>Supported Detection Models</h4>
<h5>YOLO-based Object Detection</h5>
<ul>
<li>Traditional object detection for general-purpose use cases</li>
<li>Real-time inference with configurable confidence thresholds</li>
<li>Support for custom trained YOLO models</li>
<li>Multiple input resolutions and augmentation options</li>
</ul>
<h5>Watermark Detection (Joycaption)</h5>
<ul>
<li>Specialized hybrid model combining YOLO object detection with OWLv2 classification</li>
<li>Dual-stage detection: YOLO for localization, OWLv2 for classification verification</li>
<li>Optimized for identifying watermarks and copyright markings in images</li>
<li>Binary classification with high accuracy watermark detection</li>
</ul>
<h5>Florence-2 Vision-Language Models</h5>
<ul>
<li>Conversational AI interface for natural language prompting</li>
<li>Multi-task capabilities including object detection, region description, and visual grounding</li>
<li>Support for custom prompts beyond predefined tasks</li>
<li>Advanced generation parameters (token limits, beam search, sampling)</li>
</ul>
<h4>Florence-2 Model Variants</h4>
<h5>Microsoft Official Models</h5>
<ul>
<li><code>microsoft/Florence-2-base</code> - Base model (0.23B parameters)</li>
<li><code>microsoft/Florence-2-large</code> - Large model (0.77B parameters)  </li>
<li><code>microsoft/Florence-2-base-ft</code> - Fine-tuned base model</li>
<li><code>microsoft/Florence-2-large-ft</code> - Fine-tuned large model</li>
</ul>
<h5>Specialized Models</h5>
<ul>
<li><code>HuggingFaceM4/Florence-2-DocVQA</code> - Document visual question answering</li>
<li><code>MiaoshouAI/Florence-2-base-PromptGen-v1.5</code> - Prompt generation (base)</li>
<li><code>MiaoshouAI/Florence-2-large-PromptGen-v1.5</code> - Prompt generation (large)</li>
<li><code>thwri/CogFlorence-2.2-Large</code> - Enhanced cognitive capabilities</li>
<li><code>gokaygokay/Florence-2-SD3-Captioner</code> - Stable Diffusion 3 style captions</li>
<li><code>gokaygokay/Florence-2-Flux-Large</code> - Flux model style captions  </li>
<li><code>NikshepShetty/Florence-2-pixelpros</code> - Pixel-level scene understanding</li>
</ul>
<h4>Advanced Features</h4>
<h5>Draggable Panel Interface</h5>
<ul>
<li>Resizable and repositionable UI panels for label management</li>
<li>Model prediction panel with real-time results</li>
<li>Label class management with inline editing</li>
<li>Persistent panel positions and sizes</li>
</ul>
<h5>Prediction Workflow</h5>
<ul>
<li>Run AI models on images with customizable confidence thresholds</li>
<li>Preview predictions with color-coded overlays before accepting</li>
<li>Individual prediction acceptance or batch operations</li>
<li>Integration of prediction labels into custom label class system</li>
</ul>
<h5>Custom Model Support</h5>
<ul>
<li>Load custom Florence-2 models from local paths or HuggingFace Hub</li>
<li>Automatic model type detection and appropriate UI configuration</li>
<li>Support for fine-tuned models and domain-specific variants</li>
</ul>
<h5>Smart Labeling System</h5>
<ul>
<li>Automatic color generation for consistent visual hierarchy</li>
<li>Theme-aware color schemes that adapt to dark/light modes</li>
<li>Perceptually uniform color distribution using OKLCH color space</li>
<li>Label positioning optimization to avoid overlap with bounding boxes</li>
</ul>
<h2>Installation</h2>
<p>Requirement: Python &gt;=3.9</p>
<ol>
<li><p>Download the latest release (right sidebar on github, download <code>yipyap-vx.y.z.zip</code>, not the source code) and unzip it.</p>
</li>
<li><p>In the decompressed yipyap folder, create a virtual environement and install dependencies:</p>
<ul>
<li><p>On Linux:</p>
<pre><code class="hljs language-bash">python -m venv venv
./venv/bin/pip install -r requirements.txt</code></pre></li>
<li><p>On Windows</p>
<pre><code class="hljs language-powershell">python <span class="hljs-literal">-m</span> venv venv
.\venv\Scripts\pip install <span class="hljs-literal">-r</span> requirements.txt</code></pre></li>
</ul>
<blockquote>
<p><strong>Note for Windows Users</strong>: If you encounter an error about <code>libmagic</code> not being found, run this additional command:</p>
<pre><code class="hljs language-powershell">.\venv\Scripts\pip install python<span class="hljs-literal">-magic-bin</span>
``` <span class="hljs-literal">--</span>&gt;</code></pre></blockquote>
</li>
<li><p>Run the server:</p>
<ul>
<li><p>On Linux:</p>
<pre><code class="hljs language-bash">ROOT_DIR=/path/to/your/images ./venv/bin/uvicorn app.main:app</code></pre></li>
<li><p>On Windows (PowerShell):</p>
<pre><code class="hljs language-powershell"><span class="hljs-variable">$env:ROOT_DIR</span>=<span class="hljs-string">&quot;C:\path\to\your\images&quot;</span>
.\venv\Scripts\uvicorn app.main:app</code></pre></li>
</ul>
</li>
</ol>
<p>The application will be available at <code>http://localhost:8000</code>.</p>
<p>Use <code>--port 8000</code> to set the server port, for custom server configuration refer to <a href="https://www.uvicorn.org/deployment/">uvicorn documentations</a>.</p>
<h2>Usage</h2>
<ol>
<li>Navigate to <code>http://localhost:8000</code> to start browsing the current working directory.</li>
<li>Use the controls at the top to:<ul>
<li>Search for files</li>
<li>Switch between grid and list views</li>
<li>Sort items by name, date, or size</li>
</ul>
</li>
<li>Click on images to view them in full size and edit captions.</li>
<li>Navigate directories using the breadcrumb trail or directory links.</li>
</ol>
<h2>Development</h2>
<p>Requirements: python and node.</p>
<ol>
<li><p>Clone the repository:</p>
<pre><code class="hljs language-bash">git <span class="hljs-built_in">clone</span> https://github.com/rakki194/yipyap
<span class="hljs-built_in">cd</span> yipyap</code></pre></li>
<li><p>In the decompressed yipyap folder, create a virtual environement and install dependencies</p>
<ul>
<li><p>On Linux:</p>
<pre><code class="hljs language-bash">python -m venv venv
./venv/bin/pip install -r requirements.txt</code></pre></li>
<li><p>On Windows:</p>
<pre><code class="hljs language-powershell">python <span class="hljs-literal">-m</span> venv venv
.\venv\Scripts\pip install <span class="hljs-literal">-r</span> requirements.txt</code></pre></li>
</ul>
</li>
<li><p>Run the development servers:</p>
<ul>
<li><p>On Linux:</p>
<pre><code class="hljs language-bash">DEV_PORT=1984 BACKEND_PORT=1985 ROOT_DIR=/path/to/your/images python -m app</code></pre></li>
<li><p>On Windows (PowerShell):</p>
<pre><code class="hljs language-powershell"><span class="hljs-variable">$env:DEV_PORT</span>=<span class="hljs-number">1984</span>; <span class="hljs-variable">$env:BACKEND_PORT</span>=<span class="hljs-number">1985</span>; <span class="hljs-variable">$env:ROOT_DIR</span>=<span class="hljs-string">&quot;C:\path\to\your\images&quot;</span>; python <span class="hljs-literal">-m</span> app</code></pre></li>
</ul>
</li>
</ol>
<p>This last step will:</p>
<ul>
<li>Install npm dependencies if needed</li>
<li>Start the Vite dev server (port 1984), serving the frontend and proxying api calls to the backend.</li>
<li>Start the FastAPI backend (port 1985)</li>
<li>Enable hot reload for both frontend and backend</li>
</ul>
<p>You can now open your browser to <a href="http://localhost:1984">http://localhost:1984</a></p>
<p><strong>Example with custom port and environment:</strong></p>
<pre><code class="hljs language-bash">DEV_PORT=7000 ROOT_DIR=<span class="hljs-variable">$HOME</span>/datasets NODE_ENV=development python -m app</code></pre><blockquote>
<p><strong>Note:</strong> This command uses the full development setup (unlike the uvicorn examples above which only start the backend):</p>
<ul>
<li>Automatically installs npm dependencies if needed</li>
<li>Starts both Vite frontend dev server (on port 7000) and backend server (on port 7001)</li>
<li>Sets up proper signal handling for graceful shutdown of both servers</li>
<li>Uses <code>~/datasets</code> as the default ROOT_DIR instead of current directory</li>
<li>The <code>NODE_ENV=development</code> is passed to the Vite frontend process</li>
</ul>
</blockquote>
<h3>Environment Variables</h3>
<ul>
<li><code>ENVIRONMENT</code>: development or production (default: development)</li>
<li><code>RELOAD</code>: enable hot reload (default: true in development)</li>
<li><code>IMMEDIATE_RESTART</code>: restart backend immediately on changes (default: true in development)</li>
<li><code>ROOT_DIR</code>: dataset root directory (default: current directory)</li>
<li><code>DEV_PORT</code>: Vite dev server port (default: 1984)</li>
<li><code>BACKEND_PORT</code>: backend api port (default: <code>DEV_PORT+1</code>)</li>
<li><code>NLWEB_ENABLED</code>, <code>NLWEB_CONFIG_DIR</code>, <code>NLWEB_BASE_URL</code>: NLWeb integration controls</li>
<li><code>TTS_ENABLED</code>, <code>TTS_DEFAULT_BACKEND</code>, <code>TTS_KOKORO_MODE</code>, <code>TTS_AUDIO_DIR</code>: Text-to-Speech</li>
<li><code>COMFY_ENABLED</code>, <code>COMFY_API_URL</code>, <code>COMFY_TIMEOUT</code>, <code>COMFY_IMAGE_DIR</code>: ComfyUI integration</li>
<li><code>DIFFUSION_LLM_ENABLED</code>, <code>DIFFUSION_LLM_DEVICE</code>, <code>DIFFUSION_LLM_MAX_NEW_TOKENS</code>, <code>DIFFUSION_LLM_TIMEOUT</code>, <code>DIFFUSION_LLM_BASE_URL</code>: Diffusion-LLM</li>
<li><code>CRAWL_ENABLED</code>, <code>FIRECRAWL_BASE_URL</code>, <code>CRAWL_CACHE_DIR</code>: Crawl/Firecrawl</li>
<li><code>RAG_ENABLED</code>, <code>PG_DSN</code>, <code>RAG_TEXT_MODEL</code>, <code>RAG_CODE_MODEL</code>, <code>RAG_CAPTION_MODEL</code>, <code>RAG_CLIP_MODEL</code>, <code>RAG_CLIP_PREPROCESS</code>, <code>RAG_CLIP_MULTICROP</code>: RAG + embeddings</li>
</ul>
<p>Most of these map to <code>AppConfig</code> and can also be configured at runtime via the config manager service.</p>
<h4>Enable RAG (Vector Search)</h4>
<p>Set the following environment variables and restart the app. RAG is disabled by default.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> RAG_ENABLED=<span class="hljs-literal">true</span>
<span class="hljs-built_in">export</span> PG_DSN=postgresql://yipyap:yipyap@localhost:5432/yipyap
<span class="hljs-comment"># Optional model defaults</span>
<span class="hljs-built_in">export</span> RAG_TEXT_MODEL=mxbai-embed-large
<span class="hljs-built_in">export</span> RAG_CODE_MODEL=bge-m3
<span class="hljs-built_in">export</span> RAG_CAPTION_MODEL=nomic-embed-text</code></pre><p>The vector DB service will ensure the <code>pgvector</code> extension and core tables are created on startup. See <code>docs/rag.md</code> and <code>docs/embeddings-and-vector-db.md</code> for architecture, schema, and API usage. For local Postgres setup and pgvector installation, refer to your distribution’s packages or the pgvector README.</p>
<h2>Developer Documentation</h2>
<h3>Project Structure</h3>
<pre><code class="hljs language-bash">yipyap/
├── app/                           <span class="hljs-comment"># Backend application (FastAPI)</span>
│   ├── api/                       <span class="hljs-comment"># API routers (auth, browse, audio, video, text, code, tools, etc.)</span>
│   ├── services/                  <span class="hljs-comment"># Service system (core, background, integration)</span>
│   │   └── core/                  <span class="hljs-comment"># Config manager, threading manager, service setup</span>
│   ├── managers/                  <span class="hljs-comment"># Managers (indexing, caption queue, model registry, usage tracker, etc.)</span>
│   ├── data_access/               <span class="hljs-comment"># Cached FS access, processing, indexing</span>
│   ├── caption_generation/        <span class="hljs-comment"># Captioners (JTP2, WDv3, Florence‑2, etc.)</span>
│   ├── detection_models/          <span class="hljs-comment"># Detection and VLM models</span>
│   ├── diffusion_llm/             <span class="hljs-comment"># Diffusion‑LLM integration</span>
│   ├── integration/               <span class="hljs-comment"># NLWeb, TTS, Comfy, Vector DB, etc.</span>
│   ├── utils/                     <span class="hljs-comment"># Shared utilities</span>
│   ├── main.py                    <span class="hljs-comment"># FastAPI application and lifespan wiring</span>
│   └── __main__.py                <span class="hljs-comment"># Dev/prod server launcher (starts Vite + backend in dev)</span>
├── src/                           <span class="hljs-comment"># Frontend application (SolidJS + Vite)</span>
│   ├── components/                <span class="hljs-comment"># Feature components (Gallery, ImageViewer, Settings, etc.)</span>
│   ├── composables/               <span class="hljs-comment"># SolidJS composables (reusable reactive logic)</span>
│   ├── contexts/                  <span class="hljs-comment"># App, Gallery, Sidebar, Captioners, etc.</span>
│   ├── pages/                     <span class="hljs-comment"># Route pages (e.g., Gallery, TextViewer)</span>
│   ├── router.ts                  <span class="hljs-comment"># Routing configuration</span>
│   ├── styles.css / themes.css    <span class="hljs-comment"># Global and theme styles</span>
│   └── main.tsx                   <span class="hljs-comment"># Frontend entry point</span>
├── app/tests/                     <span class="hljs-comment"># Backend tests</span>
├── src/test/                      <span class="hljs-comment"># Frontend test utilities and setup</span>
└── e2e/                           <span class="hljs-comment"># Playwright E2E tests</span></code></pre><h3>Key Components</h3>
<ol>
<li><p><strong>Frontend Architecture</strong></p>
<ul>
<li>Entry point <code>src/main.tsx</code> sets up providers and router</li>
<li>Global state in <code>src/contexts/app.tsx</code> with notifications and settings</li>
<li>Composables in <code>src/composables/</code> for reusable reactive logic</li>
<li>Routes defined in <code>src/router.ts</code></li>
<li>Comprehensive i18n under <code>src/i18n/</code></li>
</ul>
</li>
<li><p><strong>Testing Infrastructure</strong></p>
<ul>
<li>Frontend: utilities in <code>src/test/</code>, tests colocated with components</li>
<li>Backend: tests in <code>app/tests/</code></li>
<li>E2E: Playwright specs in <code>e2e/</code></li>
</ul>
</li>
<li><p><strong>Styling System</strong></p>
<ul>
<li>Global styles in <code>src/styles.css</code></li>
<li>Theme styles in <code>src/themes.css</code></li>
<li>Theme utilities and provider under <code>src/theme/</code></li>
</ul>
</li>
<li><p><strong>Backend and Services</strong></p>
<ul>
<li>FastAPI app in <code>app/main.py</code> with lifespan-managed service initialization</li>
<li>Service system in <code>app/services/core/</code> (config manager, threading, registry)</li>
<li>Feature routers in <code>app/api/*</code> (auth, browse, audio, video, text, code, tools, users, debug, etc.)</li>
<li>Integrations: NLWeb, TTS (Kokoro), ComfyUI, Diffusion‑LLM, RAG/Vector DB</li>
<li>Background services for indexing, embeddings, downloads, and queues</li>
</ul>
</li>
</ol>
<h2>License</h2>
<hr>
<p>This project is licensed under the MIT License. See the <code>LICENSE.md</code> file for details.</p>
<h2>Acknowledgements</h2>
<hr>
<p>We gratefully acknowledge the following researchers, developers, and organizations whose models and contributions make yipyap possible:</p>
<h3>Caption Generation and Tagging Models</h3>
<h4>Joint Tagger Project (JTP2)</h4>
<ul>
<li><strong>Authors</strong>: Project RedRocket (RedHotTensors, drhead, Bananapuncakes, Thouph)</li>
<li><strong>Model</strong>: <a href="https://huggingface.co/RedRocket/JointTaggerProject">RedRocket/JointTaggerProject</a></li>
<li><strong>Description</strong>: Multi-label classifier designed for furry images using E621 tags, trained on over 9000 tags with 4+ million images</li>
<li><strong>License</strong>: Apache 2.0</li>
<li><strong>Citation</strong>: &quot;The undisputed best-in-class content classifier for furry artwork&quot;</li>
</ul>
<h4>WD-1.4 Tagger (WDv3)</h4>
<ul>
<li><strong>Author</strong>: SmilingWolf</li>
<li><strong>Models</strong>: <a href="https://huggingface.co/SmilingWolf/wd-vit-tagger-v3">SmilingWolf/wd-vit-tagger-v3</a>, <a href="https://huggingface.co/SmilingWolf/wd-swinv2-tagger-v3">SmilingWolf/wd-swinv2-tagger-v3</a>, <a href="https://huggingface.co/SmilingWolf/wd-convnext-tagger-v3">SmilingWolf/wd-convnext-tagger-v3</a></li>
<li><strong>Description</strong>: General purpose image tagger with character recognition, trained on Danbooru data with multiple backbone architectures (ViT, SwinV2, ConvNeXT)</li>
<li><strong>License</strong>: Apache 2.0</li>
<li><strong>Training Data</strong>: Danbooru images with tag filtering for high-quality annotations</li>
</ul>
<h3>Object Detection and Vision-Language Models</h3>
<h4>Florence-2</h4>
<ul>
<li><strong>Authors</strong>: Bin Xiao, Haiping Wu, Weijian Xu, et al. (Microsoft)</li>
<li><strong>Paper</strong>: &quot;Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks&quot;</li>
<li><strong>Models</strong>: Multiple variants including base, large, fine-tuned, and specialized versions</li>
<li><strong>Description</strong>: Unified vision-language foundation model for comprehensive scene understanding with conversational capabilities</li>
<li><strong>License</strong>: MIT</li>
</ul>
<h4>Furrence-2-Large</h4>
<ul>
<li><strong>Authors</strong>: Thouph, silveroxides (lodestone-horizon organization)</li>
<li><strong>Model</strong>: <a href="https://huggingface.co/lodestone-horizon/furrence2-large">lodestone-horizon/furrence2-large</a></li>
<li><strong>Description</strong>: Enhanced Florence-2 model specialized for furry content understanding and generation</li>
<li><strong>License</strong>: CC-BY-NC-4.0</li>
</ul>
<h4>OWLv2 (Open-World Localization)</h4>
<ul>
<li><strong>Authors</strong>: Matthias Minderer, Alexey Gritsenko, Neil Houlsby (Google Research)</li>
<li><strong>Paper</strong>: &quot;Scaling Open-Vocabulary Object Detection&quot; (arXiv:2306.09683)</li>
<li><strong>Model</strong>: <a href="https://huggingface.co/google/owlv2-base-patch16">google/owlv2-base-patch16</a></li>
<li><strong>Description</strong>: Zero-shot text-conditioned object detection model with open-vocabulary capabilities</li>
<li><strong>License</strong>: Apache 2.0</li>
</ul>
<h4>YOLO (You Only Look Once)</h4>
<ul>
<li><strong>Original Authors</strong>: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi</li>
<li><strong>Paper</strong>: &quot;You Only Look Once: Unified, Real-Time Object Detection&quot;</li>
<li><strong>Implementation</strong>: Ultralytics YOLOv11</li>
<li><strong>Description</strong>: Real-time object detection system with high accuracy and speed</li>
<li><strong>License</strong>: AGPL-3.0 (Ultralytics), GPL (Original YOLO)</li>
</ul>
<h4>Joycaption Watermark Detection</h4>
<ul>
<li><strong>Source</strong>: fancyfeast (Hugging Face)</li>
<li><strong>Components</strong>: Custom-trained YOLO11x model with OWLv2 classification head</li>
<li><strong>Description</strong>: Hybrid model combining YOLO object detection with OWLv2 classification for precise watermark detection</li>
<li><strong>Models</strong>: <code>yolo11x-train28-best.pt</code> + <code>far5y1y5-8000.pt</code> (OWLv2 classification head)</li>
</ul>
<h2>Getting Help</h2>
<hr>
<p>If you encounter any issues or have questions, feel free to open an issue on the GitHub repository.</p>
<h2>Backend Architecture</h2>
<p>The backend is a FastAPI application with a service-oriented design:</p>
<ul>
<li>Lifespan initializes core services (config, threading, data access, file watching) and registers feature routers.</li>
<li>Data access and processing are provided by <code>app/data_access/</code> with a cached filesystem layer and thumbnail/preview generation.</li>
<li>Caption generation is modular (<code>app/caption_generation/</code>) supporting JTP2, WDv3, Florence‑2 and others via a unified model registry and download manager.</li>
<li>Integrations include NLWeb assistant tooling, TTS (Kokoro), ComfyUI, Diffusion‑LLM, and RAG/Vector DB embedding services.</li>
<li>Background services handle indexing, embedding, and long-running tasks under <code>app/services/background/</code>.</li>
</ul>
<p>Captions are supported in multiple formats: <code>.caption</code>, <code>.txt</code>, <code>.tags</code>, <code>.wd</code>, and <code>.e621</code> (JSON). The system performs progressive image processing with WebP thumbs/previews (defaults: thumbnail 256px, preview 1024px; configurable). Security includes strict path resolution against <code>ROOT_DIR</code>, with robust error handling and separate development vs production behavior.</p>
<h2>Test Organization</h2>
<ul>
<li>Frontend unit tests live near components; shared setup and helpers are under <code>src/test/</code> (<code>setup.ts</code>, <code>test-hooks.ts</code>, <code>test-utils.ts</code>).</li>
<li>Backend tests are under <code>app/tests/</code> with unit and integration coverage.</li>
<li>End‑to‑end tests use Playwright and live in <code>e2e/</code>.</li>
</ul>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>