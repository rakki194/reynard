<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Audio Ingestion - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>Audio Ingestion</h1>
      <div class="markdown-content"><h1>Audio Ingestion</h1>
<p>This subsystem provides metadata extraction, waveform visualization, format conversion, transcription, and a minimal text‑to‑speech fallback. It uses the system <code>ffmpeg</code> and <code>ffprobe</code> for consistent performance and behavior. All endpoints under <code>/api/audio</code> require authentication via the standard app guard and respond with JSON. Errors are reported with appropriate HTTP status codes and an error message.</p>
<h2>API Endpoints</h2>
<p>The <code>POST /api/audio/ingest</code> endpoint moves or copies a generated audio file into the configured library folder and records provenance. The request accepts a JSON body with <code>source_path</code> as an absolute or resolvable path to a file, <code>move</code> to prefer move over copy, and optional <code>backend</code>, <code>voice</code>, <code>summary_id</code>, <code>source_url</code>, and <code>extra</code> for arbitrary metadata. The server computes a SHA‑256 content hash over the file and uses it to generate a stable filename of the form <code>tts_&lt;hash16&gt;&lt;ext&gt;</code> inside the <code>tts_audio_dir</code> resolved from application configuration. A sidecar JSON is written next to the ingested audio. If the sidecar already exists, its contents are merged with the new fields, preserving existing keys unless explicitly overwritten by non‑null values. The response includes <code>success</code>, the resolved <code>audio_path</code>, the <code>metadata_path</code> to the sidecar, and a boolean <code>deduplicated</code> that indicates when the destination already contained the same content.</p>
<p>The <code>POST /api/audio/import-to-folder</code> endpoint copies an ingested or external audio file into a dataset directory under <code>ROOT_DIR</code>. The request body includes <code>source_path</code>, a <code>target_dir</code> interpreted relative to <code>ROOT_DIR</code> using the app’s secure path resolver, and an optional <code>new_name</code>. If a name collision occurs, an index suffix is appended to the stem (for example, <code>_1</code>, <code>_2</code>) until a free filename is found. If a sidecar JSON resides next to the source, it is also copied. The response returns <code>success</code> and the final <code>target_path</code>.</p>
<p>The <code>POST /api/audio/analyze</code> endpoint provides four analysis modes controlled by the <code>analysis_type</code> field. The <code>metadata</code> mode invokes <code>ffprobe</code> and returns a dictionary with <code>duration</code> in seconds, <code>sample_rate</code>, <code>channels</code>, <code>bitrate</code>, and <code>codec</code>. The <code>waveform</code> mode returns <code>waveform_data</code> as an opaque byte array suitable for client‑side rendering, along with <code>duration</code>. The <code>statistics</code> mode combines the same metadata with the file size and echoes selected fields for convenience. The <code>transcription</code> mode returns <code>transcription</code> text paired with <code>duration</code>. Unsupported modes result in a 400 error.</p>
<p>The <code>POST /api/audio/generate-waveform</code> endpoint renders a waveform PNG to a specified <code>output_path</code> using <code>ffmpeg</code>’s <code>showwavespic</code> filter. The request carries <code>audio_path</code>, <code>output_path</code>, and presentation parameters <code>width</code>, <code>height</code>, <code>color</code>, and <code>background_color</code>. The server ensures the output directory exists before rendering and returns information about the generated file upon success. The current implementation uses <code>showwavespic</code> and may use fixed defaults if certain parameters are not applied by the backend version.</p>
<p>The <code>POST /api/audio/convert</code> endpoint converts an input audio file into a <code>target_format</code> such as <code>mp3</code>, <code>wav</code>, <code>flac</code>, <code>aac</code>, or <code>ogg</code>. The request accepts <code>source_path</code>, <code>output_path</code>, and <code>target_format</code>, with optional <code>quality</code>, <code>sample_rate</code>, and <code>channels</code>. The server creates parent directories if necessary and uses <code>ffmpeg</code> to perform the conversion. Depending on the format, some optional parameters may be ignored by the underlying command. A successful response confirms the target format and output location.</p>
<p>The <code>POST /api/audio/extract-segment</code> endpoint extracts a portion of an audio file between <code>start_time</code> and <code>end_time</code> (in seconds) to <code>output_path</code>. The implementation uses <code>ffmpeg</code> with <code>-ss</code>, <code>-t</code>, and <code>-c copy</code> to avoid re‑encoding and typically provides a very fast copy. Optional <code>fade_in</code> and <code>fade_out</code> parameters are accepted by the API and may be surfaced by the processor in future revisions; the current backend uses stream copy semantics without applying fades. The response includes <code>start_time</code>, <code>end_time</code>, and the derived <code>duration</code>.</p>
<p>The <code>GET /api/audio/metadata/{path}</code> endpoint returns the parsed metadata, duration in seconds, a human‑readable <code>mm:ss</code> duration, and file size in bytes. The <code>GET /api/audio/duration/{path}</code> endpoint returns only the duration and a formatted string. The <code>{path}</code> parameter must reference a readable file on disk.</p>
<p>The <code>POST /api/audio/transcribe</code> endpoint accepts <code>audio_path</code> with optional <code>language</code> and <code>model</code> hints and returns <code>transcription</code>, <code>language</code>, and <code>model</code> alongside <code>success</code>. The precise transcription backend is abstracted behind the processor and may vary by deployment.</p>
<p>The <code>POST /api/audio/text-to-speech</code> endpoint provides a minimal fallback that writes a short mono WAV file containing silence. It is intended for environments with the full TTS service disabled and should not be used for production synthesis. The request includes <code>text</code>, <code>output_path</code>, <code>voice</code>, <code>language</code>, and <code>speed</code>. The response confirms the created file and parameters.</p>
<p>The <code>GET /api/audio/supported-formats</code> endpoint enumerates input formats derived from the configured set and common output formats and codecs. The <code>GET /api/audio/available-voices</code> endpoint returns a small list intended for the fallback path.</p>
<h2>Processor Details</h2>
<p>Metadata extraction uses <code>ffprobe</code> with quiet logging, JSON output, and both <code>-show_format</code> and <code>-show_streams</code>. The resulting JSON is parsed to derive <code>duration</code> from the format section and to obtain <code>sample_rate</code>, <code>channels</code>, and <code>codec</code> from the first audio stream. The <code>bitrate</code> is read from the <code>bit_rate</code> field when present. When <code>ffprobe</code> fails, an empty dictionary is returned and an error is logged.</p>
<p>Waveform rendering uses <code>ffmpeg</code> and the <code>showwavespic</code> filter. The server issues a command equivalent to a single‑frame render with a specified size and color palette and overwrites the output if present. For thumbnail and preview generation used elsewhere in the system, a synchronous call is performed and a minimal placeholder PNG is returned on failure to preserve UI continuity.</p>
<p>Format conversion invokes <code>ffmpeg</code> with input and output paths and relies on the selected container and codec defaults. The server ensures the output directory exists prior to running the command. Duration is derived from the metadata routine and returned where relevant.</p>
<p>Segment extraction uses <code>ffmpeg</code> with <code>-ss</code> and <code>-t</code> arguments and <code>-c copy</code> to minimize processing overhead. This approach avoids re‑encoding, is generally lossless, and completes quickly even for large files. If more advanced operations such as fades are required, they can be layered with <code>af</code> filters in a future version.</p>
<h2>Operational Notes and Best Practices</h2>
<p>Prefer recording and storing lossless or high‑quality audio where feasible and avoid unnecessary resampling. A sample rate of at least 16 kHz with 16‑bit depth is considered a baseline for clear speech; however, retaining the source rate is generally preferable to resampling. Limit aggressive preprocessing such as noise reduction and automatic gain control, as these can impair downstream transcription quality. For multi‑speaker scenarios, separate channels improve diarization and accuracy. Ensure proper access controls on audio libraries and sidecar metadata, and capture provenance in the sidecar by including backend, voice, and source references to support reproducibility. For large‑scale ingestion, monitor throughput and error rates and validate that inputs match expected schemas before conversion.</p>
<h2>Frontend</h2>
<p>The Audio UI follows the shared grid and modal patterns. It displays metadata and waveform previews with controls for conversion and segment extraction. Responses from the API are shaped for immediate consumption by the frontend components, with duration values provided both as numbers and formatted strings where appropriate.</p>
<ul>
<li>Files:<ul>
<li><code>app/api/audio.py</code></li>
<li><code>app/data_access/audio_processor.py</code></li>
<li><code>src/components/Audio/*</code></li>
</ul>
</li>
</ul>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>