<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SpiderWeave - Reynard Documentation Test</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../highlight.css">
  <link rel="icon" href="/favicon.ico">
</head>
<body>
  <nav class="docs-nav">
    <div class="nav-brand"><a href="../index.html">Reynard Documentation Test</a></div>
    <div class="nav-links"><a href="../index.html">Home</a><a href="../api.html">API Reference</a></div>
  </nav>
  <main class="docs-main">
    <div class="docs-content">
      <h1>SpiderWeave</h1>
      <div class="markdown-content"><h1>SpiderWeave</h1>
<p>A comprehensive web scraping framework with intelligent content extraction and quality assessment capabilities.</p>
<h2>Features</h2>
<ul>
<li><strong>Base Scraper Framework</strong>: Extensible base classes for building site-specific scrapers</li>
<li><strong>Content Extraction</strong>: Multiple extraction engines (newspaper3k, readability, trafilatura, justext)</li>
<li><strong>Quality Assessment</strong>: Intelligent content quality scoring and filtering</li>
<li><strong>Rate Limiting</strong>: Domain-aware rate limiting with robots.txt compliance</li>
<li><strong>Robots Parser</strong>: Full robots.txt parsing and compliance</li>
<li><strong>Plugin System</strong>: Extensible plugin architecture for site-specific scrapers</li>
<li><strong>Content Cleaning</strong>: Advanced content cleaning and normalization</li>
<li><strong>API Utilities</strong>: Helper utilities for common web scraping tasks</li>
</ul>
<h2>Installation</h2>
<pre><code class="hljs language-bash">pip install spiderweave</code></pre><p>For development dependencies:</p>
<pre><code class="hljs language-bash">pip install spiderweave[dev]</code></pre><p>For ML capabilities:</p>
<pre><code class="hljs language-bash">pip install spiderweave[ml]</code></pre><h2>Quick Start</h2>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> spiderweave <span class="hljs-keyword">import</span> BaseScraper, ContentExtractor, ContentQualityScorer
<span class="hljs-keyword">from</span> spiderweave.rate_limiting <span class="hljs-keyword">import</span> DomainRateLimiter
<span class="hljs-keyword">from</span> spiderweave.robots <span class="hljs-keyword">import</span> RobotsParser

<span class="hljs-comment"># Create a basic scraper</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyScraper</span>(<span class="hljs-title class_ inherited__">BaseScraper</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_latest_articles</span>(<span class="hljs-params">self, limit: <span class="hljs-built_in">int</span> = <span class="hljs-number">10</span></span>):
        <span class="hljs-comment"># Implementation here</span>
        <span class="hljs-keyword">pass</span>
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_article_details</span>(<span class="hljs-params">self, article_id</span>):
        <span class="hljs-comment"># Implementation here</span>
        <span class="hljs-keyword">pass</span>
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_article_url</span>(<span class="hljs-params">self, article_id</span>):
        <span class="hljs-comment"># Implementation here</span>
        <span class="hljs-keyword">pass</span>

<span class="hljs-comment"># Use content extraction</span>
extractor = ContentExtractor()
content = extractor.extract(<span class="hljs-string">&quot;https://example.com/article&quot;</span>)

<span class="hljs-comment"># Assess content quality</span>
scorer = ContentQualityScorer()
quality_score = scorer.score(content)

<span class="hljs-comment"># Rate limiting</span>
limiter = DomainRateLimiter()
limiter.wait_if_needed(<span class="hljs-string">&quot;example.com&quot;</span>)

<span class="hljs-comment"># Robots.txt compliance</span>
robots = RobotsParser(<span class="hljs-string">&quot;https://example.com/robots.txt&quot;</span>)
<span class="hljs-keyword">if</span> robots.can_fetch(<span class="hljs-string">&quot;*&quot;</span>, <span class="hljs-string">&quot;https://example.com/article&quot;</span>):
    <span class="hljs-comment"># Proceed with scraping</span>
    <span class="hljs-keyword">pass</span></code></pre><h2>Core Components</h2>
<h3>Base Scraper Framework</h3>
<p>The <code>BaseScraper</code> class provides a foundation for building site-specific scrapers with common functionality like session management, error handling, and content filtering.</p>
<h3>Content Extraction</h3>
<p>Multiple extraction engines are available:</p>
<ul>
<li><strong>newspaper3k</strong>: Fast, general-purpose extraction</li>
<li><strong>readability-lxml</strong>: Mozilla&#39;s readability algorithm</li>
<li><strong>trafilatura</strong>: Advanced extraction with metadata</li>
<li><strong>justext</strong>: Language-agnostic content extraction</li>
</ul>
<h3>Quality Assessment</h3>
<p>The <code>ContentQualityScorer</code> provides intelligent scoring based on:</p>
<ul>
<li>Content length and structure</li>
<li>Readability metrics</li>
<li>Language detection</li>
<li>Spam detection</li>
<li>Content type classification</li>
</ul>
<h3>Rate Limiting</h3>
<p><code>DomainRateLimiter</code> provides intelligent rate limiting that:</p>
<ul>
<li>Respects robots.txt crawl-delay directives</li>
<li>Implements exponential backoff</li>
<li>Tracks requests per domain</li>
<li>Supports custom rate limits</li>
</ul>
<h3>Robots Parser</h3>
<p>Full robots.txt parsing with support for:</p>
<ul>
<li>User-agent matching</li>
<li>Allow/disallow rules</li>
<li>Crawl-delay directives</li>
<li>Sitemap discovery</li>
</ul>
<h2>Architecture</h2>
<pre><code class="hljs">spiderweave<span class="hljs-symbol">/</span>
├── core<span class="hljs-symbol">/</span>
│   ├── base_scraper.py      <span class="hljs-comment"># Base scraper framework</span>
│   ├── config.py           <span class="hljs-comment"># Configuration management</span>
│   └── exceptions.py       <span class="hljs-comment"># Custom exceptions</span>
├── extraction<span class="hljs-symbol">/</span>
│   ├── content_extractor.py    <span class="hljs-comment"># Main extraction interface</span>
│   ├── enhanced_extractor.py   <span class="hljs-comment"># Advanced extraction</span>
│   └── engines<span class="hljs-symbol">/</span>               <span class="hljs-comment"># Extraction engines</span>
├── quality<span class="hljs-symbol">/</span>
│   ├── content_quality_scorer.py    <span class="hljs-comment"># Quality assessment</span>
│   ├── wikipedia_quality_scorer.py  <span class="hljs-comment"># Wikipedia-specific scoring</span>
│   └── content_cleaner.py          <span class="hljs-comment"># Content cleaning</span>
├── rate_limiting<span class="hljs-symbol">/</span>
│   ├── domain_rate_limiter.py      <span class="hljs-comment"># Rate limiting</span>
│   └── robots_parser.py            <span class="hljs-comment"># Robots.txt parsing</span>
├── plugins<span class="hljs-symbol">/</span>                        <span class="hljs-comment"># Plugin system</span>
└── utils<span class="hljs-symbol">/</span>
    └── api.py                      <span class="hljs-comment"># API utilities</span></code></pre><h2>Contributing</h2>
<ol>
<li>Fork the repository</li>
<li>Create a feature branch</li>
<li>Make your changes</li>
<li>Add tests</li>
<li>Run the test suite</li>
<li>Submit a pull request</li>
</ol>
<h2>License</h2>
<p>MIT License - see LICENSE file for details.</p>
<h2>Documentation</h2>
<p>Full documentation is available at <a href="https://spiderweave.readthedocs.io/">https://spiderweave.readthedocs.io/</a></p>
</div>
    </div>
  </main>
  <footer class="docs-footer"><p>&copy; 2024 Reynard Documentation Test. Built with ❤️ using SolidJS.</p></footer>
</body>
</html>