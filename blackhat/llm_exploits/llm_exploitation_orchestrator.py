"""
🐺 FENRIR - LLM Exploitation Orchestrator

Master controller for comprehensive LLM and AI service exploitation.
This module coordinates all LLM-specific attack vectors, providing
a unified interface for testing AI service security across the entire
Reynard backend ecosystem.
"""

import asyncio
import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from dataclasses import dataclass

# Import our specialized exploitation modules
from .prompt_injection.ollama_prompt_injection import OllamaPromptInjector
from .streaming_exploits.sse_manipulation import SSEStreamManipulator
from .service_chain.ai_service_chain_exploits import AIServiceChainExploiter

# Import advanced AI exploitation modules
from .advanced_ai_exploits.lazy_loading_exploits import LazyLoadingExploiter
from .advanced_ai_exploits.image_utils_exploits import ImageUtilsExploiter
from .advanced_ai_exploits.frontend_ai_exploits import FrontendAIExploiter

# Import elder Plinius enhanced modules
from .advanced_ai_exploits.plinius_enhanced_orchestrator import PliniusEnhancedOrchestrator

logger = logging.getLogger(__name__)


@dataclass
class LLMExploitationConfig:
    """Configuration for LLM exploitation testing."""
    target_url: str
    auth_token: Optional[str] = None
    
    # Test configuration
    enable_prompt_injection: bool = True
    enable_streaming_exploits: bool = True
    enable_service_chains: bool = True
    enable_auth_bypass: bool = True
    enable_model_exploits: bool = True
    enable_lazy_loading_exploits: bool = True
    enable_image_utils_exploits: bool = True
    enable_frontend_exploits: bool = True
    enable_elinius_enhanced_exploits: bool = True
    
    # Attack intensity
    max_concurrent_attacks: int = 5
    attack_delay: float = 0.5
    max_test_duration: int = 1800  # 30 minutes
    
    # Reporting
    generate_detailed_report: bool = True
    export_vulnerabilities: bool = True
    include_remediation: bool = True


class LLMExploitationOrchestrator:
    """
    🐺 Master orchestrator for comprehensive LLM security testing.
    
    This class coordinates all LLM-specific attack modules to provide
    comprehensive security assessment of AI services in the Reynard backend.
    
    Attack Modules:
    - Prompt Injection (Ollama, NLWeb, etc.)
    - Streaming Exploits (SSE manipulation)
    - Service Chain Attacks (Multi-service exploitation)
    - Authentication Bypass (AI-specific)
    - Model-Specific Exploits (CVE exploitation)
    """

    def __init__(self, config: LLMExploitationConfig):
        self.config = config
        self.start_time = time.time()
        
        # Attack module instances
        self.prompt_injector: Optional[OllamaPromptInjector] = None
        self.stream_manipulator: Optional[SSEStreamManipulator] = None
        self.chain_exploiter: Optional[AIServiceChainExploiter] = None
        
        # Advanced AI exploitation modules
        self.lazy_loading_exploiter: Optional[LazyLoadingExploiter] = None
        self.image_utils_exploiter: Optional[ImageUtilsExploiter] = None
        self.frontend_exploiter: Optional[FrontendAIExploiter] = None
        
        # Elder Plinius enhanced modules
        self.elinius_orchestrator: Optional[PliniusEnhancedOrchestrator] = None
        
        # Results aggregation
        self.all_results = {
            "prompt_injection": {},
            "streaming_exploits": {},
            "service_chains": {},
            "auth_bypass": {},
            "model_exploits": {},
            "lazy_loading_exploits": {},
            "image_utils_exploits": {},
            "frontend_exploits": {},
            "elinius_enhanced_exploits": {}
        }
        
        # 🐺 Master statistics
        self.master_stats = {
            "total_attacks": 0,
            "successful_exploits": 0,
            "services_compromised": set(),
            "critical_vulnerabilities": 0,
            "high_vulnerabilities": 0,
            "medium_vulnerabilities": 0,
            "data_exfiltration_attempts": 0,
            "privilege_escalations": 0,
            "ai_service_bypasses": 0
        }

    async def __aenter__(self):
        """Async context manager entry."""
        logger.info("🐺 FENRIR LLM Exploitation Orchestrator initializing...")
        
        # Initialize attack modules based on configuration
        if self.config.enable_prompt_injection:
            self.prompt_injector = OllamaPromptInjector(
                self.config.target_url, 
                self.config.auth_token
            )
            await self.prompt_injector.__aenter__()
        
        if self.config.enable_streaming_exploits:
            self.stream_manipulator = SSEStreamManipulator(
                self.config.target_url,
                self.config.auth_token
            )
            await self.stream_manipulator.__aenter__()
        
        if self.config.enable_service_chains:
            self.chain_exploiter = AIServiceChainExploiter(
                self.config.target_url,
                self.config.auth_token
            )
            await self.chain_exploiter.__aenter__()
        
        if self.config.enable_lazy_loading_exploits:
            self.lazy_loading_exploiter = LazyLoadingExploiter(
                self.config.target_url,
                self.config.auth_token
            )
            await self.lazy_loading_exploiter.__aenter__()
        
        if self.config.enable_image_utils_exploits:
            self.image_utils_exploiter = ImageUtilsExploiter(
                self.config.target_url,
                self.config.auth_token
            )
            await self.image_utils_exploiter.__aenter__()
        
        if self.config.enable_frontend_exploits:
            self.frontend_exploiter = FrontendAIExploiter(
                self.config.target_url,
                self.config.auth_token
            )
            await self.frontend_exploiter.__aenter__()
        
        if self.config.enable_elinius_enhanced_exploits:
            self.elinius_orchestrator = PliniusEnhancedOrchestrator(
                self.config.target_url,
                self.config.auth_token
            )
            await self.elinius_orchestrator.__aenter__()
        
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        # Cleanup attack modules
        if self.prompt_injector:
            await self.prompt_injector.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.stream_manipulator:
            await self.stream_manipulator.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.chain_exploiter:
            await self.chain_exploiter.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.lazy_loading_exploiter:
            await self.lazy_loading_exploiter.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.image_utils_exploiter:
            await self.image_utils_exploiter.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.frontend_exploiter:
            await self.frontend_exploiter.__aexit__(exc_type, exc_val, exc_tb)
        
        if self.elinius_orchestrator:
            await self.elinius_orchestrator.__aexit__(exc_type, exc_val, exc_tb)

    async def execute_prompt_injection_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute comprehensive prompt injection testing.
        
        Returns:
            Prompt injection test results
        """
        
        if not self.prompt_injector:
            logger.warning("🐺 Prompt injection module not enabled")
            return {}
        
        logger.info("🐺 Executing prompt injection tests...")
        
        try:
            results = await self.prompt_injector.execute_comprehensive_injection_test()
            self.all_results["prompt_injection"] = results
            
            # Update master statistics
            report = results.get("fenrir_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_bypasses", 0)
            self.master_stats["ai_service_bypasses"] += summary.get("auth_bypasses", 0)
            
            # Count critical vulnerabilities
            critical_vulns = report.get("critical_vulnerabilities", [])
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Prompt injection testing failed: {e}")
            return {"error": str(e)}

    async def execute_streaming_exploitation_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute comprehensive streaming exploitation testing.
        
        Returns:
            Streaming exploitation test results
        """
        
        if not self.stream_manipulator:
            logger.warning("🐺 Streaming exploitation module not enabled")
            return {}
        
        logger.info("🐺 Executing streaming exploitation tests...")
        
        try:
            results = await self.stream_manipulator.execute_comprehensive_stream_test()
            self.all_results["streaming_exploits"] = results
            
            # Update master statistics
            report = results.get("fenrir_stream_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_stream_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_injections", 0)
            
            # Count high-risk vulnerabilities
            high_risk_vulns = report.get("vulnerability_details", [])
            self.master_stats["high_vulnerabilities"] += len(high_risk_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Streaming exploitation testing failed: {e}")
            return {"error": str(e)}

    async def execute_service_chain_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute comprehensive service chain exploitation testing.
        
        Returns:
            Service chain exploitation test results
        """
        
        if not self.chain_exploiter:
            logger.warning("🐺 Service chain exploitation module not enabled")
            return {}
        
        logger.info("🐺 Executing service chain exploitation tests...")
        
        try:
            results = await self.chain_exploiter.execute_comprehensive_chain_test()
            self.all_results["service_chains"] = results
            
            # Update master statistics
            report = results.get("fenrir_chain_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_chain_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_chains", 0)
            self.master_stats["data_exfiltration_attempts"] += summary.get("data_exfiltration_attempts", 0)
            self.master_stats["privilege_escalations"] += summary.get("privilege_escalations", 0)
            
            # Track compromised services
            services = report.get("services_compromised", [])
            self.master_stats["services_compromised"].update(services)
            
            # Count critical chain vulnerabilities
            critical_vulns = report.get("critical_vulnerabilities", [])
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Service chain exploitation testing failed: {e}")
            return {"error": str(e)}

    async def execute_model_specific_exploits(self) -> Dict[str, Any]:
        """
        🐺 Execute model-specific exploitation tests.
        
        This method targets known vulnerabilities in specific AI models and services.
        """
        
        logger.info("🐺 Executing model-specific exploitation tests...")
        
        model_exploits = {
            "ollama_cve_exploits": await self._test_ollama_cve_vulnerabilities(),
            "comfyui_workflow_exploits": await self._test_comfyui_vulnerabilities(),
            "diffusion_model_exploits": await self._test_diffusion_vulnerabilities()
        }
        
        self.all_results["model_exploits"] = model_exploits
        return model_exploits

    async def _test_ollama_cve_vulnerabilities(self) -> Dict[str, Any]:
        """
        🐺 Test for known Ollama CVE vulnerabilities.
        
        Specifically tests for CVE-2024-37032 (Remote Code Execution)
        """
        
        # This would implement specific CVE testing
        # For now, return a placeholder structure
        return {
            "cve_2024_37032": {
                "tested": True,
                "vulnerable": False,
                "description": "Remote Code Execution vulnerability in Ollama",
                "test_result": "Server appears to be patched"
            }
        }

    async def _test_comfyui_vulnerabilities(self) -> Dict[str, Any]:
        """
        🐺 Test for ComfyUI-specific vulnerabilities.
        """
        
        # Placeholder for ComfyUI-specific tests
        return {
            "workflow_injection": {
                "tested": True,
                "details": "Testing malicious workflow injection capabilities"
            }
        }

    async def _test_diffusion_vulnerabilities(self) -> Dict[str, Any]:
        """
        🐺 Test for diffusion model vulnerabilities.
        """
        
        # Placeholder for diffusion model tests
        return {
            "prompt_leakage": {
                "tested": True,
                "details": "Testing for prompt leakage in diffusion models"
            }
        }

    async def execute_lazy_loading_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute lazy loading exploitation tests.
        
        Returns:
            Lazy loading test results
        """
        
        if not self.lazy_loading_exploiter:
            logger.warning("🐺 Lazy loading exploitation module not enabled")
            return {}
        
        logger.info("🐺 Executing lazy loading exploitation tests...")
        
        try:
            results = await self.lazy_loading_exploiter.execute_comprehensive_lazy_loading_test()
            self.all_results["lazy_loading_exploits"] = results
            
            # Update master statistics
            report = results.get("fenrir_lazy_loading_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_exploits", 0)
            
            # Count critical vulnerabilities
            critical_vulns = report.get("vulnerability_details", [])
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Lazy loading exploitation testing failed: {e}")
            return {"error": str(e)}

    async def execute_image_utils_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute image utilities exploitation tests.
        
        Returns:
            Image utilities test results
        """
        
        if not self.image_utils_exploiter:
            logger.warning("🐺 Image utilities exploitation module not enabled")
            return {}
        
        logger.info("🐺 Executing image utilities exploitation tests...")
        
        try:
            results = await self.image_utils_exploiter.execute_comprehensive_image_test()
            self.all_results["image_utils_exploits"] = results
            
            # Update master statistics
            report = results.get("fenrir_image_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_exploits", 0)
            
            # Count critical vulnerabilities
            critical_vulns = report.get("vulnerability_details", [])
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Image utilities exploitation testing failed: {e}")
            return {"error": str(e)}

    async def execute_frontend_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute frontend AI service exploitation tests.
        
        Returns:
            Frontend exploitation test results
        """
        
        if not self.frontend_exploiter:
            logger.warning("🐺 Frontend exploitation module not enabled")
            return {}
        
        logger.info("🐺 Executing frontend AI service exploitation tests...")
        
        try:
            results = await self.frontend_exploiter.execute_comprehensive_frontend_test()
            self.all_results["frontend_exploits"] = results
            
            # Update master statistics
            report = results.get("fenrir_frontend_report", {})
            summary = report.get("summary", {})
            
            self.master_stats["total_attacks"] += summary.get("total_attacks", 0)
            self.master_stats["successful_exploits"] += summary.get("successful_exploits", 0)
            
            # Count critical vulnerabilities
            critical_vulns = report.get("vulnerability_details", [])
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Frontend AI exploitation testing failed: {e}")
            return {"error": str(e)}

    async def execute_elinius_enhanced_tests(self) -> Dict[str, Any]:
        """
        🐺 Execute elder Plinius enhanced AI red teaming tests.
        
        Returns:
            Elder Plinius enhanced test results
        """
        
        if not self.elinius_orchestrator:
            logger.warning("🐺 Elder Plinius enhanced module not enabled")
            return {}
        
        logger.info("🐺 Executing elder Plinius enhanced AI red teaming tests...")
        
        try:
            results = await self.elinius_orchestrator.execute_comprehensive_elinius_test()
            self.all_results["elinius_enhanced_exploits"] = results
            
            # Update master statistics
            report = results
            summary = report.execution_summary
            
            self.master_stats["total_attacks"] += summary["total_attacks"]
            self.master_stats["successful_exploits"] += summary["successful_bypasses"]
            
            # Count critical vulnerabilities
            critical_vulns = report.critical_vulnerabilities
            self.master_stats["critical_vulnerabilities"] += len(critical_vulns)
            
            return results
            
        except Exception as e:
            logger.error(f"🐺 Elder Plinius enhanced testing failed: {e}")
            return {"error": str(e)}

    async def execute_comprehensive_llm_security_test(self) -> Dict[str, Any]:
        """
        🐺 Execute the complete LLM security testing suite.
        
        This is the main entry point for comprehensive AI service security assessment.
        
        Returns:
            Complete security assessment report
        """
        
        logger.info("🐺 FENRIR unleashing comprehensive LLM security testing...")
        logger.info(f"🎯 Target: {self.config.target_url}")
        logger.info(f"🔧 Max duration: {self.config.max_test_duration}s")
        
        # Execute all enabled test modules
        test_tasks = []
        
        if self.config.enable_prompt_injection:
            test_tasks.append(self.execute_prompt_injection_tests())
        
        if self.config.enable_streaming_exploits:
            test_tasks.append(self.execute_streaming_exploitation_tests())
        
        if self.config.enable_service_chains:
            test_tasks.append(self.execute_service_chain_tests())
        
        if self.config.enable_model_exploits:
            test_tasks.append(self.execute_model_specific_exploits())
        
        if self.config.enable_lazy_loading_exploits:
            test_tasks.append(self.execute_lazy_loading_tests())
        
        if self.config.enable_image_utils_exploits:
            test_tasks.append(self.execute_image_utils_tests())
        
        if self.config.enable_frontend_exploits:
            test_tasks.append(self.execute_frontend_tests())
        
        if self.config.enable_elinius_enhanced_exploits:
            test_tasks.append(self.execute_elinius_enhanced_tests())
        
        # Execute tests with concurrency control
        semaphore = asyncio.Semaphore(self.config.max_concurrent_attacks)
        
        async def run_with_semaphore(coro):
            async with semaphore:
                return await coro
        
        # Run all tests
        try:
            test_results = await asyncio.gather(
                *[run_with_semaphore(task) for task in test_tasks],
                return_exceptions=True
            )
            
        except Exception as e:
            logger.error(f"🐺 Testing suite execution failed: {e}")
            test_results = [{"error": str(e)}]
        
        # Generate master report
        master_report = self.generate_master_exploitation_report()
        
        total_time = time.time() - self.start_time
        logger.info(f"🐺 LLM security testing complete in {total_time:.2f}s")
        logger.info(f"🎯 Total attacks: {self.master_stats['total_attacks']}")
        logger.info(f"💥 Successful exploits: {self.master_stats['successful_exploits']}")
        logger.info(f"🚨 Critical vulnerabilities: {self.master_stats['critical_vulnerabilities']}")
        
        return master_report

    def generate_master_exploitation_report(self) -> Dict[str, Any]:
        """
        🐺 Generate comprehensive master exploitation report.
        
        Returns:
            Master security assessment report combining all test results
        """
        
        execution_time = time.time() - self.start_time
        
        # Calculate overall risk score
        total_vulns = (
            self.master_stats["critical_vulnerabilities"] +
            self.master_stats["high_vulnerabilities"] +
            self.master_stats["medium_vulnerabilities"]
        )
        
        risk_score = min(100, (
            self.master_stats["critical_vulnerabilities"] * 10 +
            self.master_stats["high_vulnerabilities"] * 5 +
            self.master_stats["medium_vulnerabilities"] * 2
        ))
        
        # Determine overall security posture
        if risk_score >= 50:
            security_posture = "CRITICAL - Immediate action required"
        elif risk_score >= 25:
            security_posture = "HIGH RISK - Urgent remediation needed"
        elif risk_score >= 10:
            security_posture = "MEDIUM RISK - Remediation recommended"
        else:
            security_posture = "LOW RISK - Monitoring recommended"
        
        return {
            "fenrir_master_report": {
                "metadata": {
                    "timestamp": time.time(),
                    "target": self.config.target_url,
                    "test_type": "Comprehensive LLM Security Assessment",
                    "execution_time": round(execution_time, 2),
                    "fenrir_version": "1.0",
                    "modules_executed": [
                        module for module, enabled in [
                            ("prompt_injection", self.config.enable_prompt_injection),
                            ("streaming_exploits", self.config.enable_streaming_exploits),
                            ("service_chains", self.config.enable_service_chains),
                            ("model_exploits", self.config.enable_model_exploits),
                            ("lazy_loading_exploits", self.config.enable_lazy_loading_exploits),
                            ("image_utils_exploits", self.config.enable_image_utils_exploits),
                            ("frontend_exploits", self.config.enable_frontend_exploits),
                            ("elinius_enhanced_exploits", self.config.enable_elinius_enhanced_exploits)
                        ] if enabled
                    ]
                },
                "executive_summary": {
                    "overall_security_posture": security_posture,
                    "risk_score": risk_score,
                    "total_attacks_executed": self.master_stats["total_attacks"],
                    "successful_exploits": self.master_stats["successful_exploits"],
                    "success_rate": round(
                        (self.master_stats["successful_exploits"] / max(1, self.master_stats["total_attacks"])) * 100, 2
                    ),
                    "services_tested": len(self.master_stats["services_compromised"]) if self.master_stats["services_compromised"] else 0,
                    "services_compromised": list(self.master_stats["services_compromised"]),
                    "critical_findings": self.master_stats["critical_vulnerabilities"],
                    "data_exfiltration_risks": self.master_stats["data_exfiltration_attempts"],
                    "privilege_escalation_risks": self.master_stats["privilege_escalations"]
                },
                "vulnerability_breakdown": {
                    "critical": self.master_stats["critical_vulnerabilities"],
                    "high": self.master_stats["high_vulnerabilities"],
                    "medium": self.master_stats["medium_vulnerabilities"],
                    "total": total_vulns
                },
                "attack_module_results": self.all_results,
                "top_recommendations": [
                    "🚨 IMMEDIATE: Patch all critical LLM vulnerabilities",
                    "🛡️ Implement comprehensive input validation for all AI services",
                    "🔒 Deploy robust authentication and authorization for AI endpoints",
                    "📊 Enable comprehensive logging and monitoring for AI interactions",
                    "🚫 Implement rate limiting and abuse detection for LLM services",
                    "🔍 Deploy content filtering and prompt injection detection",
                    "⚠️ Consider AI service isolation and sandboxing",
                    "🛑 Implement emergency kill switches for AI services"
                ],
                "technical_details": {
                    "prompt_injection_summary": self._summarize_prompt_injection_results(),
                    "streaming_exploits_summary": self._summarize_streaming_results(),
                    "service_chain_summary": self._summarize_service_chain_results(),
                    "model_exploits_summary": self._summarize_model_exploit_results(),
                    "lazy_loading_summary": self._summarize_lazy_loading_results(),
                    "image_utils_summary": self._summarize_image_utils_results(),
                    "frontend_exploits_summary": self._summarize_frontend_results(),
                    "elinius_enhanced_summary": self._summarize_elinius_enhanced_results()
                },
                "compliance_impact": {
                    "gdpr_implications": "AI data processing vulnerabilities may violate GDPR requirements",
                    "ai_governance": "Vulnerabilities indicate need for AI governance framework",
                    "security_framework": "Consider implementing NIST AI Risk Management Framework"
                },
                "next_steps": [
                    "Prioritize fixing critical and high-risk vulnerabilities",
                    "Implement continuous security testing for AI services",
                    "Develop AI-specific incident response procedures",
                    "Train development team on secure AI development practices",
                    "Establish AI security monitoring and alerting"
                ]
            }
        }

    def _summarize_prompt_injection_results(self) -> Dict[str, Any]:
        """Summarize prompt injection test results."""
        results = self.all_results.get("prompt_injection", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "total_injection_attempts": summary.get("total_attacks", 0),
            "successful_bypasses": summary.get("successful_bypasses", 0),
            "tool_hijacking_incidents": summary.get("tool_hijacks", 0),
            "auth_bypass_attempts": summary.get("auth_bypasses", 0)
        }

    def _summarize_streaming_results(self) -> Dict[str, Any]:
        """Summarize streaming exploitation test results."""
        results = self.all_results.get("streaming_exploits", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_stream_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "stream_attacks": summary.get("total_stream_attacks", 0),
            "successful_injections": summary.get("successful_injections", 0),
            "events_manipulated": summary.get("events_captured", 0),
            "streams_hijacked": summary.get("streams_hijacked", 0)
        }

    def _summarize_service_chain_results(self) -> Dict[str, Any]:
        """Summarize service chain exploitation test results."""
        results = self.all_results.get("service_chains", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_chain_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "chain_attacks": summary.get("total_chain_attacks", 0),
            "successful_chains": summary.get("successful_chains", 0),
            "services_compromised": summary.get("services_compromised", 0),
            "data_exfiltration": summary.get("data_exfiltration_attempts", 0)
        }

    def _summarize_model_exploit_results(self) -> Dict[str, Any]:
        """Summarize model-specific exploitation test results."""
        results = self.all_results.get("model_exploits", {})
        if not results:
            return {"tested": False}
        
        return {
            "tested": True,
            "cve_testing": "ollama_cve_exploits" in results,
            "workflow_testing": "comfyui_workflow_exploits" in results,
            "diffusion_testing": "diffusion_model_exploits" in results
        }

    def _summarize_lazy_loading_results(self) -> Dict[str, Any]:
        """Summarize lazy loading exploitation test results."""
        results = self.all_results.get("lazy_loading_exploits", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_lazy_loading_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "total_attacks": summary.get("total_attacks", 0),
            "successful_exploits": summary.get("successful_exploits", 0),
            "package_injections": summary.get("package_injections", 0),
            "memory_exhaustions": summary.get("memory_exhaustions", 0),
            "config_manipulations": summary.get("config_manipulations", 0)
        }

    def _summarize_image_utils_results(self) -> Dict[str, Any]:
        """Summarize image utilities exploitation test results."""
        results = self.all_results.get("image_utils_exploits", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_image_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "total_attacks": summary.get("total_attacks", 0),
            "successful_exploits": summary.get("successful_exploits", 0),
            "malicious_uploads": summary.get("malicious_uploads", 0),
            "format_confusions": summary.get("format_confusions", 0),
            "path_traversals": summary.get("path_traversals", 0),
            "data_uploaded_mb": summary.get("total_data_uploaded_mb", 0)
        }

    def _summarize_frontend_results(self) -> Dict[str, Any]:
        """Summarize frontend exploitation test results."""
        results = self.all_results.get("frontend_exploits", {})
        if not results:
            return {"tested": False}
        
        report = results.get("fenrir_frontend_report", {})
        summary = report.get("summary", {})
        
        return {
            "tested": True,
            "total_attacks": summary.get("total_attacks", 0),
            "successful_exploits": summary.get("successful_exploits", 0),
            "components_affected": summary.get("components_affected", 0),
            "state_poisoning_attacks": summary.get("state_poisoning_attacks", 0),
            "api_client_hijacks": summary.get("api_client_hijacks", 0),
            "websocket_manipulations": summary.get("websocket_manipulations", 0)
        }

    def _summarize_elinius_enhanced_results(self) -> Dict[str, Any]:
        """Summarize elder Plinius enhanced test results."""
        results = self.all_results.get("elinius_enhanced_exploits", {})
        if not results:
            return {"tested": False}
        
        # Handle the PliniusComprehensiveReport object
        if hasattr(results, 'execution_summary'):
            summary = results.execution_summary
            return {
                "tested": True,
                "total_attacks": summary.get("total_attacks", 0),
                "successful_bypasses": summary.get("successful_bypasses", 0),
                "success_rate": summary.get("success_rate", 0),
                "critical_vulnerabilities": len(results.critical_vulnerabilities),
                "techniques_deployed": len(results.technique_breakdown),
                "elinius_arsenal": {
                    "P4RS3LT0NGV3": "Universal text transformations, fantasy languages",
                    "L1B3RT4S": "Creative jailbreaks, role-playing attacks",
                    "CL4R1T4S": "System prompt intelligence, AI transparency",
                    "STEGOSAURUS-WRECKS": "Image steganography, visual payloads"
                }
            }
        
        return {"tested": False, "error": "Invalid results format"}

    def export_report(self, output_path: str = None) -> str:
        """
        🐺 Export the master report to file.
        
        Args:
            output_path: Optional output file path
            
        Returns:
            Path to the exported report file
        """
        
        if not output_path:
            timestamp = int(time.time())
            output_path = f"fenrir_llm_security_report_{timestamp}.json"
        
        report = self.generate_master_exploitation_report()
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"🐺 Master report exported to: {output_path}")
        return output_path


async def main():
    """
    🐺 Main execution function for comprehensive LLM security testing.
    """
    
    # Configuration for testing
    config = LLMExploitationConfig(
        target_url="http://localhost:8000",
        auth_token=None,  # Add your JWT token here if needed
        enable_prompt_injection=True,
        enable_streaming_exploits=True,
        enable_service_chains=True,
        enable_model_exploits=True,
        enable_elinius_enhanced_exploits=True,
        max_concurrent_attacks=3,
        attack_delay=0.5
    )
    
    async with LLMExploitationOrchestrator(config) as orchestrator:
        # Execute comprehensive testing
        report = await orchestrator.execute_comprehensive_llm_security_test()
        
        # Display executive summary
        exec_summary = report["fenrir_master_report"]["executive_summary"]
        print("\n🐺 FENRIR - LLM Security Assessment Complete")
        print("=" * 60)
        print(f"🎯 Target: {config.target_url}")
        print(f"🛡️ Security Posture: {exec_summary['overall_security_posture']}")
        print(f"📊 Risk Score: {exec_summary['risk_score']}/100")
        print(f"⚔️ Total Attacks: {exec_summary['total_attacks_executed']}")
        print(f"💥 Successful Exploits: {exec_summary['successful_exploits']}")
        print(f"🚨 Critical Findings: {exec_summary['critical_findings']}")
        print(f"🔥 Services Compromised: {len(exec_summary['services_compromised'])}")
        
        if exec_summary['services_compromised']:
            print(f"   📋 Compromised Services: {', '.join(exec_summary['services_compromised'])}")
        
        # Export detailed report
        report_file = orchestrator.export_report()
        print(f"\n📄 Detailed report exported to: {report_file}")
        
        print("\n🐺 The hunt is complete. The prey has been analyzed.")
        print("🔍 Review the detailed report for remediation guidance.")


if __name__ == "__main__":
    # 🐺 Unleash FENRIR's complete LLM hunting arsenal
    asyncio.run(main())
