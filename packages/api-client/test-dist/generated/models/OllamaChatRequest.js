"use strict";
/* tslint:disable */
/* eslint-disable */
/**
 * Reynard API
 * Secure API backend for Reynard applications
 *
 * The version of the OpenAPI document: 1.0.0
 *
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.instanceOfOllamaChatRequest = instanceOfOllamaChatRequest;
exports.OllamaChatRequestFromJSON = OllamaChatRequestFromJSON;
exports.OllamaChatRequestFromJSONTyped = OllamaChatRequestFromJSONTyped;
exports.OllamaChatRequestToJSON = OllamaChatRequestToJSON;
exports.OllamaChatRequestToJSONTyped = OllamaChatRequestToJSONTyped;
/**
 * Check if a given object implements the OllamaChatRequest interface.
 */
function instanceOfOllamaChatRequest(value) {
  if (!("message" in value) || value["message"] === undefined) return false;
  return true;
}
function OllamaChatRequestFromJSON(json) {
  return OllamaChatRequestFromJSONTyped(json, false);
}
function OllamaChatRequestFromJSONTyped(json, ignoreDiscriminator) {
  if (json == null) {
    return json;
  }
  return {
    message: json["message"],
    model: json["model"] == null ? undefined : json["model"],
    systemPrompt: json["system_prompt"] == null ? undefined : json["system_prompt"],
    temperature: json["temperature"] == null ? undefined : json["temperature"],
    maxTokens: json["max_tokens"] == null ? undefined : json["max_tokens"],
    stream: json["stream"] == null ? undefined : json["stream"],
    tools: json["tools"] == null ? undefined : json["tools"],
    context: json["context"] == null ? undefined : json["context"],
  };
}
function OllamaChatRequestToJSON(json) {
  return OllamaChatRequestToJSONTyped(json, false);
}
function OllamaChatRequestToJSONTyped(value, ignoreDiscriminator) {
  if (ignoreDiscriminator === void 0) {
    ignoreDiscriminator = false;
  }
  if (value == null) {
    return value;
  }
  return {
    message: value["message"],
    model: value["model"],
    system_prompt: value["systemPrompt"],
    temperature: value["temperature"],
    max_tokens: value["maxTokens"],
    stream: value["stream"],
    tools: value["tools"],
    context: value["context"],
  };
}
